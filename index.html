<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QMCHQQ8STW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-QMCHQQ8STW');
</script>  
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CVNet - Community Calendar</title>
<meta name="theme-color" content="#FFD12D"/>
<style>

/* Search filtering + highlight */
.is-filtered-out { display: none !important; }
mark.search-hit { background: #fff3b0; padding: 0 .05em; border-radius: 2px; }


.section-desc {
  margin: 6px 0 8px;        /* lowered bottom so the +* can matter */
  color: #2f3a56;
  font-size: 0.95rem;
}

/* adds a little breathing room to whatever follows the blurb */
.section-desc + * {
  margin-top: 8px;
}


  /* Added: predictably size elements and improve wrapping */
  *, *::before, *::after { box-sizing: border-box; }

  :root{
  --brand-yellow:#FFD12D;
  --brand-blue:#000066;
  --text:#000;
  --muted:#f5f7fb;
  --border:#dfe3ea;
  --card-bg:#ffffff;
  --card-shadow: 0 6px 18px rgba(0,0,0,.08), 0 2px 8px rgba(0,0,0,.06);
  --card-shadow-hover: 0 10px 26px rgba(0,0,0,.10), 0 6px 14px rgba(0,0,0,.08);
  /* Helps anchor jumps clear the fixed header */
  scroll-padding-top: 90px;
}

html { scroll-behavior: smooth; }
  body {
    font-family: Arial, sans-serif;
    color: var(--text);
    background-color: var(--muted);
    margin: 0;
    line-height: 1.6;
    font-size: 16px;
    -webkit-text-size-adjust: 100%;
    text-size-adjust: 100%;
  }

  /* Page container margins for content */
  .page {
    margin: 20px;
  }

  /* ===== Fixed header container (covers top edge; content scrolls beneath) ===== */
  .header-fixed {
    position: fixed;
    top: 0; left: 0; right: 0;
    z-index: 1000;
    background: #fff; /* prevents content showing through above header */
    box-shadow: 0 1px 0 rgba(0,0,0,.05);
  }
  /* Inner header respects page side margins and leaves space above yellow banner */
  .site-header {
    margin: 0 20px;         /* aligns with body’s side margin */
    padding-top: 16px;      /* gap above yellow banner */
  }

  /* Title banner */
  .title-banner{
    background: var(--brand-yellow);
    border-radius: 10px;
    padding: 16px 20px;
    text-align:center;
  }
  .title-banner h2{
    margin: 0;
    /* Added: allow long page titles to wrap safely on small screens */
    overflow-wrap:anywhere;
    word-break: break-word;
  }
  .title-banner .subtitle{
    margin-top: 6px;
    font-size: 14px;
  }

  /* Search and nav links */
  .topbar{
    display:flex;
    flex-direction: column;      /* stack main row + back link */
    align-items:flex-start;
    gap: 8px;
    margin: 14px 0 16px 0;
  }
  .topbar-row{
    display:flex;
    width: 100%;
    justify-content: space-between;
    align-items: center;
    gap: 12px;
    flex-wrap: wrap;
  }
  .quick-links{
    display:flex;
    gap:14px;
    flex-wrap: wrap;
  }
  .quick-links a{
    color: var(--brand-blue);
    text-decoration:none;
    font-weight: bold;
    background: rgba(0,0,102,.06);
    padding: 6px 10px;
    border-radius: 999px;
    border: 1px solid rgba(0,0,102,.12);
    white-space: nowrap; /* keep single-line labels */
  }
  .quick-links a:hover{ text-decoration: underline; }

.cv-conference-links{
  display:flex;
  flex-wrap:wrap;
  gap:.4rem;
  margin-bottom:.8rem;
}

  .topbar .back-link{
    margin: 0;
  }

  .search-wrap{
    margin-left:auto;
    display:flex;
    align-items:center;
    gap:8px;
  }
  .search-wrap .icon{ font-size:18px; line-height:1; }
  #search-input{
    width: 280px;
    max-width: 50vw;
    padding: 10px 12px;
    border:1px solid #cfd6e4;
    border-radius:999px;
    background:#fff;
  }

  h3 {
    color: var(--brand-blue);
    border-bottom: 3px solid var(--brand-yellow);
    padding-bottom: 6px;
    margin: 28px 0 14px;
  }
  h4 { margin: 18px 0 8px; color: var(--brand-blue); }

  a {
    color: #003366; text-decoration: none;
    /* Added: ensure very long URLs or emails wrap instead of overflowing */
    overflow-wrap:anywhere;
    word-break: break-word;
  }
  a:hover { text-decoration: underline; }

  mark.cvnet-highlight {
    background-color: yellow;
    color: black;
  }

/* ===== Card layout ===== */
.cards {
  display: grid;
  grid-template-columns: repeat(12, 1fr);
  gap: 14px;
  margin-bottom: 24px;
}

.card {
  grid-column: span 12; /* full width by default (mobile-first) */
  background: var(--card-bg);
  border: 1px solid var(--border);
  border-radius: 12px;
  box-shadow: var(--card-shadow);
  padding: 14px;
  transition: box-shadow .2s ease, transform .2s ease;
  /* Added: make sure content inside cards can wrap and never overflow */
  overflow-wrap: anywhere;
  word-break: break-word;
}

.card:hover {
  box-shadow: var(--card-shadow-hover);
  transform: translateY(-1px);
}

.card .title {
  font-weight: 700;
  color: #001652;
  margin-bottom: 6px;
  line-height: 1.35;
  /* Added: prevent long titles from causing overflow */
  overflow-wrap: anywhere;
  word-break: break-word;
}

.meta {
  display: grid;
  grid-template-columns: 1fr;
  gap: 8px;
}

.meta-row {
  display: grid;
  grid-template-columns: 160px minmax(0,1fr);
  gap: 10px;
  align-items: start;
}

.meta-label {
  font-weight: 700;
  color: var(--brand-blue);
}

.notes-list {
  margin: 0;
  padding-left: 1.1em;
}

.notes-list li {
  margin: 2px 0;
}

/* Added: guarantee the value column actually shrinks and wraps on narrow screens */
.meta-row > :last-child {
  min-width: 0;
  overflow-wrap: anywhere;
  word-break: break-word;
}

/* Section intro paragraph card */
.intro {
  background: #fffaf0;
  border: 1px solid #ffe3a3;
  color: #3a2a00;
}

/* Responsive columns */
@media (min-width: 720px) {
  .card.sm-6 { grid-column: span 6; }
  .card.sm-4 { grid-column: span 4; }
  .card.md-6 { grid-column: span 6; } /* ensure half width at medium+ */
}

/* Mobile niceties & quick-links scroll */
@media (max-width: 700px) {
  .site-header { margin: 0 12px; padding-top: 12px; }
  .topbar-row { gap: 8px; }
  .quick-links {
    flex-wrap: nowrap;
    overflow-x: auto;
    -webkit-overflow-scrolling: touch;
    gap: 10px;
    scroll-snap-type: x mandatory;
    padding-bottom: 6px;
  }
  .quick-links a {
    scroll-snap-align: start;
  }
  .search-wrap { width: 100%; margin-left: 0; }
  #search-input { width: 100%; max-width: 100%; }

  /* Slightly narrower label on phones; allow label wrapping */
  .meta-row { grid-template-columns: minmax(90px,140px) minmax(0,1fr); }
  .meta-label { overflow-wrap:anywhere; }
}

/* ✅ Accessible focus outlines for keyboard users */
a:focus-visible,
button:focus-visible,
input:focus-visible,
select:focus-visible,
textarea:focus-visible {
  outline: 2px solid var(--brand-yellow);
  outline-offset: 2px;
  border-radius: 6px;
}

/* ✅ Improve search input UX */
input[type="search"] {
  -webkit-appearance: textfield;
  accent-color: var(--brand-blue);
  padding-right: 2.2em;
}
input[type="search"]::-webkit-search-cancel-button { cursor: pointer; }

/* ✅ Skip link for keyboard users */
.sr-skip {
  position:absolute; left:-9999px; top:auto; width:1px; height:1px; overflow:hidden;
}
.sr-skip:focus {
  position:fixed; left:16px; top:16px; width:auto; height:auto; padding:8px 12px;
  background:#fff; border:2px solid var(--brand-yellow); border-radius:8px; z-index:1001;
}

/* ✅ Respect reduced-motion preferences */
@media (prefers-reduced-motion: reduce) {
  html { scroll-behavior: auto; }
  .card { transition: none; }
  .card:hover { transform: none; }
}

/* === Footer (calendar + resources hybrid) ================================ */
.site-footer{
  background:#fff;
  border-top:1px solid var(--border);
  margin-top:32px;
}
.footer-inner{
  margin:0 20px;
  padding:18px 0;
  display:flex;
  flex-wrap:wrap;
  gap:12px;
  justify-content:space-between;
  align-items:center;
}
.footer-left small{ color:#2f3a56; }
.footer-nav{
  display:flex;
  gap:14px;
  flex-wrap:wrap;
}
.footer-nav a{
  color:var(--brand-blue);
  text-decoration:none;
  font-weight:bold;
  border:1px solid rgba(0,0,102,.12);
  background:rgba(0,0,102,.06);
  padding:6px 10px;
  border-radius:999px;
}
.footer-nav a:hover{ text-decoration: underline; }

/* optional micro-row if you ever want a second line */
.footer-sub{
  margin:0 20px;
  padding:0 0 16px;
  color:#56607a;
  font-size:.9rem;
}

.deadline-open{
  font-weight: 600;
  color: var(--brand-blue, #000066);
  text-decoration: underline;
  text-underline-offset: 2px;
}
.visually-hidden{
  position:absolute;left:-9999px;width:1px;height:1px;overflow:hidden;
}


</style>

<!-- ✅ External links: open safely in a new tab (robust) -->
<script>
  document.addEventListener('DOMContentLoaded', () => {
    const { host } = location;
    document.querySelectorAll('a[href]').forEach(a => {
      try {
        const url = new URL(a.getAttribute('href'), location.origin);
        const isHttp = url.protocol === 'http:' || url.protocol === 'https:';
        const external = isHttp && url.host !== host;
        if (external) { a.target = '_blank'; a.rel = 'noopener noreferrer'; }
      } catch (e) {}
    });
  });
</script>


</head>
<body>
<a href="#conferences" class="sr-skip">Skip to main content</a>
<!-- Fixed header container that spans the very top -->
<div class="header-fixed">
  <header class="site-header">
    <div class="title-banner">
      <h2>CVNet Community Calendar</h2>
      <div class="subtitle">Last updated: 4 December, 2025</div> 
    </div>

    <div class="topbar">
      <div class="topbar-row">
        <nav class="quick-links" aria-label="Quick section links">
  <a href="#conferences">Conferences</a>
  <a href="#talks">Online Seminars/Clubs</a>
  <a href="#calls">Special/Feature Issues</a>
  <a href="#education">Education</a>
  <a href="#gradprograms">Grad Programs</a>
  <a href="#jobs">Jobs</a>
  <a href="#funding">Funding</a>
  <a href="#competitions">Competitions</a>
</nav>
        <div class="search">
  <input id="search-input" type="search" placeholder="Search..." />
</div>

<!-- Search result counter (for accessibility + user feedback) -->
<div id="search-status" class="visually-hidden" aria-live="polite"></div>

      </div>
      <!-- Back link under main nav buttons -->
      <p class="back-link">
        <a href="https://info.cvnet.org/">← Back to main CVNet info page</a>
      </p>
    </div>
  </header>
</div>

<div class="page" id="top"></div>

<div class="page">
  <div class="card intro">
    This page lists upcoming <strong>conferences, courses, talks, jobs</strong>, and <strong>funding opportunities</strong> for the color &amp; vision research community. Send additions or corrections to
    <a href="mailto:davidpeterzell@mac.com">davidpeterzell@mac.com</a> and
    <a href="mailto:mtedesco12@gmail.com">mtedesco12@gmail.com</a>. Additional opportunities can be found in the
    <a href="https://cvnet.org/mailman/listinfo/cvnet">CVNet Archive</a> and the
    <a href="http://visionscience.com/pipermail/visionlist_visionscience.com">VisionList Archive</a>.
  </div>

 <!-- Conferences -->
<h3 id="conferences">Conferences</h3>
<p class="section-desc">
  Major academic and industry meetings in vision science, neuroscience, imaging, color, and related fields. Includes key dates for abstracts, registration, and workshops. Conferences are listed in chronological order by event date (not by submission deadlines).
</p>
<!-- Conference Quick Links -->
<nav class="cv-conference-links quick-links" aria-label="Conference quick links">
  <a href="#apa" class="cv-btn">APA</a>
  <a href="#apcv" class="cv-btn">APCV</a>
  <a href="#aps" class="cv-btn">APS</a>
  <a href="#arvo" class="cv-btn">ARVO</a>
  <a href="#ava" class="cv-btn">AVA</a>
  <a href="#bavrd" class="cv-btn">BAVRD</a>
  <a href="#ecvp" class="cv-btn">ECVP</a>
  <a href="#gruppo-del-colore" class="cv-btn">Gruppo del Colore</a>
  <a href="#hvei" class="cv-btn">HVEI</a>
  <a href="#icvs" class="cv-btn">ICVS</a>
  <a href="#modvis" class="cv-btn">MODVIS</a>
  <a href="#optica-fall-vision" class="cv-btn">Optica Fall Vision</a>
  <a href="#psychonomics" class="cv-btn">Psychonomics</a>
  <a href="#sfn" class="cv-btn">SfN</a>
  <a href="#vsac" class="cv-btn">VSAC</a>
  <a href="#vss" class="cv-btn">VSS</a>
</nav>
<div class="cards">


    <!-- Each conference as a card -->    

<div class="card md-6">
  <div class="title"><b>CVMP 2025 — ACM SIGGRAPH European Conference on Visual Media Production</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual Conference</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>December 3–4, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>BFI Southbank, London, UK</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.cvmp-conference.org/2025/">cvmp-conference.org/2025</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Deadline for short papers &amp; demos:</b> September 19, 2025 (extended)</li>
          <li>Bring together researchers &amp; industry in video processing, imaging, graphics, and production practice</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<div class="card md-6" id="smoky-mirrors-2025"
     data-tags="symposium conference hybrid vr hci social-interaction self-other virtual-reality neuroscience psychology robotics">
  <div class="title">
    <b>Smoky Mirrors Symposium: Self–Other Actions and Interactions in Real and Virtual Worlds</b>
  </div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Type</div>
      <div>Interdisciplinary symposium (hybrid)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>December 12, 2025 &mdash; 09:45–18:00 (CET / UTC+1)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>
        <b>In person:</b> TU Darmstadt, Building S3|20 Room 18 (Friedrich-Ludwig-Weidig-Saal), Rundeturmstraße 10, 64283 Darmstadt, Germany<br>
        <b>Online:</b> via Zoom (link provided after registration)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        Interdisciplinary event on human–human and human–machine interactions across real and virtual environments,
        connecting work in neuroscience, psychology, VR/HCI, cognitive science, and robotics.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Confirmed Speakers</div>
      <div>
        Juan Avendano (Aalto University) &bull;
        Helge Gillmeister (University of Essex) &bull;
        Loes van Dam (TU Darmstadt) &bull;
        Georgia Chalvatzaki (TU Darmstadt) &bull;
        Anna Ciaunica &amp; Altea Vanni (ULisboa) &bull;
        Amir Jahanian Najafabadi (Bielefeld University)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://www.aalto.fi/en/events/smoky-mirrors-symposium-self-other-actions-and-interactions-in-real-and-virtual-worlds">
          Event details
        </a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Registration</div>
      <div>
        Attendance is <b>free</b>, but registration is required.<br>
        Registration form: 
        <a href="https://link.webropolsurveys.com/S/A68A36D6F25248D5">Registration link</a><br>
        <b>In-person deadline:</b> Tuesday, December 9, 2025 (17:00 CET)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Organisers</div>
      <div>
        Loes van Dam (TU Darmstadt) &bull;
        Anna Ciaunica (ULisboa) &bull;
        Juan Avendano (Aalto University)
      </div>
    </div>

  </div>
</div>



<div class="card md-6" id="ava-xmas-2025" data-tags="conference ava uk vision-science optometry perception aston birmingham">
  <div class="title"><b>AVA Xmas Meeting 2025</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>December 15, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Aston University, Birmingham, UK</div></div>
    <div class="meta-row"><div class="meta-label">Organizer</div><div>Applied Vision Association (AVA)</div></div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>Annual end-of-year meeting of the AVA community, featuring talks and posters across vision science and perception research. Especially supportive of early-career and student presenters. Abstracts published in <i>Perception</i> (SAGE).</div>
    </div>
    <div class="meta-row"><div class="meta-label">Abstract Deadline</div><div>November 13, 2025 (5:00 PM UK)</div></div>
    <div class="meta-row"><div class="meta-label">Registration</div><div><a href="https://forms.office.com/e/z0Ci74AY9V">Initial registration form</a> (collects attendee info; payment link to follow)</div></div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Encourages submissions from PhD and Master’s students</li>
          <li>Abstracts follow Perception format; re-publication opt-out available</li>
          <li>Organized by Dr. Samantha Strong (Aston University)</li>
          <li>Announcement posted via AVA Listserv on Oct 31, 2025</li>
        </ul>
      </div>
    </div>
  </div>
</div>

    <!-- 2026 -->

<div class="card md-6">
  <div class="title"><b>AAAI 2026 Workshop — Neuro for AI &amp; AI for Neuro: Towards Multi-Modal Natural Intelligence</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Workshop @ AAAI</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>January 27, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Singapore</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://neuroai-multimodal-workshop.github.io/">Workshop site</a> •
        <a href="https://openreview.net/group?id=AAAI.org/2026/Workshop/NeuroAI">OpenReview submissions</a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Submission deadline:</b> October 30, 2025</li>
          <li>Topics: Neuro&nbsp;→&nbsp;AI &amp; AI&nbsp;→&nbsp;Neuro; multimodal natural intelligence</li>
          <li>Invited speakers include Chklovskii, Tolias, Schrimpf, Chen, Razi, Shou</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Vision and Depiction 2026 -->
<div class="card md-6" id="vision-depiction-2026">
  <div class="title"><b>Vision and Depiction 2026 — Delft</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>February 4–6, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Delft, Netherlands</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div>
        <a href="https://visionanddepiction.github.io/">visionanddepiction.github.io</a> &nbsp;|&nbsp;
        <a href="https://visionanddepiction.github.io/program2024">Archive (2024)</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Abstract submission:</b> CLOSED</li>
          <li>Scope: formal elements bridging science &amp; art (texture, colour, light, shape, space, material, motion, etc.)</li>
          <li>Organiser: Maarten Wijntjes (TU Delft)</li>
          <li>Contact if unsure about fit; selection, timeline, and registration on the website</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="card md-6" id="innovations-in-neuroimaging-methods-nyuad-2026" data-tags="conference neuroimaging methods cognitive-neuroscience tutorials posters panels uae nyuad">
  <div class="title"><b>Innovations in Neuroimaging Methods — NYU Abu Dhabi 2026</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>February 9–11, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>NYU Abu Dhabi, United Arab Emirates</div></div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>Conference bringing together an international cohort to assess the state of human cognitive neuroimaging and chart future directions, including tutorials for students and junior researchers.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Program</div>
      <div>Research talks • Poster sessions • Panels: “Women in Neuroscience”, “Neuroimaging in the UAE &amp; MENA” • Hands-on tutorials.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Confirmed Speakers</div>
      <div>Yanchao Bi • Olivia Cheung • Olivier Collignon • Ida Gobbini • Clayton Hickey • Angelika Lingnau • Liuba Papeo • Marius Peelen • Nathan Weisz</div>
    </div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://nyuad.nyu.edu/en/events/2026/february/innovations-in-neuroimaging-methods.html">NYUAD event page</a></div></div>
    <div class="meta-row"><div class="meta-label">Organizers</div><div>David Melcher (NYU Abu Dhabi) • Olivia Cheung (United Arab Emirates University)</div></div>
    <div class="meta-row"><div class="meta-label">In Collaboration With</div><div>NYUAD Center for Brain and Health • NYU Abu Dhabi Institute</div></div>
  </div>
</div>


<!-- IRCDL 2026 — 22nd Conference on Information and Research Science Connecting to Digital and Library Science -->
<div class="card md-6" id="ircdl-2026"
     data-tags="conference 2026 digital-libraries information-science digital-humanities retrieval ai machine-learning italy">
  <div class="title">
    <b>IRCDL 2026 — 22nd Conference on Information &amp; Research Science Connecting to Digital and Library Science</b>
  </div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>February 19–20, 2026</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Modena, Italy</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://ircdl2026.unimore.it/" rel="noopener">ircdl2026.unimore.it</a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Deadline</div>
      <div>
        <b>Extended:</b> December 12, 2025 (11:59 p.m. AoE)<br>
        Notification: January 23, 2026<br>
        Camera-ready: February 6, 2026
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Tracks</div>
      <div>
        <b>Track 1:</b> Computer Science Foundations for Digital Libraries  
        <span class="small">(Algorithms, systems, retrieval, data management)</span><br>
        <b>Track 2:</b> Digital Humanities &amp; Cultural Heritage  
        <span class="small">(Computational humanities, text mining, preservation)</span>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Topics</div>
      <div>
        Digital libraries • information retrieval • generative AI • ML/data mining • digital humanities • 
        metadata &amp; ontologies • document analysis • cultural heritage • HCI • multimodal information 
        management • open science • preservation • scientometrics.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Submissions</div>
      <div>
        Via CMT: <a href="https://cmt3.research.microsoft.com/IRCDL2026">CMT submission portal</a><br>
        Research papers: 10–12 pages<br>
        Short papers: 6–7 pages<br>
        Extended abstracts: 5 pages<br>
        (References not counted)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Proceedings</div>
      <div>
        CEUR-WS (open access)
      </div>
    </div>

  </div>
</div>



<div class="card md-6">
  <div class="title"><b>Electronic Imaging Symposium (EI 2026)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual (multi-conference symposium)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>March 1–5, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Hyatt Regency San Francisco Airport, Burlingame, CA</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://imaging.org/IST/IST/Conferences/EI/EI2026/Program.aspx">imaging.org – EI 2026</a></div></div>
    <div class="meta-row"><div class="meta-label">Notes</div><div>
      <ul class="notes-list">
        <li><b>Abstract submission deadline: October 15, 2025</b> (extended)</li>
        <li>Includes 17 technical conferences (e.g., HVEI)</li>
      </ul>
    </div></div>
  </div>
</div>

<div class="card md-6" id="hvei">
      <div class="title"><b>Human Vision and Electronic Imaging (HVEI)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>March 1–5, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Hyatt Regency San Francisco Airport, Burlingame, California</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.imaging.org/IST/IST/Conferences/EI/EI2026/Conference/C_HVEI.aspx?23416eb0dbe7=3">imaging.org • HVEI 2026</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Extended submission deadline: September 12, 2025</li>
              <li>Author notifications: October 10, 2025</li>
              <li>Journal-first: Final manuscripts due October 31, 2025</li>
              <li>FastTrack (early online): January 9, 2026</li>
              <li>Non-FastTrack (post-symposium): March 23, 2026</li>
              <li>Demo application deadline: January 15, 2026, 09:00 (NY)</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

<div class="card md-6">
  <div class="title"><b>Winter Conference on Applications of Computer Vision (WACV)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>March 6–10, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>JW Marriott Starr Pass, Tucson, Arizona</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://wacv.thecvf.com/">wacv.thecvf.com</a></div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Call for Tutorial Proposals</b> — proposals due <b>October 27, 2025 (11:59 PM PDT)</b>; notifications by <b>November 17, 2025</b>.</li>
          <li>Tutorials will be held in person on <b>Friday, March 6</b> or <b>Saturday, March 7, 2026</b>.</li>
          <li>Submission: <a href="https://openreview.net/group?id=thecvf.com/WACV/2026/Tutorials">OpenReview — WACV 2026 Tutorials</a>. Chairs: Marta Gomez-Barrero &amp; Vishal M. Patel.</li>
          <li>Round 1: Reviews issued September 3, 2025</li>
          <li>Round 1: Camera-ready (accepted) September 18, 2025</li>
          <li>Round 2: New paper registration September 12, 2025</li>
          <li>Round 2: Submission September 19, 2025</li>
          <li>Final decisions: November 5, 2025</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- VisionDocs @ WACV 2026 -->
<div class="card md-6" id="visiondocs-wacv2026" data-tags="workshop wacv2026 document-analysis recognition computer-vision">
  <div class="title"><b>VisionDocs @ WACV 2026 — Computer Vision Systems for Document Analysis and Recognition</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Workshop (in conjunction with WACV 2026)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>March 6, 2026 (one-day workshop)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>JW Marriott Starr Pass, Tucson, Arizona, USA</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://sites.google.com/view/avml-lab-visiondocs-wacv2026/home">
          VisionDocs @ WACV 2026
        </a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div><a href="mailto:visiondocs.organizers@gmail.com">visiondocs.organizers@gmail.com</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        3rd workshop on advancing document analysis and recognition through computer vision — tackling heterogeneous document classes,
        low-data environments, and multi-modal fusion strategies for improved accuracy.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Topics</div>
      <div>
        Document image processing • Layout and handwriting recognition • Document forensics • Table and formula recognition •
        Multimedia and multi-modal document analysis • Graphics recognition • Structured document generation •
        Historical documents • Datasets and benchmarks.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Key Dates (PST)</div>
      <div>
        <ul class="notes-list">
          <li><b>Paper submissions (extended):</b> Dec 7, 2025 (11:59 PM PST)</li>
          <li><b>Notifications:</b> Jan 2, 2026 (11:59 PM PST)</li>
          <li><b>Camera-ready:</b> Jan 9, 2026 (11:59 PM PST)</li>
        </ul>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Organizers</div>
      <div>Axel De Nardin, Silvia Zottin, Silvia Cascianelli, Claudio Piciarelli, Gian Luca Foresti</div>
    </div>

  </div>
</div>


<!-- Workshop: Physical Retail AI Workshop (PRAW) @ WACV 2026 -->
<div class="card md-6" id="praw-wacv2026" data-tags="workshop wacv2026 retail ai computer-vision grocery datasets challenges">
  <div class="title"><b>3rd Physical Retail AI Workshop (PRAW) @ WACV 2026</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Workshop (in conjunction with WACV 2026)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>March 6–7, 2026</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Tucson, Arizona, USA</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://grocery-vision.github.io">grocery-vision.github.io</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        3rd edition of the Physical Retail AI Workshop at WACV, focusing on computer vision and AI
        for physical retail environments, with shared public datasets (Grocery Vision, RetailAction)
        and associated challenges in the grocery/retail domain.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Workshop Papers</div>
      <div>
        <ul class="notes-list">
          <li><b>Submission deadline:</b> December 19, 2025 (23:59, anywhere on earth)</li>
          <li><b>Notification of acceptance:</b> December 26, 2025</li>
          <li><b>Camera-ready deadline:</b> January 9, 2026</li>
        </ul>
        <div>Submission site: <a href="https://openreview.net/group?id=thecvf.com/WACV/2026/Workshop/PRAW">OpenReview (PRAW)</a></div>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Grocery Vision Challenges (GV26)</div>
      <div>
        <ul class="notes-list">
          <li><b>Tracks:</b> 
            Track 1 — Shopping Cart Video Temporal/Spatio-Temporal Action Localization (SC-TAL / SC-STAL); 
            Track 2 — Multi-modal Product Retrieval (MPR); 
            Track 3 — Top-View Spatio-Temporal Action Localization (TV-STAL, hosted on Kaggle).
          </li>
          <li><b>Registration &amp; training data release:</b> December 12, 2025</li>
          <li><b>Results submission deadline:</b> January 16, 2026</li>
          <li><b>Winner notification:</b> January 30, 2026</li>
        </ul>
        <div>Challenge details: <a href="https://grocery-vision.github.io">Grocery Vision Challenge (GV26)</a></div>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Organizers</div>
      <div>
        Rocco Pietrini (Mercatorum University) • Bruno Abbate • David Woollard • Davide Mazzini (Standard AI) •
        Quanfu Fan • Sean Ma • Shun Miao • Weijian Li (Amazon Worldwide Grocery Stores)
      </div>
    </div>

  </div>
</div>

<!-- COSYNE Main Conference -->
<div class="card md-6" id="cosyne">
  <div class="title"><b>COSYNE — Computational &amp; Systems Neuroscience</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>Main: March 12–15, 2026 • Workshops: March 16–17, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Lisbon Congress Centre (Lisbon) &amp; Hotel Cascais Miragem (Cascais), Portugal</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.cosyne.org/">cosyne.org</a></div></div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Abstract submission deadline: October 16, 2025 (11:59 PM PST)</li>
          <li>Author notifications: December 2025</li>
          <li>Workshop proposals due: October 23, 2025 (11:59 PM PST)</li>
          <li>Undergraduate travel grant deadline: November 12, 2025 (11:59 PM PST)</li>
          <li>Travel grants: at least &euro;500 (larger possible); priority for first-time attendees, under-represented groups, mentor–student pairs, undergrads, and authors of submitted abstracts</li>
          <li>Student volunteer applications due: December 31, 2025 (free registration; limited slots)</li>
          <li>Contact: <a href="mailto:meeting@cosyne.org">meeting@cosyne.org</a></li>
          <li>Associated event: <a href="#cosyne-workshops">COSYNE Workshops</a>, March 16–17, 2026 (Hotel Cascais Miragem)</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- German Conference on Medical Image Computing (BVM) -->
<div class="card md-6">
      <div class="title"><b>German Conference on Medical Image Computing (BVM)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>March 15–17, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Lübeck, Germany</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.bvm-conf.org/">https://www.bvm-conf.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Long papers &amp; abstracts: October 24, 2025</li>
              <li>Author notifications: November 28, 2025</li>
              <li>Camera-ready: January 4, 2026</li>
              <li>BVM Award applications: January 31, 2026</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

<!-- COSYNE Workshops -->
<div class="card md-6" id="cosyne-workshops">
  <div class="title"><b>COSYNE Workshops</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual (part of <a href="#cosyne">COSYNE 2026</a>)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>March 16–17, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Hotel Cascais Miragem, Cascais, Portugal</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.cosyne.org/">cosyne.org</a></div></div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Call for workshop proposals open</li>
          <li>Submission deadline: October 23, 2025 (11:59 PM PST)</li>
          <li>Notifications: late November–early December 2025</li>
          <li>One free registration per organizer (others pay)</li>
          <li>Related main meeting: <a href="#cosyne">COSYNE 2026</a> (Lisbon Congress Centre), March 12–15, 2026</li>
        </ul>
      </div>
    </div>
  </div>
</div>

    <div class="card md-6">
      <div class="title"><b>European Sensory Science Society (E3S)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>March 26–27, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Zurich, Switzerland</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://e3sensory.eu/events/">https://e3sensory.eu/events/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Deadline: March 13, 2026</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="aps">
      <div class="title"><b>Association for Psychological Science (APS)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>April 17–19, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Barcelona, Spain</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.psychologicalscience.org/conventions/annual">https://www.psychologicalscience.org/conventions/annual</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Symposiums due: November 15, 2025</li>
              <li>Poster &amp; paper abstracts due: December 5, 2025</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

<div class="card md-6" id="modvis">
      <div class="title"><b>Models in Vision Science (MODVIS) Conference</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>May 2026 – TBD</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>St. Pete Beach, Florida</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.purdue.edu/conferences/events/modvis/">https://www.purdue.edu/conferences/events/modvis/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD 2026</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="arvo">
      <div class="title"><b>Association for Research in Vision and Ophthalmology (ARVO)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>May 3–7, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Colorado Convention Center, Denver, Colorado</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.arvo.org/annual-meeting/">https://www.arvo.org/annual-meeting/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Abstract deadline: December 5, 2025</li></ul></div></div>
      </div>
    </div>

<div class="card md-6">
  <div class="title"><b>SAND Challenge @ IEEE ICASSP 2026</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>May 4–8, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Barcelona, Spain (with ICASSP 2026)</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://sand.icar.cnr.it" aria-label="SAND Challenge website">sand.icar.cnr.it</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Registration &amp; dataset release:</b> September 1, 2025</li>
          <li><b>Submission closes:</b> November 20, 2025</li>
          <li><b>Results:</b> December 2, 2025</li>
          <li>Test set available; submissions open</li>
          <li>Top teams present at ICASSP 2026 (Barcelona); selected works in proceedings</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="card md-6">
  <div class="title"><b>IECMA 2026 – 3rd International Electronic Conference on Machines &amp; Applications</b></div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Biennial</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>May 12–14, 2026</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Online</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://sciforum.net/event/IECMA2026" aria-label="IECMA 2026 event page">sciforum.net – IECMA 2026</a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Abstract submission deadline: January 9, 2026</li>
          <li>Free registration deadline: May 6, 2026</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<div class="card md-6" id="modvis-2026"
     data-tags="conference workshop 2026 modvis vss satellite computational-models mathematical-models">
  <div class="title">
    <b>MODVIS 2026 – Models in Vision Satellite Workshop</b>
  </div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Annual VSS satellite workshop</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>May 13–14, 2026 (immediately before VSS 2026)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>St. Pete Beach, Florida, USA</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        Workshop on computational and mathematical models in vision, with longer, more interactive talks than
        typical VSS presentations and ample discussion time during coffee breaks and a long lunch.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Program</div>
      <div>
        Contributed oral presentations • Informal discussion periods • Keynote lecture by
        Jonathan Victor (Weill Cornell Medical College).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Abstracts &amp; Registration</div>
      <div>
        Abstract submission and registration are handled through the VSS website. Registration deadlines align
        with VSS; MODVIS abstract deadlines will be later than those for VSS, and limited post-deadline
        submissions may be accepted on a space-available basis.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://www.modvis.org" rel="noopener">modvis.org</a>
      </div>
    </div>

  </div>
</div>




<!-- CONFERENCE UPDATE: VSS 2026 — Vision Sciences Society -->
<div class="card md-6" id="vss" data-tags="conference 2026 vision-science vss symposia florida virtual hybrid">
  <div class="title"><b>VSS 2026 — Vision Sciences Society Annual Meeting</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>May 15 – 19, 2026</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>TradeWinds Island Resorts, St. Pete Beach, Florida · and Online</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Symposia</div>
      <div>
        November 20, 2025 – Symposium Submission Deadline <b>(closed)</b>
      </div>
    </div>

    <div class="meta-row">
  <div class="meta-label">Abstracts</div>
  <div>
    <b>December 10, 2025 – Abstract Submission Deadline (extended)</b><br>
    February 9, 2026 – Abstract Decision Notifications
  </div>
</div>


    <div class="meta-row">
      <div class="meta-label">Other Key Dates</div>
      <div>
        November 13, 2025 – T-shirt Design Competition Opens<br>
        November 20, 2025 – Satellite &amp; Social Event Applications Open<br>
        November 24, 2025 – VSS Award Nominations Open<br>
        December 16, 2025 – Registration Opens<br>
January 5, 2026 – Travel Grant Applications Open <br>
        January 9, 2026 – Satellite &amp; Social Event Applications Deadline<br>
        January 15, 2026 – T-shirt Design Submission Deadline<br>
        February 1, 2026 – Winning T-shirt Design Announced
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">New Features</div>
      <div>
        <ul class="notes-list">
          <li>Expanded online symposium series for members throughout the year, recorded and published in the <i>Journal of Vision</i>.</li>
          <li>Limited remote presentation options (talks and posters) for those unable to attend in person.</li>
        </ul>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">More Info</div>
      <div><a href="https://www.visionsciences.org/">visionsciences.org</a></div>
    </div>

  </div>
</div>

<!-- Conference: Applied Vision Science (AVS) Meeting -->
<div class="card md-6" data-tags="conference 2026 applied-vision vision-sciences avs vss satellite">
  <div class="title"><b>Applied Vision Science (AVS) Meeting</b></div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Annual (inaugural year in 2026)</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>May 20, 2026 (Wednesday following VSS 2026)</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>TradeWinds Island Grand, St. Pete Beach, Florida, USA</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Organizers</div>
      <div>Andrew Watson &amp; Laurie Wilcox (in collaboration with the Vision Sciences Society)</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://appliedvisionscience.org">appliedvisionscience.org</a><br>
        <a href="https://www.visionsciences.org/">visionsciences.org</a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Fees</div>
      <div>Student $50 • Postdoc $50 • Faculty $100 • Industry $200</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Important Dates</div>
      <div>
        December 16, 2025 – Registration Opens<br>
        <b>January 16, 2026 – Abstract Submission Deadline</b><br>
        <b>February 9, 2026 – Notification of Accepted Abstracts</b><br>
        <b>February 27, 2026 – Early Registration Deadline</b><br>
        May 20, 2026 – AVS 2026 Meeting
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Inaugural one-day meeting highlighting high-quality applied vision research.</li>
          <li>Program includes invited talks and poster sessions.</li>
          <li>Co-located with the Vision Sciences Society annual meeting.</li>
          <li>Focuses on the science underlying practical applications such as display technology, AR/VR/MR, HDR imaging, computational photography, UI/UX design, and computational graphics.</li>
        </ul>
      </div>
    </div>

  </div>
</div>

<!-- Conference: CRV 2026 -->
<div class="card md-6">
  <div class="title"><b>CRV 2026 — 23rd Conference on Robots and Vision</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>May 25–28, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Vancouver, British Columbia, Canada</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.computerrobotvision.org/" rel="noopener">computerrobotvision.org</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Submission</div>
      <div>
        <a href="https://openreview.net/group?id=computerrobotvision.org/CRV/2026/Conference" rel="noopener">
          OpenReview — CRV 2026
        </a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Important dates</div>
      <div>
        <ul class="notes-list">
          <li><b>Paper submission:</b> Feb 3, 2026</li>
          <li><b>Notifications:</b> Mar 27, 2026</li>
          <li><b>Camera-ready:</b> Apr 24, 2026</li>
        </ul>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Full, in-person conference with keynotes, oral presentations, posters.</li>
          <li>IEEE format; standalone 4–8 page submissions (references excluded).</li>
          <li>Original work only; no duplicate submissions.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="card md-6">
  <div class="title"><b>IEEE FG 2026 – International Conference on Automatic Face and Gesture Recognition</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>May 25–29, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Kyoto, Japan</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://fg2026.ieee-biometrics.org/" aria-label="IEEE FG 2026 conference page">fg2026.ieee-biometrics.org</a><br/>
        <a href="https://cmt3.research.microsoft.com/FG2026/" aria-label="IEEE FG 2026 submission portal">Submission portal</a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Round 1:</b> Abstract Sept 25, 2025 • Full Paper Oct 2, 2025 • Notification Dec 11, 2025</li>
          <li><b>Round 2:</b> Abstract Jan 9, 2026 • Full Paper Jan 15, 2026 • Notification Apr 2, 2026 • Camera-ready Apr 21, 2026</li>
          <li><b>Workshops:</b> Proposal deadline Nov 13, 2025</li>
          <li>Calls also open for tutorials and demos</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Conference — Athens Institute (ATINER) Psychology 2026 -->
<div class="card md-6">
  <div class="title"><b>20th Annual International Conference on Psychology</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>May 25–29, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Athens, Greece</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 21, 2025</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://www.atiner.gr/psychology" rel="noopener">atiner.gr/psychology</a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Organizer: Athens Institute for Education and Research (ATINER)</li>
          <li>Academic leads: Dr. Jim Clark (University of Winnipeg), Dr. Thanos Patelis (University of Kansas)</li>
          <li>Call for papers and participation details available on the ATINER site.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Conference: IASAS 2026 -->
<div class="card md-6" data-tags="conference synesthesia multisensory iasas perception cognition art psychology music design">
  <div class="title"><b>IASAS 2026 — Synesthesia: Interdisciplinary Research in a Multisensory World</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial Symposium</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>May 27–29, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>University of Amsterdam &amp; The Orgelpark, Amsterdam, NL</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="mailto:2026amsterdam@gmail.com">2026amsterdam@gmail.com</a></div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>December 1, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Notification</div><div>January 10, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Keynote</div><div>Professor Anina Rich (Macquarie University, Australia)</div></div>
    <div class="meta-row"><div class="meta-label">Chairs</div>
      <div>
        Tessa van Leeuwen (Tilburg University) &amp; Romke Rouw (University of Amsterdam)
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Topics</div>
      <div>
        Synaesthesia and multisensory integration · Synaesthetic art across media · Lived experiences of synaesthetes
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Hosted by the International Association of Synaesthetes, Artists, and Scientists (IASAS).</li>
          <li>Interdisciplinary symposium welcoming both research papers and creative works exploring synaesthetic experience.</li>
          <li>Abstracts ≤500 words; accepted formats: PDF, DOC, or RTF.</li>
          <li>Include name, affiliation, and contact details in submission.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="card md-6" id="etra-2026" data-tags="conference acm eye-tracking research applications hci cg it data-analysis gaze interaction perception cognition marrakech">
  <div class="title"><b>ACM Symposium on Eye Tracking Research &amp; Applications (ETRA 2026)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>June 1–4, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Marrakech, Morocco</div></div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>Premier international meeting bridging eye tracking research and applied domains across computer science, psychology, perception, and human–computer interaction. Hosted by ACM SIGCHI and SIGGRAPH.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Key Dates (AoE)</div>
      <div>
        <ul class="notes-list">
          <li><b>Abstract submission (mandatory):</b> <span class="status closed">CLOSED</span> — Nov 3, 2025 (AoE)</li>
          <li><b>Full paper submission:</b> November 10, 2025 (AoE)</li>
          <li><b>1st review notifications:</b> January 8, 2026</li>
          <li><b>Revisions due:</b> February 16, 2026</li>
          <li><b>2nd review notifications:</b> March 13, 2026</li>
          <li><b>Camera-ready:</b> March 30, 2026</li>
        </ul>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Ethics Requirement</div>
      <div>Each submission must include a brief privacy and ethics statement (2–3 sentences) addressing potential societal risks, fairness, or broader impacts of the work.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Publication</div>
      <div>Accepted papers appear in <i>PACM on CGIT</i> or <i>PACM on HCI</i> and are indexed in the ACM Digital Library.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Topics</div>
      <div>Eye-tracking systems, gaze prediction, visualization, perception and cognition, gaze-based interaction, and applied methods in real-world contexts.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://etra.acm.org/2026/">etra.acm.org/2026</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div>
      <div><a href="mailto:etra.conference@gmail.com">etra.conference@gmail.com</a> • Nour Aburaed (Publicity Chair)</div>
    </div>
  </div>
</div>

<div class="card md-6">
      <div class="title"><b>AREADNE 2026 (Research in Encoding and Decoding of Neural Ensembles)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>June 23–27, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Eliopoulos Conference Center, Milos, Greece</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://areadne.org">https://areadne.org</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="apcv">
  <div class="title"><b>EPC–APCV 2026 — Joint Meeting of the Australasian Society for Experimental Psychology &amp; Asia Pacific Conference on Vision</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Joint Meeting (Biennial)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>July 1–4, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>University of Auckland, New&nbsp;Zealand</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://visualneuroscience.auckland.ac.nz/epc-apcv-2026/">visualneuroscience.auckland.ac.nz/epc-apcv-2026</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Hosted jointly by the <b>Australasian Society for Experimental Psychology (EPC)</b> and the <b>Asia Pacific Conference on Vision (APCV)</b>.</li>
          <li><b>Member-initiated symposia:</b> 90-minute sessions (3–4 speakers + discussant); submission deadline <b>January 31, 2026</b>.</li>
          <li><b>General abstract submissions:</b> open January 2026, accepted on a rolling basis until April 2026.</li>
          <li>Topics welcome from all areas of experimental psychology and vision science.</li>
          <li>Contact: <a href="mailto:d.arnold@psy.uq.edu.au">d.arnold@psy.uq.edu.au</a></li>
          <li>Further details on keynotes and satellite workshops to follow.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Conference: Eurohaptics 2026 — Siena, Italy -->
<div class="card md-6"
  data-tags="conference 2026 haptics touch perception multisensory vr ar robotics interfaces biomechanics eurohaptics italy">

  <div class="title"><b>Eurohaptics 2026 — Siena, Italy</b></div>

  <div class="meta">

    <div class="meta-row"><div class="meta-label">Dates</div><div>July 6 – 9, 2026</div></div>

    <div class="meta-row"><div class="meta-label">Location</div><div>Siena, Italy</div></div>

    <div class="meta-row"><div class="meta-label">Themes</div>
      <div>
        Haptic perception · neuroscience of touch · tactile interfaces · sensors &amp; actuators · 
        teleoperation · VR/AR haptics · human-computer interaction · robotics · health &amp; wellness applications.
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Contribution Types</div>
      <div>
        Conference papers · Workshops · Published Research Track · Work-in-Progress · Hands-on Demonstrations.
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Deadlines</div>
      <div>
        Jan 19, 2026 – Conference Papers<br>
        Mar 12, 2026 – Workshop Proposals<br>
        Mar 15, 2026 – Published Research Track<br>
        Apr 12, 2026 – Work-in-Progress<br>
        Apr 16, 2026 – Demonstrations
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://2026.eurohaptics.org/">2026.eurohaptics.org</a></div>
    </div>

  </div>
</div>

<div class="card md-6" id="apa">
      <div class="title"><b>American Psychological Association (APA)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>August 6–8, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Washington, DC</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://convention.apa.org/attend/future-conventions">Future conventions</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="icvs">
      <div class="title"><b>International Colour Vision Society (ICVS) Conference</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>August 14-18, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Brighton, UK</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.icvs2026.org/" rel="noopener">icvs2026.org (local 2026 site)</a><br>
        <a href="https://icvs.info/meetings" rel="noopener">icvs.info/meetings (main ICVS page)</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<!-- Imaging & Perimetry Society (IPS) — Bern 2026 -->
<div class="card md-6" id="ips-bern-2026" data-tags="conference 2026 visual-field imaging perimetry ophthalmology bern switzerland">
  <div class="title">
    <b>Imaging &amp; Perimetry Society (IPS) — XXVIth International Visual Field &amp; Imaging Symposium</b>
  </div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Biennial meeting of the Imaging &amp; Perimetry Society (IPS)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>August 18–21, 2026</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Bern, Switzerland</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://www.ips-bern2026.org/" rel="noopener">ips-bern2026.org</a><br>
        <a href="http://www.perimetry.org" rel="noopener">perimetry.org – IPS Society site</a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Abstract deadline</div>
      <div>March 1, 2026 (23:59 UTC)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Scope</div>
      <div>
        Visual field assessment · ophthalmic imaging · technologies and methods for evaluating visual performance; aimed at researchers and clinicians at all career stages.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Support</div>
      <div>
        Travel grants and awards for students and early-career researchers (see conference website for details).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div>
        <a href="mailto:ips2026.bern@gmail.com">ips2026.bern@gmail.com</a>
      </div>
    </div>
  </div>
</div>


<div class="card md-6" id="spatial-cognition-2026" data-tags="conference spatial-cognition psychology neuroscience cs linguistics navigation">
  <div class="title"><b>Spatial Cognition 2026 — University of Glasgow</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>August 25–28, 2026</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>University of Glasgow, United Kingdom</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.gla.ac.uk/research/az/spatialreasoning/sc26/">Conference page</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        Hosted by the Turner Kirk Centre for Spatial Reasoning.  
        Focus on the acquisition, development, representation, and use of spatial knowledge in real, virtual, and hybrid environments by humans and artificial agents.  
        Interdisciplinary scope across cognitive & developmental psychology, computer science, linguistics, geography, cartography, philosophy, neuroscience, education, and robotics.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Program</div>
      <div>
        Single-track, refereed conference featuring invited talks, oral presentations, poster sessions, workshops, and symposia.  
        In-person only.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div><a href="mailto:jack.parkinson@glasgow.ac.uk">jack.parkinson@glasgow.ac.uk</a></div>
    </div>

  </div>
</div>

<div class="card md-6" data-tags="conference 2026 ai healthcare medical-imaging machine-learning imperial london">
  <div class="title"><b>AIiH 2026 — International Conference on Artificial Intelligence in Healthcare</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Annual (3rd edition)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>August 26–28, 2026</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Imperial College London, UK</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://aiih.cc">aiih.cc</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Call for Special Sessions</div>
      <div>
        Proposal deadline: <b>January 23, 2026</b> (minimum 5 presentations per session).<br>
        Proposals to: <a href="mailto:contact@aiih.cc">contact@aiih.cc</a><br>
        Accepted special sessions receive one free full registration; full papers appear in
        Springer <i>Lecture Notes in Computer Science</i> (LNCS).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Important Dates</div>
      <div>
        <ul class="notes-list">
          <li><b>Special session proposals:</b> January 23, 2026</li>
          <li><b>Full paper submission:</b> April 10, 2026</li>
          <li><b>Author notification:</b> May 25, 2026</li>
          <li><b>Author registration deadline:</b> June 16, 2026</li>
          <li><b>Early registration deadline:</b> July 13, 2026</li>
          <li><b>Main conference:</b> August 26–28, 2026 (Imperial College London)</li>
        </ul>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Keynotes</div>
      <div>
        Mihaela van der Schaar (University of Cambridge) • Alejandro Frangi (University of Manchester) •
        Huiru Zheng (Ulster University) • Michael Lones (Heriot-Watt University)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Tutorial</div>
      <div>
        Prof. Stephen Smith (University of York): “Translating AI in Clinical Practices”
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Organizers</div>
      <div>
        Conference Co-Chairs: Hao Ni (University College London) &amp; Xianghua Xie (Swansea University)<br>
        Clinicians &amp; Medical Students Engagement: Duaa Alim (Imperial College London)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Single-track conference with oral and poster presentations.</li>
          <li>Focus on AI methods tailored to the specific challenges of healthcare, from ethics to deployment.</li>
          <li>Full papers undergo double-blind peer review; short abstracts are archived online with DOIs.</li>
          <li>Previous editions hosted in Cambridge (2025) and earlier; participants from 27+ countries.</li>
        </ul>
      </div>
    </div>

  </div>
</div>


    <div class="card md-6">
      <div class="title"><b>European Working Memory Symposium (EWOMS)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>August 26–28, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Lyon, France</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.escop.eu/news/news/conference-news/ewoms-european-working-memory-symposium">ESCOP • EWOMS</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Abstract deadline: March 13, 2026</li>
              <li>Registration deadline: June 20, 2026</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

<!-- CONFERENCE: ECEM 2026 — European Conference on Eye Movements -->
<div class="card md-6" id="ecem2026" data-tags="conference europe eye-movements vision cognition neuroscience ulm germany 2026">
  <div class="title"><b>23rd European Conference on Eye Movements (ECEM 2026)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>August 30 – September 3 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Ulm University, Germany</div></div>
    <div class="meta-row"><div class="meta-label">Symposia Deadline</div><div>January 31 2026</div></div>
    <div class="meta-row"><div class="meta-label">Abstract / Poster Deadline</div><div>March 31 2026</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.uni-ulm.de/in/ecem2026/">uni-ulm.de/in/ecem2026</a> • <a href="https://www.conftool.com/ecem2026/">Submit via Conftool</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>World’s largest meeting devoted to eye-movement research, uniting neuroscience, psychology, linguistics, computer science, and applied domains such as XR, education, and clinical vision science.</div>
    </div>
  </div>
</div>

<div class="card md-6" id="gruppo-del-colore">
    <div class="title"><b>Gruppo del Colore — Associazione Italiana Colore (Annual Meeting)</b></div>
<div class="meta">
  <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
  <div class="meta-row"><div class="meta-label">Dates</div><div>September 3–4, 2026</div></div>
  <div class="meta-row"><div class="meta-label">Location</div><div>Florence, Italy</div></div>
  <div class="meta-row"><div class="meta-label">Website</div>
    <div><a href="https://www.aic2026.org/welcome-message/">aic2026.org/welcome-message</a></div>
  </div>
  <div class="meta-row"><div class="meta-label">Deadline</div><div>Abstracts due December 15, 2025</div></div>
  <div class="meta-row"><div class="meta-label">Notes</div>
    <div>
      <ul class="notes-list">
        <li>Co-located with AIC 2026; joint Florence dates and venue.</li>
      </ul>
    </div>
  </div>
</div>
    </div>

<!-- Conference: AIC 2026 — International Colour Association -->
<div class="card md-6" id="aic2026" data-tags="conference color aic florence italy 2026">
  <div class="title"><b>AIC 2026 — International Colour Association</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>September 3–4, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Florence, Italy</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.aic2026.org/welcome-message/">aic2026.org/welcome-message</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Abstracts due December 15, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Co-located with Gruppo del Colore – Associazione Italiana Colore.</li>
          <li>Applied &amp; basic colour/vision topics; strong overlap with the CVNet community.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- CONFERENCE: Progress in Colour Studies (PICS 2026) -->
<div class="card md-6" id="pics2026" data-tags="conference colour-studies interdisciplinary linguistics humanities poland 2026">
  <div class="title"><b>Progress in Colour Studies (PICS 2026)</b></div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Recurring conference in the Progress in Colour Studies series</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>September 10–12, 2026</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Wrocław, Poland</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://phc.uni.wroc.pl/PICS2026/">phc.uni.wroc.pl/PICS2026</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Key Dates</div>
      <div>
        <ul class="notes-list">
          <li><b>Abstract submission:</b> February 28, 2026</li>
          <li><b>Notification of acceptance:</b> April 15, 2026</li>
          <li><b>Registration &amp; payment:</b> April 30 – May 31, 2026</li>
          <li><b>Conference:</b> September 10–12, 2026</li>
          <li><b>Paper submission for publication:</b> February 28, 2027</li>
        </ul>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Chairs</div>
      <div>Adam Pawłowski &amp; Danuta Stanulewicz (University of Wrocław / University of Gdańsk)</div>
    </div>
  </div>
</div>

<div class="card md-6">
      <div class="title"><b>Asian Conference on Computer Vision (ACCV)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>December 8–12, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://accv2022.org/">https://accv2022.org/</a> (most recent edition; 2026 site TBD)</div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

    <!-- 2026 TBD / Unknown Dates -->
    
<div class="card md-6">
  <div class="title"><b>Trieste “Spring Congressino”</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual (Spring)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Trieste, Italy</div></div>
   <div class="meta-row">
  <div class="meta-label">Website</div>
  <div><a href="https://portale.units.it/en/events/calendar">https://portale.units.it/en/events/calendar</a></div>
</div>
    <div class="meta-row"><div class="meta-label">Notes</div><div>
      <ul class="notes-list">
      </ul>
    </div></div>
  </div>
</div>

<div class="card md-6" id="optica-fall-vision">
      <div class="title"><b>Optica Fall Vision Meeting</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.osafallvisionmeeting.org/">https://www.osafallvisionmeeting.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Submissions TBD</li></ul></div></div>
      </div>
    </div>
    
<div class="card md-6">
      <div class="title"><b>ACM Symposium on Applied Perception (SAP)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://sap.acm.org/">https://sap.acm.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="ava">
      <div class="title"><b>Applied Vision Association (AVA)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Various</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Brisbane, Australia</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.theava.net/meetings.php">https://www.theava.net/meetings.php</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>International Multisensory Research Forum (IMRF)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://imrf2025.sciencesconf.org/">https://imrf2025.sciencesconf.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="sfn">
  <div class="title"><b>Society for Neuroscience (SfN)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>

    <!-- Last known meeting info -->
    <div class="meta-row"><div class="meta-label">Next Meeting</div>
      <div>TBD 2026</div>
    </div>

    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.sfn.org/meetings">https://www.sfn.org/meetings</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Official 2026 dates and location have not yet been announced.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

    <div class="card md-6">
      <div class="title"><b>16th Pangborn Sensory Science Symposium</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.pangbornsymposium.com/">https://www.pangbornsymposium.com/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="vsac">
      <div class="title"><b>Visual Science Art Conference (VSAC)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://2025.vsac.eu/">https://2025.vsac.eu/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="ecvp">
      <div class="title"><b>European Conference on Visual Perception (ECVP)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Bournemouth, UK</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="http://ecvp.org/future.html">http://ecvp.org/future.html</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>European Society for Cognitive Psychology (ESCOP)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.escop2025.com/">https://www.escop2025.com/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>EuroGraphics 3D Object Retrieval Workshop (3DOR)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><em>TBD</em></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Official site/link pending</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="bavrd">
      <div class="title"><b>Bay Area Vision Research Day (BAVRD)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>International House, UC Berkeley, Berkeley, CA</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://vision.berkeley.edu/events/bavrd">https://vision.berkeley.edu/events/bavrd</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list"><li>TBD</li></ul>
          </div>
        </div>
      </div>
    </div>


<!-- Conference: ACM SUI 2025 → roll forward -->
<div class="card md-6" data-tags="conference hci spatial interaction acm sui montreal">
  <div class="title"><b>ACM Symposium on Spatial User Interaction (SUI)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://sui.acm.org/2025/overview/">Program Overview</a> •
        <a href="https://sui.acm.org/2025/keynotes/">Keynotes</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Keynotes</div><div>TBD</div></div>
    <div class="meta-row"><div class="meta-label">Notes</div><div>TBD</div></div>
  </div>
</div>

    <div class="card md-6">
  <div class="title"><b>Fechner Day (International Society for Psychophysics)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://fechner.fonae.org/">fechner.fonae.org</a></div></div>
    <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
  </div>
</div>

    <div class="card md-6">
      <div class="title"><b>Color and Imaging Conference (CIC 33)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.imaging.org/IST/IST/Conferences/CIC/CIC2025/CIC_Home.aspx">imaging.org – CIC 2025</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Submissions TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
  <div class="title"><b>SCAIR — Southern California AI &amp; Robotics Symposium</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="http://www.scair2025.org/">scair2025.org</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>TBD</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="card md-6">
  <div class="title"><b>OPAM — Annual Workshop on Object Perception, Attention, and Memory</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual Workshop (satellite to Psychonomics)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.opamconference.com/">opamconference.com</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          TBD
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="card md-6" id="psychonomics">
      <div class="title"><b>Psychonomic Society Annual Meeting</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.psychonomic.org/">https://www.psychonomic.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6">
      <div class="title"><b>Asian Conference on Pattern Recognition (ACPR)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2027</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.mva-org.jp/acpr2025/">https://www.mva-org.jp/acpr2025/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

   <div class="card md-6">
      <div class="title"><b>7th Instance-Level Recognition and Generation Workshop (ILR) @ ICCV</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial (with ICCV)</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2027</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://ilr-workshop.github.io/ICCVW2025/">https://ilr-workshop.github.io/ICCVW2025/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>


  </div><!-- /cards (Conferences) -->



<!-- Online Seminars/Clubs -->
<h3 id="talks">Online Seminars/Clubs</h3>
<p class="section-desc">
  Recurring journal clubs and seminar series open to the community, plus selected online webinars and workshops.
</p>

<div class="cards">

  <!-- RECURRING & ONGOING SERIES -->

  <div class="card md-6">
    <div class="title"><b>Intermural Colour Club</b></div>
    <div class="meta">
      <div class="meta-row">
        <div class="meta-label">Type</div>
        <div>Online journal club</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Schedule</div>
        <div>Ongoing in 2025 (see schedule document)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Location</div>
        <div>Online</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Schedule &amp; Info</div>
        <div>
          <a href="https://docs.google.com/document/d/1LH_FEBCwvE0z8OuWMLs7e_OVKg1VB2vwleyXzHHug44/edit?tab=t.0">
            Journal Club Schedule 2025
          </a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Contact</div>
        <div>
          Christoph Witzel — <a href="mailto:witzel@daad-alumni.de">witzel@daad-alumni.de</a><br>
          Ilgin Cebioglu — <a href="mailto:I.Cebioglu@newcastle.ac.uk">I.Cebioglu@newcastle.ac.uk</a>
        </div>
      </div>
    </div>
  </div>

  <div class="card md-6">
    <div class="title"><b>Optica Color Technical Group Webinar Series</b></div>
    <div class="meta">
      <div class="meta-row">
        <div class="meta-label">Type</div>
        <div>Online webinar series</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Schedule</div>
        <div>Ongoing</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Location</div>
        <div>Online (hosted by Optica)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div>
          <a href="https://opg.optica.org/ao/colorgroup.cfm">Optica Color Technical Group</a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Focus</div>
        <div>
          Recurring talks on color science, vision, and display technology (psychophysics, XR, perception, color imaging, etc.).
        </div>
      </div>
    </div>
  </div>

  <div class="card md-6">
    <div class="title"><b>Current Topics in Sleep &amp; Circadian Health — TSCN (Online Seminar Series)</b></div>
    <div class="meta">
      <div class="meta-row">
        <div class="meta-label">Type</div>
        <div>Online seminar series</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Dates</div>
        <div>October 2025 – January 2026</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Location</div>
        <div>Online (Zoom)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div>
          <a href="https://www.tscnlab.org/">
            Translational Sensory &amp; Circadian Neuroscience Unit (TSCN)
          </a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Themes</div>
        <div>
          Light &amp; visual exposure logging • Sleep technology &amp; digital health • Clocks beyond 24 hours • Sleep &amp; women’s health.
        </div>
      </div>
    </div>
  </div>

  <!-- WEBINARS & ONE-OFF WORKSHOPS (2025) -->

  <div class="card md-6">
    <div class="title"><b>OPAM Online Workshop — Preregistration &amp; Registered Reports</b></div>
    <div class="meta">
      <div class="meta-row">
        <div class="meta-label">Date</div>
        <div>October 8, 2025 • 10:00 AM ET</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Location</div>
        <div>Online (recording available after)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Speaker</div>
        <div>Dr. Caro Hautikiet (VU Amsterdam)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Details</div>
        <div>
          <a href="https://www.opamconference.com/online-workshops">Workshop page</a> •
          <a href="https://docs.google.com/forms/d/e/1FAIpQLSe09feybP_GYvo2NrTOzYrTcFxaQnt25pn-b6DnUls9PHccIg/viewform?usp=header">
            Sign-up form
          </a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>Part of OPAM33 (satellite to Psychonomics); focus on improving credibility &amp; reproducibility in research.</div>
      </div>
    </div>
  </div>

  <div class="card md-6">
    <div class="title"><b>JIVP Webinar — Trends in Image Aesthetics Assessment</b></div>
    <div class="meta">
      <div class="meta-row">
        <div class="meta-label">Speaker</div>
        <div>Prof. Leida Li (Xidian University)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Date</div>
        <div>October 9, 2025</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Time</div>
        <div>12:00–13:00 UTC • 14:00–15:00 CEST • 05:00–06:00 PDT</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Location</div>
        <div>Online (Cassyni)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div>
          <a href="https://cassyni.com/events/X5JtSQaWByayBdTbPQZEfX" rel="noopener">
            RSVP / watch on Cassyni
          </a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Host</div>
        <div>EURASIP Journal on Image and Video Processing (JIVP) Webinar Series</div>
      </div>
    </div>
  </div>

  <div class="card md-6">
    <div class="title"><b>Optica Vision Technical Group Webinar — The Global Myopia Crisis</b></div>
    <div class="meta">
      <div class="meta-row">
        <div class="meta-label">Date</div>
        <div>October 14, 2025 • 12:00–13:00 EDT (UTC−04:00)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Location</div>
        <div>Online (Optica Vision Technical Group)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Speaker</div>
        <div>Machelle T. Pardue (Emory University; Atlanta VA)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div>
          <a href="https://www.optica.org/events/webinar/2025/10_october/the_global_myopia_crisis_insights_from_the_2024_nasem_report/">
            Event page
          </a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>Summary of the 2024 NASEM consensus report on the global myopia crisis.</div>
      </div>
    </div>
  </div>

  <div class="card md-6">
    <div class="title"><b>BIU Vision Science Online Seminar — Prof. MiYoung Kwon</b></div>
    <div class="meta">
      <div class="meta-row">
        <div class="meta-label">Speaker</div>
        <div>Prof. MiYoung Kwon (Northeastern University)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Title</div>
        <div>
          From retinal encoding to oculomotor adaptation: How the human visual system adapts to degraded sensory information
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Date</div>
        <div>November 18, 2025</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Time (global)</div>
        <div>3pm Israel • 2pm Central Europe • 1pm UK • 8am Eastern (US/Canada) • 22:00 Tokyo • 23:59 Melbourne/Sydney</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Location</div>
        <div>
          Online (Zoom) —
          <a href="https://biu-ac-il.zoom.us/j/82928146713?pwd=Wa7RxsDNR1vErbuXaMYOZi100kp2V5.1" target="_blank" rel="noopener">
            Join via Zoom
          </a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Series</div>
        <div>
          BIU Vision Science Seminar (Bar-Ilan University) •
          <a href="https://www.worldwideneuro.com/seminar-series.html?name=BIU_Vision_Science" target="_blank" rel="noopener">
            Series page
          </a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Recordings</div>
        <div>
          <a href="https://www.youtube.com/channel/UCnZQ6jqzktLbeoEJgufKg6Q/videos" target="_blank" rel="noopener">
            BIU Vision Science YouTube channel
          </a>
        </div>
      </div>
    </div>
  </div>

</div>


 <!-- Publications -->
<h3 id="calls">Special/Feature Issues</h3>

<p class="section-desc">
  <a href="./journals%20list.html">Click here</a> for a comprehensive list of relevant journals,
  complete with website information, type of journal, open access status, and more.
</p>

<p class="section-desc">
  For special or feature issues, we try to keep up to date with the following journals:
  Attention, Perception &amp; Performance; Color Research and Application; IOVS; JOSA A/B; JOV;
  Multisensory Perception; Perception; and Vision Research. We also provide permanent links for
  some journals&rsquo; special or feature issues. If you have suggestions for journals or calls we
  should track, or would like to share information about a special issue, please contact us.
</p>


<div class="cards">

<!-- Visual Cognition — Special Issue -->
<div class="card md-6">
  <div class="title">
    <b>Visual Cognition — Special Issue: The Control of Visual Attention</b>
  </div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Deadline</div><div><b>December 1, 2025</b></div></div>
    <div class="meta-row"><div class="meta-label">Theme</div>
      <div>Honoring the contributions of Charles Folk & Roger Remington; attentional control and orienting mechanisms.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Guest Editors</div>
      <div>Andrew Leber · Brad Gibson · Brian Anderson</div>
    </div>
    <div class="meta-row"><div class="meta-label">Link</div>
      <div><a href="https://think.taylorandfrancis.com/special_issues/the-control-of-visual-attention-in-honor-of-charles-folk-roger-remington/">Special Issue Details</a></div>
    </div>
  </div>
</div>

<!-- Journal of Vision — Special Issue -->
<div class="card md-6">
  <div class="title"><b>Journal of Vision — Interface Between Vision and Language</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Deadline</div><div>December 31, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Scope</div>
      <div>Interactions between visual and language systems; reading; sign language; literacy; cortical specialization.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Link</div>
      <div><a href="https://jov.arvojournals.org/ss/visionlanguageinterface.aspx">Special Issue Page</a></div>
    </div>
  </div>
</div>

<!-- Nature Portfolio — Rhythmic Cognition (Closed) -->
<div class="card md-6">
  <div class="title"><b>Rhythmic Cognition — Cross-Journal Collection (CLOSED)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Journals</div>
      <div>Communications Psychology · Nature Communications · Scientific Reports</div>
    </div>
    <div class="meta-row"><div class="meta-label">Scope</div>
      <div>Rhythmic attention, neural entrainment, oscillations, rhythmic perception & decision-making.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Link</div>
      <div><a href="https://www.nature.com/collections/jhcjbjgfgj/about-this-collection">Collection Page</a></div>
    </div>
  </div>
</div>

<!-- BMC Biology — Serial Dependence -->
<div class="card md-6">
  <div class="title"><b>BMC Biology — Integrative and Translational Serial Dependence</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 11, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Link</div>
      <div><a href="https://www.biomedcentral.com/collections/itsd">Collection Page</a></div>
    </div>
  </div>
</div>

<!-- Optics Express — 3D Image Acquisition & Display -->
<div class="card md-6">
  <div class="title"><b>Optics Express — 3D Image Acquisition and Display (2025)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Deadline</div><div>November 30, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Link</div>
      <div><a href="https://opg.optica.org/content/feature/announcement/item/oe-3d-image-acquisition-and-display-2025">Feature Issue</a></div>
    </div>
  </div>
</div>

<!-- COSI 2025 — Applied Optics / BOE / OE -->
<div class="card md-6">
  <div class="title"><b>COSI 2025 — Computational Optical Sensing and Imaging</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Deadline</div><div>December 1, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Journals</div>
      <div>Applied Optics · Biomedical Optics Express · Optics Express</div>
    </div>
    <div class="meta-row"><div class="meta-label">Link</div>
      <div><a href="https://opg.optica.org/content/feature/announcement/item/oe-cosi2025">Feature Issue</a></div>
    </div>
  </div>
</div>

<!-- JOSA A & B — Structured Light / Optics in South Asia -->
<div class="card md-6">
  <div class="title"><b>JOSA A / JOSA B — Feature Issues</b></div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Deadlines</div>
      <div>
        Spatiotemporal Structured Light → <b>December 15, 2025</b><br>
        Optics in South Asia → <b>January 15, 2026</b>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Link</div>
      <div><a href="https://opg.optica.org/josaa/feature_issues.cfm">Feature Issues</a></div>
    </div>
  </div>
</div>

<!-- Color Research & Application — Facial Appearance -->
<div class="card md-6">
  <div class="title"><b>Color Research & Application — Facial Appearance Measurement & Perception</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Status</div><div>Rolling (ongoing collection)</div></div>
    <div class="meta-row"><div class="meta-label">Link</div>
      <div><a href="https://onlinelibrary.wiley.com/doi/toc/10.1002/%28ISSN%291520-6378.facial-appearance">Collection Page</a></div>
    </div>
  </div>
</div>

<!-- IOVS — Nystagmus (Closed) -->
<div class="card md-6">
  <div class="title"><b>IOVS — Nystagmus Special Issue (Closed)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Status</div><div>CLOSED</div></div>
    <div class="meta-row"><div class="meta-label">Link</div>
      <div><a href="https://iovs.arvojournals.org/special-issues/nystagmus">Special Issue Page</a></div>
    </div>
  </div>
</div>

</div>

  
<!-- Courses -->
<h3 id="education">Education</h3>
<p class="section-desc">
  Workshops, classes, and summer schools to build skills in perception, imaging, analysis, and research methods.
</p>

<!-- SUMMER SCHOOLS -->
<h4>Summer Schools</h4>
<div class="cards">

  <!-- ANAIS 2025 -->
  <div class="card md-6">
    <div class="title"><b>Annual Nepal AI School (ANAIS 2025)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>December 29, 2025 – January 8, 2026</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Kathmandu, Nepal</div></div>
      <div class="meta-row"><div class="meta-label">Apply by</div><div><b>October 16, 2025</b></div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://anais.naamii.org.np/" rel="noopener">anais.naamii.org.np</a></div></div>
      <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:anais@naamii.org.np">anais@naamii.org.np</a></div></div>
      <div class="meta-row"><div class="meta-label">Notes</div>
        <div><ul class="notes-list"><li>11-day program on AI foundations & applications.</li></ul></div>
      </div>
    </div>
  </div>

  <!-- Visual Neuroscience: Spikes to Awareness -->
  <div class="card md-6">
    <div class="title"><b>Summer School — Visual Neuroscience: from spikes to awareness</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>August 30 – September 11, 2026</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Rauischholzhausen Castle (near Frankfurt), Germany</div></div>
      <div class="meta-row"><div class="meta-label">Category</div><div>Summer school / advanced training</div></div>
      <div class="meta-row"><div class="meta-label">Application deadline</div><div>March 8, 2026</div></div>
      <div class="meta-row"><div class="meta-label">Website</div>
        <div><a href="https://www.allpsych.uni-giessen.de/rauisch/" target="_blank" rel="noopener">https://www.allpsych.uni-giessen.de/rauisch/</a></div>
      </div>
      <div class="meta-row"><div class="meta-label">Tuition</div><div>1500 € (room & meals)</div></div>
    </div>
    <p class="card-notes">
      Advanced European program for late PhDs and early postdocs combining psychophysics, computation, theory, and neural methods.
    </p>
  </div>

  <!-- ESSVN 2026 -->
  <div class="card md-6">
    <div class="title"><b>European Summer School in Visual Neuroscience (ESSVN 2026)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>August 30 – September 11, 2026</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>TBD (European venue)</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.visualneuroscience.eu">visualneuroscience.eu</a></div></div>
      <div class="meta-row"><div class="meta-label">Overview</div><div>Selective advanced training in visual neuroscience with lectures, workshops, and projects.</div></div>
      <div class="meta-row"><div class="meta-label">Eligibility</div><div>Graduate students & early postdocs.</div></div>
      <div class="meta-row"><div class="meta-label">Application</div><div>2026 details forthcoming.</div></div>
    </div>
  </div>

  <!-- Systems Vision Science -->
  <div class="card md-6">
    <div class="title"><b>Systems Vision Science Summer School and Symposium</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>August 2026 (dates TBA)</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Tübingen, Germany</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://summerschool.lizhaoping.org/">summerschool.lizhaoping.org</a></div></div>
      <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>2026 details forthcoming.</li></ul></div></div>
    </div>
  </div>

  <!-- Eastern European ML Summer School -->
  <div class="card md-6">
    <div class="title"><b>Eastern European Machine Learning Summer School (EEML)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.eeml.eu/application">eeml.eu/application</a></div></div>
      <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
    </div>
  </div>

  <!-- ICVS Summer School 2027 -->
  <div class="card md-6">
    <div class="title"><b>International Colour Vision Society (ICVS) Summer School</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2027</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Pembroke College, Oxford, UK</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.icvs.info/index.php/summer-school">icvs.info/summer-school</a></div></div>
      <div class="meta-row"><div class="meta-label">Notes</div>
        <div><ul class="notes-list"><li>Includes 2026 Verriest Medal (Prof. Keiji Uchikawa).</li></ul></div>
      </div>
    </div>
  </div>

</div>


<!-- WORKSHOPS -->
<h4>Workshops</h4>
<div class="cards">


  <!-- Why Are There Neuroscientists -->
  <div class="card md-6">
    <div class="title"><b>Why are there neuroscientists? — Workshop of Ideas in Neuroscience</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>December 5, 2025</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Heidelberg, Germany</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://openlabhd.org/why/">openlabhd.org/why</a></div></div>
      <div class="meta-row"><div class="meta-label">Notes</div>
        <div><ul class="notes-list"><li>Short talks, discussion, values in neuroscience.</li></ul></div>
      </div>
    </div>
  </div>

</div>


<!-- CLASSES -->
<h4>Classes</h4>
<div class="cards">

  
  <!-- Eye Tracking -->
  <div class="card md-6">
    <div class="title"><b>A Practical Introduction to Eye Tracking</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>November 26–28, 2025</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Lund, Sweden</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.humlab.lu.se/education/commissioned-education/">Course page</a></div></div>
    </div>
  </div>

  <!-- LightLogR -->
  <div class="card md-6">
    <div class="title"><b>LightLogR: Open & Reproducible Analysis of Light Exposure Data</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div>
        <div>Beginner: March 4, 2026 • Advanced: December 9, 2025 & May 6, 2026</div>
      </div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Online</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://tum-conf.zoom.us/webinar/register/WN_xGoHi1i_Sz2flC6-qGcLoQ#/registration">Registration</a></div></div>
    </div>
  </div>

  <!-- MSc Research Methods -->
  <div class="card md-6">
    <div class="title"><b>MSc in Research Methods in Experimental Psychology</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>2026 TBD</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Universidad Autónoma de Madrid (UAM), Spain</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.uam.es/uam/inicio">uam.es</a></div></div>
    </div>
  </div>

</div>






  
<h3 id="gradprograms">Grad Programs</h3>
<p class="section-desc">
  Doctoral and Masters opportunities. Includes dedicated PhD and MSc programs submitted by members.
</p>

<div class="cards">

<!-- PhD Position — Jozwik Lab, University of Cambridge -->
<div class="card md-6" data-tags="phd-position 2025 2026 cambridge visual-cognition computational-neuroscience neuroai mrc-cbu">
  <div class="title"><b>4-Year PhD in Visual Cognitive Computational Neuroscience — University of Cambridge (Jozwik Lab)</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Application Deadline</div>
      <div>December 2, 2025</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Start Date</div>
      <div>October 2026</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Institution</div>
      <div>MRC Cognition and Brain Sciences Unit &amp; University of Cambridge</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Supervision</div>
      <div>Kamila Maria Jóźwik (Royal Society University Research Fellow, Assistant Research Professor)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Project Title</div>
      <div>
        <i>Disentangling and modelling behaviourally-relevant visual and semantic dimensions of visual cognition in the human brain</i>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Research Areas</div>
      <div>
        Visual and semantic representations • Animacy perception • Large-scale fMRI/MEG datasets • Mobile EEG &amp; VR •
        Deep learning (topographical DNNs, multimodal models, LLMs) • NeuroAI • Behavioural modelling • Ecologically valid vision experiments
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Project Aims</div>
      <div>
        <ol class="notes-list">
          <li>Characterise behaviourally-relevant visual &amp; semantic dimensions using neural and behavioural datasets with AI models.</li>
          <li>Define and model animacy-related dimensions using videos (behaviour, fMRI, MEG).</li>
          <li>Examine how these representations change during naturalistic immersion (mobile EEG, VR).</li>
        </ol>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Ideal Candidate</div>
      <div>
        Strong programming (Python/Matlab) • Experience with neuroimaging (fMRI / M/EEG) • Machine learning background •
        Highly motivated, curious, with prior research experience.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Application Link</div>
      <div><a href="https://www.mrc-cbu.cam.ac.uk/study-with-us/research-degrees/how-to-apply/">Apply via MRC CBU — Cambridge</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div>
        <a href="mailto:jozwik.kamila@gmail.com">jozwik.kamila@gmail.com</a> • kj287@cam.ac.uk
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Funding</div>
      <div>
        Fully funded (UK Home students). International students encouraged to apply — automatically considered for Cambridge studentships.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        Jozwik Lab integrates cognitive science, neuroscience, AI, and NeuroAI; uses fMRI, MEG, EEG, mobile EEG, VR, ultrasound stimulation, and deep learning models.
        The PI will also be recruiting a postdoc for October 2026 in a similar research area.
      </div>
    </div>

  </div>
</div>


<!-- Penn ITCN -->
<div class="card md-6">
  <div class="title"><b>Interdisciplinary Training in Computational Neuroscience (ITCN) — University of Pennsylvania</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>PhD-level interdisciplinary training integrating computational methods with experimental neuroscience.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Deadlines</div>
      <div>December 1, 2025 (Neuroscience, Psychology, Physics) • December 15, 2025 (Bioengineering)</div>
    </div>
    <div class="meta-row"><div class="meta-label">Participation</div>
      <div>Students apply to a home PhD program first, then apply to ITCN before or during Year 1.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Application</div><div>Submit materials to Vijay Balasubramanian, Maria Geffen, and Joshua Gold.</div></div>
    <div class="meta-row"><div class="meta-label">Contact</div><div>mgeffen@pennmedicine.upenn.edu</div></div>
  </div>
</div>

<!-- Lingnan University — Eye Movements to Faces -->
<div class="card md-6">
  <div class="title"><b>PhD — Eye Movements to Faces (Lingnan University, Hong Kong)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">PI</div><div>Prof. Will Hayward</div></div>
    <div class="meta-row"><div class="meta-label">Lab</div><div>Lingnan Visual Cognition Lab</div></div>
    <div class="meta-row"><div class="meta-label">Project</div>
      <div>Funded doctoral project on idiosyncratic eye-movement patterns to faces using eye-tracking and advanced analytics.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Preferred Skills</div>
      <div>Eye-tracking; PsychoPy/PsychToolbox; strong quantitative skills.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>December 1, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>September 2026</div></div>
    <div class="meta-row"><div class="meta-label">Funding</div>
      <div>Hong Kong PhD Fellowship Scheme (HKPFS)</div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:willhayward@ln.edu.hk">willhayward@ln.edu.hk</a></div></div>
  </div>
</div>

<!-- Birmingham NeuroAI -->
<div class="card md-6">
  <div class="title"><b>PhD Positions in NeuroAI — University of Birmingham (UK)</b></div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Institution</div>
      <div>School of Computer Science &amp; Centre for Human Brain Health, University of Birmingham</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Areas</div>
      <div>Computational neuroscience • neural dynamics • neural network modeling • motor sequence planning.</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Featured Project Deadline</div>
      <div>November 27, 2025</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Other Openings</div>
      <div>Additional funded PhD projects (rolling).</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Apply</div>
      <div><a href="https://warwick.ac.uk/fac/cross_fac/mibtp/phd/supervisors/jliu">Application page</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div>jiankliu@gmail.com</div>
    </div>
  </div>
</div>


<!-- Giessen × MPI — Computational Cognitive Neuroscience -->
<div class="card md-6">
  <div class="title"><b>PhD — Computational Cognitive Neuroscience (JLU Giessen × MPI CBS)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Institution</div>
      <div>JLU Giessen (Hebart Lab) with collaboration at Max Planck Institute CBS</div>
    </div>
    <div class="meta-row"><div class="meta-label">Cluster</div><div>The Adaptive Mind</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>November 25, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>January 2026 or ASAP</div></div>
    <div class="meta-row"><div class="meta-label">Requirements</div><div>Strong Python skills</div></div>
    <div class="meta-row"><div class="meta-label">Apply</div>
      <div><a href="https://www.uni-giessen.de/de/ueber-uns/karriere/stellenangebote/wissenschaftliche-mitarbeiter/531-11-e">Official posting</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div><div>martin.hebart@gmail.com</div></div>
  </div>
</div>

<!-- NYU Visual Sciences -->
<div class="card md-6">
  <div class="title"><b>Doctoral Studies in Visual Sciences — New York University</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Participating Departments</div>
      <div>CNS • Psychology • Data Science • Computer Science • Mathematics • Biology • Philosophy</div>
    </div>
    <div class="meta-row"><div class="meta-label">Deadlines</div>
      <div>Dec 1–Dec 18, 2025 (varies by department) • Philosophy Jan 7, 2026</div>
    </div>
    <div class="meta-row"><div class="meta-label">Faculty Highlights</div>
      <div>Eero Simoncelli, Marisa Carrasco, David Heeger, Yann LeCun, Rob Fergus, and others.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://as.nyu.edu/cns/DoctoralProgram.html">NYU CNS</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div><div>eero.simoncelli@nyu.edu</div></div>
  </div>
</div>

<!-- UMass Boston -->
<div class="card md-6">
  <div class="title"><b>PhD in Developmental & Brain Sciences — UMass Boston</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Deadline</div><div>December 1, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Boston, MA (USA)</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.umb.edu/academics/program-finder/developmental-brain-sciences-phd/">Program page</a></div>
    </div>
  </div>
</div>

<!-- University of Houston — Vision Science -->
<div class="card md-6">
  <div class="title"><b>Graduate Program in Physiological Optics & Vision Science — University of Houston</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Degrees</div><div>PhD & MS</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>January 31, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.opt.uh.edu/education/graduate-programs/">Program overview</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Funding</div><div>Tuition fellowships and TA/RA support</div></div>
  </div>
</div>

<!-- UMass Amherst — Cognitive & Neural Development -->
<div class="card md-6">
  <div class="title"><b>PhD Position — Cognitive & Neural Development Lab (UMass Amherst)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Start</div><div>Fall 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Amherst, MA</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.umass.edu/pbs/blaser-lab">Blaser Lab</a></div>
    </div>
  </div>
</div>

<!-- Iceland Vision Science — TBD -->
<div class="card md-6">
  <div class="title"><b>PhD or Postdoctoral Position — Vision Science (University of Iceland)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>TBD</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Reykjavik, Iceland</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://visionscience.com/pipermail/visionlist_visionscience.com/2025/009473.html">VisionList post</a></div>
    </div>
  </div>
</div>

</div>


  <!-- Jobs -->
  <h3 id="jobs">Jobs</h3>
  <p class="section-desc">
    PhD, postdoc, RA, and faculty positions in vision, perception, neuroscience, imaging, and allied areas.
  </p>

  <!-- ========================= -->
  <!-- Jobs & Student Positions  -->
  <!-- ========================= -->
  <h4>Jobs and Student Positions</h4>
  <div class="cards">




<div class="card md-6"
     data-tags="job faculty research-fellow senior-fellow ocular-immunology anterior-eye clinical-translation melbourne australia 2026">
  <div class="title">
    <b>Research Fellow / Senior Research Fellow – Ocular Immunology &amp; Clinical Translation</b>
  </div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Institution</div>
      <div>
        Department of Optometry and Vision Sciences, University of Melbourne (Australia)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Unit</div>
      <div>
        Anterior Eye, Clinical Trials and Research Translation Unit (Downie Laboratory)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Role Type</div>
      <div>Full-time; fixed-term for 3 years</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Salary</div>
      <div>
        Level B: AUD $124,656–$148,023 p.a. (pro rata)<br>
        Level C: AUD $152,695–$176,065 p.a. (pro rata)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Focus</div>
      <div>
        Anterior eye and ocular immunology, particularly uveitis; development and validation of new biomarkers
        and translational research to improve patient care.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Responsibilities</div>
      <div>
        Lead innovative research projects • Contribute to and/or lead competitive grant applications •
        Produce high-impact publications • Mentor graduate students and early-career researchers •
        Build collaborations with academic, clinical, industry, and professional partners.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Official Ad</div>
      <div>
        <a href="https://unimelb.wd105.myworkdayjobs.com/en-US/UoM_External_Career/details/Research-Fellow---Senior-Research-Fellow--Eye-and-Immunology-Research_JR-003361?q=Ocular+Immunology"
           rel="noopener">
          University of Melbourne – job advertisement
        </a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Position Description</div>
      <div>
        <a href="https://unimelbcloud.sharepoint.com/:b:/s/UOM-Anon-EXT-001/IQB0_NRt-IglSIWrSnGc9pEkAdYXmOQ_ggXcALUGQ_sanw?e=eSa0J5"
           rel="noopener">
          Detailed position description (PDF)
        </a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Deadline</div>
      <div>
        January 6, 2026 · 11:55 pm AEST (Melbourne time)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div>
        Prof. Laura Downie – <a href="mailto:ldownie@unimelb.edu.au">ldownie@unimelb.edu.au</a>
      </div>
    </div>

  </div>
</div>


    <!-- Student Position: Genentech Summer 2026 Internship (Ophthalmology Biomarkers) -->
    <div class="card md-6"
      data-tags="job internship student-position genentech industry translational-medicine retina biomarkers imaging clinical-trials 2026">

      <div class="title"><b>Summer 2026 Internship — OMNI Ophthalmology (Genentech, South San Francisco)</b></div>

      <div class="meta">

        <div class="meta-row"><div class="meta-label">Role</div>
          <div>
            Lead a research project on retinal imaging biomarkers and clinical outcomes in retinal vascular disease. 
            Analyze clinical-trial datasets; explore mechanisms &amp; treatment response; communicate findings across teams.
          </div>
        </div>

        <div class="meta-row"><div class="meta-label">Location</div><div>South San Francisco, CA (On-site)</div></div>

        <div class="meta-row"><div class="meta-label">Start</div><div>May / June 2026</div></div>

        <div class="meta-row"><div class="meta-label">Apply</div>
          <div>
            Apply via Workday:<br>
            <a href="https://roche.wd3.myworkdayjobs.com/ROG-A2O-GENE/job/South-San-Francisco/XMLNAME-2026-Summer-Intern---Translational-Medicine--Ophthalmology-Biomarker-Development_202511-130178">
            Genentech Careers: Ophthalmology Biomarker Internship</a>
          </div>
        </div>

        <div class="meta-row"><div class="meta-label">Contact</div><div>Charlotte Wang, OD PhD — wang.charlotte@gene.com</div></div>

      </div>
    </div>

    <!-- Industry: Vision Scientist (Computational Modeling) — Apple -->
    <div class="card md-6" id="apple-vision-scientist-2025" data-tags="jobs industry vision-science computational-modeling imaging displays apple cupertino">
      <div class="title"><b>Vision Scientist (Computational Modeling) — Apple Vision Science</b></div>
      <div class="meta">

        <div class="meta-row">
          <div class="meta-label">Organization</div>
          <div>Apple — Vision Science Team</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Location</div>
          <div>Cupertino, California, USA</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Role</div>
          <div>
            Vision scientist focusing on computational modeling and psychophysical experiments
            to support cutting-edge imaging and display technologies across the Apple ecosystem
            (e.g., iPhone, iPad, Mac, Watch, and emerging platforms).
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Overview</div>
          <div>
            Join a collaborative, interdisciplinary team working at the intersection of
            vision science, image processing, color science, neuroscience, and optics.
            The position centers on developing vision models and metrics to quantify and
            optimize user experience for advanced display and imaging systems.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Responsibilities</div>
          <div>
            Develop and validate models/metrics of human vision and image quality •
            Design and run psychophysical studies of user experience •
            Compare and optimize algorithms for perceptual quality •
            Inform feature and specification decisions for future Apple products.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Profile</div>
          <div>
            Strong background in vision science or related fields (e.g., neuroscience,
            optics, image processing) • Experience with computational modeling and
            quantitative data analysis • Comfortable working in an interdisciplinary,
            product-oriented environment.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div>
            <a href="https://jobs.apple.com/en-us/details/200631317-0836/vision-scientist">
              Apple Jobs — Vision Scientist (Computational Modeling)
            </a>
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Contact</div>
          <div>
            Applicants may optionally notify<br>
            <a href="mailto:laura_walker@apple.com">Laura&nbsp;Walker, PhD (Apple | Vision Science)</a><br>
            (no responses to direct inquiries; applications via Apple Jobs link only).
          </div>
        </div>

      </div>
    </div>

    <!-- Internship: JEP:HPP Intern Junior Editorial (IJE) 2026 -->
    <div class="card md-6" data-tags="jobs editorial internship phd postdoc jep-hpp psychology publishing">
      <div class="title"><b>Intern Junior Editorial (IJE) Positions — JEP:HPP 2026</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>Journal of Experimental Psychology: Human Perception &amp; Performance (APA)</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>Term: January 1 – December 31, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>November 28, 2025</div></div>
        <div class="meta-row"><div class="meta-label">Eligibility</div>
          <div>Open to PhD students (≥3rd year) and postdocs ≤5 years; international applicants welcome</div>
        </div>
        <div class="meta-row"><div class="meta-label">Compensation</div><div>$1,500 USD honoraria stipend</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://redcap.link/HPPIJE26">Application link (REDCap)</a></div></div>
        <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:isabel.gauthier@vanderbilt.edu">Isabel Gauthier</a> (Editor, JEP:HPP)</div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>12-month editorial training role assisting with the journal’s pre-external review process.</li>
              <li>Work on 3–4 manuscripts / month under direct mentorship of the editor.</li>
              <li>Focus on transparency, rigor, and feedback to authors prior to full peer review.</li>
              <li>Encourages participation from historically excluded groups in scientific publishing leadership.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <!-- Doctoral positions: MSCA-DN EXPLORA (categorized below as PhD, but also a "job") -->
    <!-- We'll keep the card in the PhD section to avoid duplication. -->

    <!-- Job: Johnson & Johnson Vision — Staff R&D Scientist (IOL / Vision Science) -->
    <div class="card md-6" data-tags="jobs industry optics iol vision-science netherlands jnj">
      <div class="title"><b>Johnson & Johnson Vision — Staff R&amp;D Scientist (IOL Optics & Vision Science)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Employer</div><div>Johnson &amp; Johnson Vision (AMO Groningen B.V.)</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Groningen, Netherlands</div></div>
        <div class="meta-row"><div class="meta-label">Website</div>
          <div><a href="https://jj.wd5.myworkdayjobs.com/JJ/job/Groningen-Netherlands/R-D-Scientist_R-030298">Apply / Job posting</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Contact</div>
          <div><a href="mailto:CCanovas@its.jnj.com">CCanovas@its.jnj.com</a> (Carmen Cánovas Vidal, Director R&amp;D)</div>
        </div>
        <div class="meta-row"><div class="meta-label">Requirements</div>
          <div>PhD in Optics/Physics/Vision Science/Mechanical Modelling (ideal); 5+ years experience; proficiency with optical design (Zemax/OSLO/etc.).</div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>Role focuses on design &amp; development of intraocular lens (IOL) optical systems within a small, integrated R&amp;D team.</div>
        </div>
      </div>
    </div>

    <!-- Job: Scientific Coordinator — Excellence Cluster "The Adaptive Mind" (JLU Giessen) -->
    <div class="card md-6"
      data-tags="jobs coordinator scientific-coordinator germany europe giessen adaptive-mind psychology neuroscience cognitive-science ai ml robotics">
      <div class="title"><b>Scientific Coordinator — Excellence Cluster “The Adaptive Mind” (JLU Giessen)</b></div>

      <div class="meta">
        <div class="meta-row">
          <div class="meta-label">Organization</div>
          <div>Justus Liebig University Giessen (JLU), Excellence Cluster “The Adaptive Mind”</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Location</div>
          <div>Giessen, Germany</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Role</div>
          <div>Scientific Coordinator (project-duration, leadership role)</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Fields</div>
          <div>
            Experimental, clinical & developmental psychology; cognitive science; neuroscience; AI/ML; robotics.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Deadline</div>
          <div>Listed as “soon” (apply immediately)</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Apply</div>
          <div>
            <a href="https://www.uni-giessen.de/de/ueber-uns/karriere/stellenangebote/administratives-personal/527-06-1" target="_blank" rel="noopener">
              Job posting
            </a>
          </div>
        </div>
      </div>
    </div>

    <!-- Job: TrainingHub Coordinator — Excellence Cluster "The Adaptive Mind" (JLU Giessen) -->
    <div class="card md-6"
      data-tags="jobs coordinator traininghub coordinator-permanent germany europe giessen adaptive-mind psychology neuroscience cognitive-science ai ml robotics">
      <div class="title"><b>TrainingHub Coordinator — Excellence Cluster “The Adaptive Mind” (JLU Giessen)</b></div>

      <div class="meta">
        <div class="meta-row">
          <div class="meta-label">Organization</div>
          <div>Justus Liebig University Giessen (JLU), Excellence Cluster “The Adaptive Mind”</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Location</div>
          <div>Giessen, Germany</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Role</div>
          <div>Coordinator of TrainingHub (permanent position; German language required)</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Fields</div>
          <div>
            Experimental, clinical & developmental psychology; cognitive science; neuroscience; AI/ML; robotics.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Deadline</div>
          <div>Listed as “soon” (apply immediately)</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Apply</div>
          <div>
            <a href="https://www.uni-giessen.de/de/ueber-uns/karriere/stellenangebote/administratives-personal/532-06" target="_blank" rel="noopener">
              Job posting (German)
            </a>
          </div>
        </div>
      </div>
    </div>

    <!-- Administrative — NECO Senior Research Grants & Contracts Manager -->
    <div class="card md-6">
      <div class="title"><b>Senior Research Grants &amp; Contracts Manager</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>New England College of Optometry (NECO)</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Boston, MA, USA</div></div>
        <div class="meta-row"><div class="meta-label">Category</div><div>Administrative / Staff</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>Open until filled</div></div>
        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div><a href="https://recruiting.paylocity.com/Recruiting/Jobs/Details/3562667" rel="noopener">Apply — Paylocity (Job #3562667)</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Contact</div><div>Prof. Fuensanta Vera-Diaz — <a href="mailto:vera_diazf@neco.edu">vera_diazf@neco.edu</a></div></div>
        <div class="meta-row">
          <div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>End-to-end research administration: pre-award, post-award, compliance, reporting (NIH/eRA Commons, ASSIST, Research.gov).</li>
              <li>Budgets, subawards, industry agreements; policy guidance and training.</li>
              <li>Req: ≥3 years higher-ed research grants admin; Pref: 5+ years; CRA a plus.</li>
              <li>Benefits include medical/dental, paid holidays, and 9% employer 403(b) contribution after 1 year.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <!-- Internship — Meta -->
    <div class="card md-6">
      <div class="title"><b>Research Scientist Intern — Computational Photography</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>Meta</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>AR/VR (12-month internship)</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>Rolling / Until filled</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.metacareers.com/jobs/1049499833732409">Job posting</a></div></div>
      </div>
    </div>

    <!-- Industry Research — Google -->
    <div class="card md-6">
      <div class="title"><b>Research Scientist — Computational Photography/Vision + Generative AI</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>Google</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>Rolling / Until filled</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.google.com/about/careers/applications/jobs/results/97937785272312518-research-scientist-computational-photography-generative-ai">Job posting</a></div></div>
      </div>
    </div>

  </div> <!-- end cards: Jobs & Student Positions -->


  <!-- ========================= -->
  <!-- PhD & Graduate Positions  -->
  <!-- ========================= -->
  <h4>PhD &amp; Graduate Positions</h4>
  <div class="cards">

<div class="card" id="phd-vhit-sydney"
     data-tags="phd job vestibular testing ai computer-vision biomedical-engineering robotics australia neurology deep-learning">
  <div class="title">
    <b>Fully Funded PhD — AI-Driven Vestibular Testing (Sydney, Australia)</b>
  </div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Institution</div>
      <div>
        Royal Prince Alfred Hospital — Balance Clinic &amp; Laboratory<br>
        University of Sydney — Robotic Imaging Lab (Australian Centre for Robotics)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Application Deadline</div>
      <div>15 January 2026 (Sydney Time)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Project Area</div>
      <div>
        Biomedical engineering • Computer vision • AI • Multi-sensor fusion • Real-time clinical imaging
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        PhD project developing a <b>non-head-mounted, real-time camera system</b> for the Video Head Impulse Test (vHIT) —
        a key diagnostic tool for vestibular disorders and differentiating benign ear infections from life-threatening brainstem strokes.
        The goal is an inexpensive smartphone/webcam-based vHIT suitable for emergency rooms worldwide.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Motivation</div>
      <div>
        Current head-mounted systems suffer from motion artifacts, calibration drift, and patient discomfort.
        This project aims to improve accuracy, reduce artifacts, and enhance usability using advanced AI and imaging techniques.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Supervisors</div>
      <div>
        A/Prof Miriam Welgampola — Director, Balance Clinic &amp; Laboratory, Royal Prince Alfred Hospital<br>
        Dr. Donald Dansereau — Head, Robotic Imaging Lab, University of Sydney
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Funding</div>
      <div>
        Fully funded PhD with competitive stipend; open to domestic &amp; international applicants.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">How to Apply</div>
      <div>
        Full application details:  
        <a href="https://roboticimaging.org/Join/vHIT/">roboticimaging.org/Join/vHIT/</a><br>
        Enquiries: <a href="mailto:donald.dansereau@sydney.edu.au">donald.dansereau@sydney.edu.au</a>
      </div>
    </div>

  </div>
</div>


    <!-- Doctoral positions: MSCA-DN EXPLORA -->
    <div class="card md-6" data-tags="jobs phd doctoral europe msca materials perception multisensory">
      <div class="title"><b>MSCA-DN EXPLORA — Doctoral Positions (Perception of Materials, Objects &amp; Spaces)</b></div>
      <div class="meta">

        <div class="meta-row"><div class="meta-label">Employer</div>
          <div>EXPLORA — Marie Skłodowska-Curie Doctoral Network (multiple host universities & labs)</div>
        </div>

        <div class="meta-row"><div class="meta-label">Fields</div>
          <div>Psychology • Neuroscience • Robotics • Computer Science • Architecture &amp; Design</div>
        </div>

        <div class="meta-row"><div class="meta-label">Location</div>
          <div>Multiple sites across Europe (host-specific)</div>
        </div>

        <div class="meta-row"><div class="meta-label">Website</div>
          <div>
            <a href="https://explora-network.github.io/web/">Project overview</a> •
            <a href="https://explora-network.github.io/web/projects.html">Open projects &amp; hosts</a>
          </div>
        </div>

        <div class="meta-row"><div class="meta-label">Requirements</div>
          <div>
            Must not already hold a doctoral degree; EU mobility rule applies (not resident in host country &gt;12 months in the 36 months before recruitment).
          </div>
        </div>

        <div class="meta-row"><div class="meta-label">Funding</div>
          <div>MSCA fellowship (3 years); remuneration per EC rules; total duration may be 3–4 years depending on host regulations.</div>
        </div>

        <div class="meta-row"><div class="meta-label">Contact</div>
          <div>
            EXPLORA Coordinator: <a href="mailto:katja.doerschner@psychol.uni-giessen.de">katja.doerschner@psychol.uni-giessen.de</a> •
            Co-coordinator: <a href="mailto:S.C.Pont@tudelft.nl">S.C.Pont@tudelft.nl</a>
          </div>
        </div>

        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            International training network with cross-sector workshops, collaboration across partner labs, and 2 research secondments for each doctoral candidate.
          </div>
        </div>

      </div>
    </div>

    <!-- PhD — University of Nevada, Reno (Berryhill & Haigh Labs) -->
    <div class="card md-6">
      <div class="title"><b>PhD Positions — Cognitive &amp; Brain Sciences (UNR)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>University of Nevada, Reno — Labs of Marian Berryhill &amp; Sarah Haigh</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Reno, Nevada, USA</div></div>
        <div class="meta-row"><div class="meta-label">Deadlines</div><div><b>Nov 3, 2025</b> (Spring 2026 start) • <b>Dec 15, 2025</b> (Fall 2026 start)</div></div>
        <div class="meta-row"><div class="meta-label">Research</div>
          <div>Sensory &amp; working memory; subclinical populations (e.g., schizotypy, autism); team-based research</div>
        </div>
        <div class="meta-row"><div class="meta-label">Funding</div><div>Permanent teaching assistantships; support for external fellowships</div></div>
        <div class="meta-row"><div class="meta-label">Contact</div>
          <div><a href="mailto:mberryhill@unr.edu">mberryhill@unr.edu</a> • <a href="mailto:shaigh@unr.edu">shaigh@unr.edu</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Web</div>
          <div><a href="https://labs.psych.unr.edu/mblab/" rel="noopener">Berryhill Lab</a> • <a href="https://sarahmhaigh.github.io/" rel="noopener">Haigh Lab</a></div>
        </div>
      </div>
    </div>

    <!-- PhD — ESI / Max Planck (Rademaker Lab) -->
    <div class="card md-6">
      <div class="title"><b>PhD Position — Perception &amp; Cognitive Computational Neuroscience</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>Ernst Strüngmann Institute (ESI), Frankfurt — Max Planck research group of Dr. Rosanne Rademaker</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Frankfurt, Germany</div></div>
        <div class="meta-row"><div class="meta-label">Category</div><div>PhD (Fully funded)</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>First review: October 20, 2025</div></div>
        <div class="meta-row"><div class="meta-label">Start</div><div>January 2026</div></div>
        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div><a href="https://www.rademakerlab.org/" rel="noopener">rademakerlab.org</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Application</div><div>Email <a href="mailto:rademakerlab@gmail.com">rademakerlab@gmail.com</a> with a 1-page (double-spaced) research statement, recent CV incl. publications/competencies/GPA+scale, and 2 referees.</div></div>
        <div class="meta-row">
          <div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Topics span perception, working memory, spatial representations, cognition &amp; action.</li>
              <li>Please avoid AI-generated text; such submissions won’t be evaluated per the call.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <!-- 2 PhD Positions — AI/Computer Vision for Healthcare (CAMMA, Strasbourg) -->
    <div class="card md-6" id="camma-phd-vision-healthcare-2026" data-tags="job phd computer-vision ai healthcare surgery france erc">
      <div class="title"><b>2 PhD Positions — AI/Computer Vision for Healthcare (CAMMA, Strasbourg)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Institution</div><div>CAMMA Research Group, University of Strasbourg • IHU Strasbourg</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Strasbourg, France</div></div>
        <div class="meta-row"><div class="meta-label">Focus</div><div>Foundation models for surgery; large-scale surgical video analysis; semantic graphs; self-supervised &amp; multi-modal learning.</div></div>
        <div class="meta-row"><div class="meta-label">Funding</div><div>ERC Consolidator Project <i>CompSURG</i> &amp; ENACT AI Chair.</div></div>
        <div class="meta-row"><div class="meta-label">Requirements</div>
          <div>
            <ul class="notes-list">
              <li>Master’s in Computer Science (or equivalent)</li>
              <li>Strong Python; solid CV/ML background</li>
              <li>English proficiency (oral &amp; written)</li>
              <li>Plus: DL/NLP, multi-modal video+text experience</li>
            </ul>
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">How to Apply</div>
          <div>Send CV, motivation letter, and transcripts to <a href="mailto:npadoy@unistra.fr">Nicolas Padoy</a>.</div>
        </div>
        <div class="meta-row"><div class="meta-label">Links</div>
          <div>
            <a href="http://camma.u-strasbg.fr/opportunities">Position details / Opportunities</a> •
            <a href="https://camma.unistra.fr/">CAMMA Group</a>
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>Projects with international clinical collaborators; goal: improve surgical safety via new video modeling methods.</div>
        </div>
      </div>
    </div>

    <!-- PhD: Colour Vision Deficiencies & Universal Design — UCM Madrid -->
    <div class="card md-6" data-tags="jobs phd colour-vision color-vision deficiencies madrid ucm psychology">
      <div class="title"><b>4-Year Funded PhD — Colour Vision Deficiencies &amp; Universal Design (UCM, Madrid)</b></div>
      <div class="meta">
        <div class="meta-row">
          <div class="meta-label">Organization</div>
          <div>Research Group “Color blindness and universal design”, Faculty of Psychology, Universidad Complutense de Madrid (UCM)</div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Location</div>
          <div>Madrid, Spain</div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Project</div>
          <div>“Colour Vision Deficiencies: Diagnosis, Compensation Mechanisms and Perceptual Learning in children and adults” (PID2024-155495NA-I00; PI: Leticia Álvaro)</div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Duration</div>
          <div>4-year fully funded PhD position (final year may convert into a postdoctoral contract if the thesis is completed within 3 years)</div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Overview</div>
          <div>
            Investigates colour vision deficiency (CVD) across the lifespan, combining molecular genetics and perceptual training to improve screening, diagnosis, and compensation strategies in children and adults.
          </div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Eligibility</div>
          <div>Bachelor’s and Master’s degree in Psychology or a related field; good English skills.</div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Application</div>
          <div>
            Apply via the UCM online platform:
            <a href="https://www.ucm.es/ct65-25">ucm.es/ct65-25</a>
            (national call “Ayudas para contratos predoctorales para la formación de doctores 2025”). Applicants must hold a valid electronic certificate.
          </div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Research Group</div>
          <div>
            <a href="https://produccioncientifica.ucm.es/grupos/5410/detalle">Color blindness and universal design</a>
          </div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Deadline</div>
          <div>November 24, 2025</div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Contact</div>
          <div><a href="mailto:lalvaro@ucm.es">Dr. Leticia Álvaro</a></div>
        </div>
      </div>
    </div>

    <!-- Graduate Position: Visual Perception, Medical Imaging & AI — University of Arizona -->
    <div class="card md-6"
      data-tags="phd graduate funded vision-perception visual-search medical-imaging ai radiology psychology arizona usa eye-tracking eeg">
      <div class="title">
        <b>Graduate Student Position — Visual Perception, Medical Imaging &amp; AI (University of Arizona)</b>
      </div>

      <div class="meta">
        <div class="meta-row">
          <div class="meta-label">Lab</div>
          <div>ADAMO Lab (Attention Detection and Medical Observation)</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Departments</div>
          <div>Psychology; Radiology &amp; Imaging Sciences</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Location</div>
          <div>Tucson, Arizona, USA</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Research Areas</div>
          <div>
            Visual search and attention; medical image perception; breast cancer detection in simulated radiology tasks;
            applied human factors; AI-assisted perception; behavioral, eye-tracking, and EEG experiments.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Responsibilities</div>
          <div>
            Lead and support grant projects; conduct psychophysics, EEG, and eye-tracking studies; manage data collection
            (domestic &amp; international); prepare manuscripts; contribute to grants; present at meetings.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Qualifications</div>
          <div>
            Bachelor’s in Psychology, Neuroscience, Cognitive Science, Computer Science, Biomedical Engineering, or related fields;
            experience with programming (Python, MATLAB, R, etc.); behavioral research experience; EEG/eye-tracking preferred;
            AI/ML skills preferred.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Funding</div>
          <div>Full-time and fully funded position</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Apply</div>
          <div>
            <a href="https://psychology.arizona.edu/graduate/how-apply" target="_blank" rel="noopener">
              psychology.arizona.edu/graduate/how-apply
            </a>
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Contact</div>
          <div>
            Dr. Stephen Adamo — <a href="mailto:rachelmorris@arizona.edu">rachelmorris@arizona.edu</a>
          </div>
        </div>

      </div>
    </div>

    <!-- PhD: Computational Cognitive Neuroscience — Hebart Lab (Giessen) -->
    <div class="card md-6" id="phd-hebart-giessen" data-tags="jobs phd cognitive-neuroscience computational modeling decision-making vision">
      <div class="title"><b>PhD — Computational Cognitive Neuroscience (Hebart Lab, University of Giessen)</b></div>
      <div class="meta">

        <div class="meta-row"><div class="meta-label">Lab</div>
          <div>Dept. of General Psychology, University of Giessen (Germany) — Prof. Michael N. Hebart</div>
        </div>

        <div class="meta-row"><div class="meta-label">Research Areas</div>
          <div>
            Computational models of perceptual decision-making, probabilistic inference, perceptual organization, and human behavior in complex environments.
          </div>
        </div>

        <div class="meta-row"><div class="meta-label">Position</div>
          <div>Fully funded PhD — 3-year term with extension possible</div>
        </div>

        <div class="meta-row"><div class="meta-label">Requirements</div>
          <div>
            Master’s in psychology, neuroscience, computer science, cognitive science, or related field.  
            Strong background in quantitative methods, modeling, machine learning, or computational approaches desirable.
          </div>
        </div>

        <div class="meta-row"><div class="meta-label">Application</div>
          <div>Send application materials to <a href="mailto:apply@cog-giessen.de">apply@cog-giessen.de</a> (details per digest announcement).</div>
        </div>

        <div class="meta-row"><div class="meta-label">Contact</div>
          <div><a href="mailto:Michael.Hebart@psychol.uni-giessen.de">Michael.Hebart@psychol.uni-giessen.de</a></div>
        </div>

      </div>
    </div>

    <!-- PhD Positions: ML/AI — COSMOS Center, UT Arlington -->
    <div class="card md-6" id="phd-uta-cosmos-2026" data-tags="jobs phd ml ai deep-learning data-science texas 2026">
      <div class="title"><b>PhD Positions in ML/AI — COSMOS Center, University of Texas at Arlington (Spring &amp; Fall 2026)</b></div>
      <div class="meta">

        <div class="meta-row">
          <div class="meta-label">Center</div>
          <div>Center on Stochastic Modeling, Optimization, &amp; Statistics (COSMOS), University of Texas at Arlington (UTA)</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Location</div>
          <div>Arlington, Texas, USA (Dallas–Fort Worth metro area)</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Start</div>
          <div>PhD positions available for Spring 2026 (for students already in the US) and Fall 2026 intakes.</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Research Areas</div>
          <div>
            Deep learning • Data analytics • Artificial intelligence • Intelligent systems •
            Stochastic modeling &amp; optimization.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Topics</div>
          <div>
            Deep learning &amp; decision analytics for healthcare and biomedical imaging (cancer/brain imaging, precision medicine) •
            Generative AI for computational life science &amp; drug discovery (protein/DNA/RNA, molecular design) •
            Multivariate time series &amp; sequential data modeling (energy, healthcare, agriculture, finance) •
            Interpretable &amp; probabilistic deep learning (uncertainty, robustness) •
            AI-driven smart systems for agriculture •
            AI engineering &amp; intelligent agent systems (LLMs, autonomous AI agents).
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Support</div>
          <div>
            Full financial support for PhD students (tuition, stipend, and benefits).  
            Visiting student/scholar positions also available in ML/AI and intelligent systems.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Eligibility</div>
          <div>
            Strong quantitative background, solid programming skills, and experience in machine learning / deep learning.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Application</div>
          <div>
            Email CV, transcripts, and any materials highlighting experience (papers, thesis, awards, certificates, etc.) to  
            <a href="mailto:shouyiw@uta.edu">Dr. Shouyi&nbsp;Wang (shouyiw@uta.edu)</a>.  
            Applications are reviewed on a rolling basis with prompt feedback.
          </div>
        </div>

      </div>
    </div>

    <!-- Two 4-Year PhD Positions — Scene Grammar Lab (LMU Munich) -->
    <div class="card md-6" id="phd-scene-grammar-lmu" data-tags="jobs phd scene-grammar visual-attention real-world-vision vr eye-tracking eeg computational-modeling germany">
      <div class="title"><b>Two 4-Year PhD Positions — Scene Grammar Lab (LMU Munich)</b></div>
      <div class="meta">

        <div class="meta-row">
          <div class="meta-label">Lab</div>
          <div>Scene Grammar Lab, Chair of Neuro-Cognitive Psychology (Prof. Melissa Lê-Hoa Võ), LMU Munich</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Location</div>
          <div>Munich, Germany</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Positions</div>
          <div>
            <b>PhD #1:</b> 4-year funded position within the Cluster of Excellence <i>The Adaptive Mind (TAM)</i>, focusing on real-world search using VR eye-tracking and (mobile) EEG.<br>
            <b>PhD #2:</b> University-funded 4+2 year position (with ~4 hr/week teaching), flexible research agenda, long-term perspective.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Research Areas</div>
          <div>
            Scene perception • Scene grammar • Object memory • Visual attention • Real-world &amp; VR environments • Eye-tracking • EEG • Computational modeling.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Methods</div>
          <div>Psychophysics • Real-world &amp; VR eye-tracking • EEG (including mobile EEG) • Computational modeling • Machine learning approaches.</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Eligibility</div>
          <div>
            MSc in Psychology, Neuroscience, Computer Vision, Cognitive Science, or related fields.  
            Experience with eye-tracking and/or EEG strongly preferred.  
            Programming experience (Python, Unity) and solid statistical/methodological skills beneficial.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Application</div>
          <div>
            Send CV, statement of research interests, and contact information for two academic references to  
            <a href="mailto:melissa.vo@psy.lmu.de">melissa.vo@psy.lmu.de</a>.  
            Preferred deadline: <b>before Nov 30, 2025</b> (later applications considered).
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div><a href="https://www.scenegrammarlab.com/">scenegrammarlab.com</a></div>
        </div>

      </div>
    </div>

  </div> <!-- end cards: PhD & Graduate Positions -->


  <!-- ================================ -->
  <!-- Postdoctoral Career Opportunities -->
  <!-- ================================ -->
  <h4>Postdoctoral Career Opportunities</h4>
  <div class="cards">

<div class="card" id="anderson-postdoc-texasam" data-tags="job postdoc mri fmri attention learning cognitive-neuroscience imaging texas">
  <div class="title"><b>Postdoctoral Position — Anderson Lab (Texas A&amp;M University)</b></div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Institution</div>
      <div>Texas A&amp;M University — Department of Psychological &amp; Brain Sciences</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Area</div>
      <div>MRI-based research in learning &amp; attention; cognitive neuroscience; biomedical imaging</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        Postdoctoral researcher to join the lab of <b>Dr. Brian A. Anderson</b>, Director of the Human Imaging Facility and Interim Executive Director of the Human Clinical Research Facility. 
        The position is supported by institutional funds, allowing broad flexibility in research direction based on mutual interest.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Focus</div>
      <div>
        fMRI investigations of learning and attention; opportunities for broader biomedical imaging work using TMS, CT, ultrasound, and EEG.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Qualifications</div>
      <div>
        Strong publication record with multiple first-author fMRI papers; ability to independently develop and complete fMRI projects.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Additional Opportunities</div>
      <div>
        Engagement with academic–industry partnerships; experience in applied and translational imaging; preparation for academic or industry pathways.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">How to Apply</div>
      <div>
        Email cover letter, CV, and contact information for 2–3 references to  
        <a href="mailto:brian.anderson@tamu.edu">brian.anderson@tamu.edu</a>.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Lab Website</div>
      <div>
        <a href="http://andersonlab.sites.tamu.edu/">andersonlab.sites.tamu.edu</a>
      </div>
    </div>

  </div>
</div>



    <!-- Postdoc — InnoHK Centre for Eye & Vision Research (Hong Kong) -->
    <div class="card md-6"
      data-tags="job postdoc vision-science neuromodulation brain-stimulation perceptual-learning eye-tracking eeg hong-kong cevr neuroplasticity rehabilitation">

      <div class="title"><b>Postdoctoral Positions — InnoHK Centre for Eye &amp; Vision Research (Hong Kong)</b></div>

      <div class="meta">

        <div class="meta-row">
          <div class="meta-label">Project</div>
          <div>
            Novel neuromodulation approaches for vision enhancement using non-invasive brain stimulation 
            and perceptual learning.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Research Methods</div>
          <div>
            Psychophysics · Eye tracking · EEG · Electrophysiology · Non-invasive brain stimulation (TMS/tES).
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Qualifications</div>
          <div>
            Doctoral degree in vision science, optometry, psychology, neuroscience, or related fields; 
            programming skills (Python/Matlab); strong analytical &amp; software skills; team-focused.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Desirable</div>
          <div>
            Experience with brain stimulation, EEG, eye-tracking, psychophysics; optometry or clinical background.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Salary</div>
          <div>
            HKD 35,000/month + HKD 10,000 living allowance.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Apply</div>
          <div>
            Email CV + cover letter + completed application form to 
            <a href="mailto:career@cevr.hk">career@cevr.hk</a> (quote “RP 1.5S Postdoc Position”).
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">About</div>
          <div>
            CEVR is a collaboration between Hong Kong Polytechnic University and University of Waterloo, 
            based at Hong Kong Science Park.
          </div>
        </div>

      </div>
    </div>


    <!-- Postdoc — Visual & Cognitive Neuroscience Lab, University of Fribourg -->
    <div class="card md-6"
      data-tags="job postdoc neuroscience vision cognition blindness attention fMRI eeg eye-tracking switzerland auditory spatial-attention rehabilitation">
      
      <div class="title"><b>Post-doctoral Position — Visual &amp; Cognitive Neuroscience Lab (University of Fribourg, Switzerland)</b></div>

      <div class="meta">

        <div class="meta-row">
          <div class="meta-label">Project</div>
          <div>
            “Auditory spatial attention and eye-movement guidance in blindness and its use for sight rehabilitation.”
            Collaboration with Georgetown University (Ella Striem-Amit) and Lausanne University Hospital (Marzia de Lucia).
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Research Methods</div>
          <div>
            Behavioral experiments · fMRI &amp; advanced fMRI analysis · EEG/EOG · eye-tracking · sensory-aid translation.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Qualifications</div>
          <div>
            PhD in Cognitive Neuroscience or related field; strong programming skills (Python/R/Matlab);
            experience with fMRI design/analysis; strong analytical/statistical skills; publication record;
            ability to work independently &amp; supervise students.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Desired</div>
          <div>
            Advanced fMRI (pRF, MVPA, RSA, retinotopy, ML approaches); EEG/EOG experience; eye-tracking expertise.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Location</div>
          <div>
            Department of Psychology, University of Fribourg, Switzerland.  
            Optional research stays at Georgetown University (Washington DC).
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Duration</div>
          <div>
            Funded for ≥2 years (possible extension to 3 years).  
            Start date: Flexible in 2026.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Deadline</div>
          <div><b>January 18, 2026</b> (late applications considered if no suitable candidate found)</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Apply</div>
          <div>
            Email CV, motivation letter, and 2 referee contacts to  
            <a href="mailto:petra.vetter@unifr.ch">petra.vetter@unifr.ch</a>.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Lab Website</div>
          <div><a href="https://www.unifr.ch/psycho/en/research/vcnl/">unifr.ch/psycho/en/research/vcnl</a></div>
        </div>

      </div>
    </div>

    <!-- Postdoc & PhD Positions — Denison Lab (Boston University) -->
    <div class="card md-6" id="denison-bu" data-tags="jobs postdoc phd perception attention neuroimaging decision-making boston">
      <div class="title"><b>Postdoc &amp; PhD Positions — Denison Lab (Boston University)</b></div>
      <div class="meta">

        <div class="meta-row">
          <div class="meta-label">Lab</div>
          <div>Denison Lab — Psychological &amp; Brain Sciences, Boston University (PI: Dr. Rachel Denison)</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Research Areas</div>
          <div>
            Visual perception • Attention • Decision-making • Temporal dynamics of vision  
            Neuroimaging (fMRI, EEG, MEG) • Behavioral &amp; computational modeling approaches.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Positions</div>
          <div>
            <b>Postdoctoral Researcher:</b> Experience with human neuroimaging required (fMRI/EEG/MEG).  
            <br>
            <b>PhD Students:</b> Applications via BU's BBC Program and Graduate Program in Neuroscience.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Requirements</div>
          <div>
            Strong quantitative skills and interests.  
            PhD applicants: Minimum 2 years relevant research experience.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Application</div>
          <div>
            <b>Postdoc:</b> Email CV, cover letter, and 2 reference contacts to  
            <a href="mailto:rdenison@bu.edu">rdenison@bu.edu</a>.  
            <br>
            <b>PhD:</b> Apply through BU programs:<br>
            <a href="https://www.bu.edu/psych/academics/phd/bbc/">Brain, Behavior &amp; Cognition PhD</a> •  
            <a href="https://www.bu.edu/neuro/academics/graduate/">Graduate Program in Neuroscience</a>
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div><a href="https://sites.bu.edu/denisonlab/">Denison Lab</a></div>
        </div>

      </div>
    </div>

    <!-- Postdoc: Smith-Kettlewell Eye Research Institute (San Francisco) -->
    <div class="card md-6" data-tags="jobs postdoc vision-research low-vision neuroscience sanfrancisco ski">
      <div class="title"><b>Postdoctoral Fellowships — Smith-Kettlewell Eye Research Institute (San&nbsp;Francisco)</b></div>
      <div class="meta">

        <div class="meta-row">
          <div class="meta-label">Organization</div>
          <div>Smith-Kettlewell Eye Research Institute</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Location</div>
          <div>San&nbsp;Francisco, California, USA</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Fellowships (2026)</div>
          <div>
            <ul class="notes-list">
              <li>
                <b>NEI Training Grant Fellowship:</b> 1-year appointment (with possible renewal) for
                U.S. citizens or permanent residents. Apply by <b>December&nbsp;10,&nbsp;2025</b>;
                start before <b>March&nbsp;31,&nbsp;2026</b>.
              </li>
              <li>
                <b>Smith-Kettlewell Funded Fellowship:</b> 2-year appointment open to fellows
                of all nationalities. Apply by <b>January&nbsp;30,&nbsp;2026</b>;
                start in <b>summer or fall 2026</b>.
              </li>
            </ul>
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Eligibility</div>
          <div>Ph.D., O.D., or M.D. required.</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Research Areas</div>
          <div>
            Visual development • Strabismus &amp; amblyopia • Eye movements • Visual attention •
            Brain plasticity • Mobility • Low vision / blindness rehabilitation • Accessibility
            technologies.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Methods</div>
          <div>
            Psychophysics • Eye &amp; head tracking • Clinical assessment • Computational modeling •
            EEG • fMRI • Computer vision &amp; AI • Sensor technologies.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Program</div>
          <div>
            Fellows can propose independent projects, work with one or more mentors, and
            apply for mentored or independent grants, with potential to move onto the PI track.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div>
            <a href="https://ski.org/what-does-fellowship-smith-kettlewell-offer">What does a fellowship offer?</a> •
            <a href="https://ski.org/how-apply">How to apply</a> •
            <a href="https://ski.org/current-mentors">Current mentors</a>
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Contact</div>
          <div>
            <a href="mailto:fellowships@ski.org">fellowships@ski.org</a> •
            <a href="mailto:preeti@ski.org">Preeti&nbsp;Verghese</a>
          </div>
        </div>

      </div>
    </div>

    <!-- Postdoc: InnoHK Centre for Eye and Vision Research (Hong Kong) -->
    <div class="card md-6" data-tags="jobs postdoc neuroscience vision cevr hongkong brain-stimulation">
      <div class="title"><b>Postdoctoral Fellowships — InnoHK Centre for Eye &amp; Vision Research (CEVR)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>InnoHK Centre for Eye &amp; Vision Research (CEVR)</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Hong Kong Science Park, Sha Tin, Hong Kong</div></div>
        <div class="meta-row"><div class="meta-label">Supervisors</div><div>Dr. Ken&nbsp;Tan &amp; Prof.&nbsp;Benjamin&nbsp;Thompson</div></div>
        <div class="meta-row"><div class="meta-label">Duration</div><div>Up to 3&nbsp;years</div></div>
        <div class="meta-row"><div class="meta-label">Salary</div><div>HKD&nbsp;35,000&nbsp;/month&nbsp;+&nbsp;HKD&nbsp;10,000&nbsp;living allowance</div></div>
        <div class="meta-row"><div class="meta-label">Research</div>
          <div>
            Novel neuromodulation approaches for vision enhancement using non-invasive brain stimulation and perceptual learning; psychophysical, eye-tracking, and electrophysiological techniques.
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Prerequisites</div>
          <div>
            PhD in vision science, optometry, psychology, or neuroscience; programming experience (Python/MATLAB); strong analytical and teamwork skills.
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Preferred Experience</div>
          <div>
            Non-invasive brain stimulation (TMS/tES), EEG, eye-tracking, psychophysics, or clinical optometry background.
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Application</div>
          <div>
            Send completed <a href="http://cevr.hk/wp-content/uploads/2023/10/Job-application-form-CEVR_202310.docx">application form</a>, CV, and cover letter to 
            <a href="mailto:career@cevr.hk">career@cevr.hk</a> (quote “RP&nbsp;1.5S Postdoc&nbsp;Position”).
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Preference for candidates experienced in brain stimulation or research with elderly/visually impaired populations.</li>
              <li>Joint initiative of Hong&nbsp;Kong&nbsp;PolyU and University&nbsp;of&nbsp;Waterloo under the InnoHK program.</li>
              <li>Focus areas: myopia, ocular drug delivery, vision enhancement, tear film &amp; ocular surface, and optometric technology.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <!-- Postdoc: NIMH – Neural Circuits of Vision & Recovery After V1 Injury -->
    <div class="card md-6" data-tags="jobs postdoc vision neuroscience circuits marmoset neuroanatomy imaging injury recovery NIH NIMH USA bethesda 2026-start">
      <div class="title"><b>Postdoctoral Fellow — Neural Circuits of Vision &amp; Recovery After V1 Injury (NIMH/NIH)</b></div>

      <div class="meta">
        <div class="meta-row">
          <div class="meta-label">Organization</div>
          <div>Section on Cellular &amp; Cognitive Neurodevelopment (SCCN), NIMH/NIH</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Location</div>
          <div>Bethesda, Maryland, USA</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Research Focus</div>
          <div>
            Neural circuitry underlying visual perception and recovery after primary visual cortex (V1) damage;
            pulvinar &amp; LGN contributions to visual processing; translational marmoset model.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Techniques</div>
          <div>
            Viral circuit tracing, chemogenetics (DREADDs), MRI-guided stereotaxic surgery, DTI,
            calcium imaging (GCaMP/miniscope), behavioral &amp; eye-tracking assays, histology, immunohistochemistry.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Start Date</div>
          <div>Flexible; early 2026 preferred</div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Ideal Candidate</div>
          <div>
            Ph.D. in neuroscience or related field; experience in systems/visual neuroscience,
            behavioral analysis, neuroanatomy, in-vivo imaging, viral vector work, or surgical methods.
          </div>
        </div>

        <div class="meta-row">
          <div class="meta-label">Contact</div>
          <div>
            Dr. James Bourne — <a href="mailto:james.bourne@nih.gov">james.bourne@nih.gov</a>
          </div>
        </div>
      </div>
    </div>

    <!-- Postdoc — UT Austin -->
    <div class="card md-6">
      <div class="title"><b>Postdoctoral Fellow — Computer Vision (Center for Generative AI)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>UT Austin — ML Lab</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Austin, TX, USA</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>Rolling / Until filled</div></div>
        <div class="meta-row"><div class="meta-label">Contact</div><div>Adam Klivans, PhD (Lab Director)</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://ml.utexas.edu/postdoctoral-positions">Postdoc positions</a></div></div>
      </div>
    </div>

    <!-- Postdoc/PhD — MPI Tübingen -->
    <div class="card md-6">
      <div class="title"><b>Postdoc &amp; PhD Positions — Human Psychophysics</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>Max Planck Institute for Biological Cybernetics</div></div>
        <div class="meta-row"><div class="meta-label">Department</div><div>Sensory &amp; Sensorimotor Systems</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Tübingen, Germany</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>Rolling / Until filled</div></div>
        <div class="meta-row"><div class="meta-label">PI</div><div>Li Zhaoping, PhD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.kyb.mpg.de/328993/vacancies">Vacancies</a></div></div>
      </div>
    </div>

    <!-- Postdoctoral Fellow – Neuroimaging of Visual System (Stanford) -->
    <div class="card md-6">
      <div class="title"><b>Postdoctoral Fellow – Neuroimaging of Visual System</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Stanford University, Spencer Center for Vision Research</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.nivs-lab.org/open-positions">Open positions</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Contact: kevinchan@stanford.edu</li></ul></div></div>
      </div>
    </div>

    <!-- Postdoc — Rademaker Lab (general postdoc ad) -->
    <div class="card md-6">
      <div class="title"><b>Postdoctoral Researcher — Perception &amp; Cognitive Computational Neuroscience</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>Ernst Strüngmann Institute (ESI), Frankfurt — Max Planck research group of Dr. Rosanne Rademaker</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Frankfurt, Germany</div></div>
        <div class="meta-row"><div class="meta-label">Category</div><div>Postdoctoral (Fully funded)</div></div>
        <div class="meta-row"><div class="meta-label">Start</div><div>2026 (flexible)</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>Rolling / Until filled</div></div>
        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div><a href="https://www.rademakerlab.org/" rel="noopener">rademakerlab.org</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:rademakerlab@gmail.com">rademakerlab@gmail.com</a></div></div>
        <div class="meta-row">
          <div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Focus: interaction of sensation, working memory, spatial representations, cognition &amp; action.</li>
              <li>Resources: research-dedicated high-field fMRI, MEG, and strong computational infrastructure.</li>
              <li>Computational methods experience expected; neuroimaging (EEG/fMRI) a plus; inclusive lab culture.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <!-- Postdoc — University of Oklahoma Health Sciences Center (CLOSED: removed) -->
    <!-- Postdoc — UAB (CLOSED: removed) -->
    <!-- Postdoc — KAUST (CLOSED: removed) -->
    <!-- Postdoc/RA — Cambridge ABL (CLOSED: removed) -->
    <!-- Postdoc — Leeds (CLOSED: removed) -->
    <!-- Postdoc – Grounded Gesture Generation (CLOSED: removed) -->

  </div> <!-- end cards: Postdoctoral Career Opportunities -->


  <!-- ================== -->
  <!-- Faculty Positions  -->
  <!-- ================== -->
  <h4>Faculty Positions</h4>
  <div class="cards">

<!-- Lecturer in Cognitive Science — RPI -->
<div class="card md-6" id="rpi-lecturer-cogsci"
     data-tags="job faculty lecturer cognitive-science teaching 2026 rpi">
  <div class="title">
    <b>Lecturer in Cognitive Science — Rensselaer Polytechnic Institute (RPI)</b>
  </div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Start</div>
      <div>Fall 2026</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Department of Cognitive Science, Rensselaer Polytechnic Institute, Troy, NY, USA</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Role</div>
      <div>
        Full-time lecturer (one-year appointment, renewable) focused on teaching and developing
        core undergraduate courses in cognitive science, especially introductory offerings that
        serve students across the Institute.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Desired Background</div>
      <div>
        Strong grounding in cognitive science and related areas (psychology, philosophy,
        neuroscience, artificial intelligence, or cognitive systems) plus evidence of effective
        college-level teaching.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Qualifications</div>
      <div>
        Terminal degree (or foreign equivalent) in a relevant field by time of appointment;
        demonstrated or potential scholarly and/or research achievement in the candidate’s area.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Review Process</div>
      <div>
        Application review begins immediately and continues until the position is filled.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Apply</div>
      <div>
        <a href="https://careers.rpi.edu/en-us/job/495590/lecturer-cognitive-science">
          careers.rpi.edu – Lecturer in Cognitive Science
        </a>
      </div>
    </div>

  </div>
</div>


<!-- Faculty: Associate Professor in Motor Control (University of Leeds) -->
<div class="card md-6"
     id="leeds-assoc-prof-motor-control"
     data-tags="job faculty associate-professor motor-control biomechanics rehabilitation ageing leeds uk biomedical-sciences">
  <div class="title">
    <b>Associate Professor in Motor Control — University of Leeds</b>
  </div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Institution</div>
      <div>School of Biomedical Sciences, University of Leeds (UK)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Rank</div>
      <div>Associate Professor (Motor Control)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Leeds, United Kingdom</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Interviews</div>
      <div>Week commencing <b>12 January 2026</b></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Area</div>
      <div>
        Motor control and related topics within Biomedical Sciences, including
        neural control of movement, motor learning, plasticity and rehabilitation,
        and sensorimotor integration across the life course.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Research Environment</div>
      <div>
        Part of a vibrant Cardiovascular and Exercise Sciences research pillar
        spanning motor control, biomechanics, exercise physiology, computational
        modelling, and exercise &amp; health psychology; research from
        fundamental science through translational rehabilitation applications.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Profile</div>
      <div>
        Experienced and influential academic with internationally excellent
        publications, strong record of competitive research funding, evidence
        of impact, and commitment to delivering world-leading research and an
        exceptional student experience.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Role</div>
      <div>
        Contribute to research, teaching, supervision, and academic leadership
        in Biomedical Sciences; supervise postgraduate researchers and mentor
        colleagues; help shape the future of the School’s research portfolio.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Benefits</div>
      <div>
        26 days holiday plus ~16 bank/university-closure days (≈42 days total);
        generous pension (14.5% employer contribution); access to campus gym
        (The Edge), wellbeing support, professional development, childcare,
        shopping discounts, and travel schemes.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Visa</div>
      <div>
        Post may be eligible for sponsorship under the UK Skilled Worker route;
        eligibility under the Global Talent visa will also be considered.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        Job details / application information:<br>
        <a href="https://jobs.leeds.ac.uk/EmailFriend.aspx?ref=FBSBM1225">
          jobs.leeds.ac.uk (Ref: FBSBM1225)
        </a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contacts</div>
      <div>
        Professor Elaine Martin — Interim Head of School<br>
        <a href="mailto:E.Martin@leeds.ac.uk">E.Martin@leeds.ac.uk</a><br>
        Professor Ronaldo Ichiyama — Professor of Neural Control of Movement<br>
        <a href="mailto:R.M.Ichiyama@leeds.ac.uk">R.M.Ichiyama@leeds.ac.uk</a><br>
        Professor Sarah Astill<br>
        <a href="mailto:s.l.astill@leeds.ac.uk">s.l.astill@leeds.ac.uk</a>
      </div>
    </div>

  </div>
</div>


<!-- UC Berkeley — Clinical Optometry Faculty Search -->
<div class="card md-6" data-tags="job faculty clinical-optometry optometry berkeley vision-science recruitment">
  <div class="title"><b>Assistant, Associate, or Full Professor of Clinical Optometry — UC Berkeley</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Institution</div>
      <div>Herbert Wertheim School of Optometry & Vision Science, UC Berkeley</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Position</div>
      <div>Full-time faculty appointment (Assistant, Associate, or Full Professor)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Areas of Interest</div>
      <div>Clinical eye and vision care, including low vision, vision rehabilitation, geriatric vision, and related innovations.</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Application Link</div>
      <div><a href="https://aprecruit.berkeley.edu/JPF04456">aprecruit.berkeley.edu/JPF04456</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div>Maria Alegria, Academic HR Analyst • Search Chair: Prof. Meng Lin</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        Seeking candidates across broad areas of clinical optometry and vision science. Please share widely with potential applicants.
      </div>
    </div>

  </div>
</div>


    <div class="card md-6" id="bilkent-psychology-faculty-2026" data-tags="job faculty psychology neuroscience cognitive developmental social clinical turkey bilkent">
      <div class="title"><b>Tenure-Track Faculty Positions — Department of Psychology (Bilkent University)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Institution</div><div>Bilkent University, Department of Psychology</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Ankara, Turkey</div></div>
        <div class="meta-row"><div class="meta-label">Start Date</div><div>September 2026 (negotiable)</div></div>
        <div class="meta-row"><div class="meta-label">Rank</div><div>Open rank (Assistant, Associate, or Full Professor)</div></div>
        <div class="meta-row"><div class="meta-label">Areas</div><div>All areas of psychology, including social, developmental, cognitive, and clinical psychology</div></div>
        <div class="meta-row"><div class="meta-label">Overview</div>
          <div>Bilkent invites applications for multiple tenure-track positions as part of a departmental expansion. Successful applicants will maintain active research programs, supervise graduate students, and teach two courses per semester (no summer teaching).</div>
        </div>
        <div class="meta-row"><div class="meta-label">Facilities</div>
          <div>Newly renovated labs; access to 3 T MRI, animal research center, EEG suites, and interdisciplinary Neuroscience Program resources.</div>
        </div>
        <div class="meta-row"><div class="meta-label">Compensation</div>
          <div>Competitive salary, free on-campus housing, private health insurance, and International Baccalaureate schooling for dependents.</div>
        </div>
        <div class="meta-row"><div class="meta-label">Application Materials</div>
          <div>
            <ul class="notes-list">
              <li>Cover letter</li>
              <li>Curriculum Vitae</li>
              <li>Teaching and Research Statements</li>
              <li>PDFs of three representative publications</li>
              <li>Contact information for three professional references</li>
            </ul>
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>January 15 2026 (rolling review until filled)</div></div>
        <div class="meta-row"><div class="meta-label">Application Portal</div><div><a href="https://stars.bilkent.edu.tr/staffapp/PSYC2026OR">stars.bilkent.edu.tr/staffapp/PSYC2026OR</a></div></div>
        <div class="meta-row"><div class="meta-label">Department Info</div>
          <div><a href="https://psy.bilkent.edu.tr/">psy.bilkent.edu.tr</a> • <a href="https://neuro.bilkent.edu.tr/">neuro.bilkent.edu.tr</a> • <a href="https://feass.bilkent.edu.tr/">feass.bilkent.edu.tr</a></div>
        </div>
      </div>
    </div>


    <div class="card md-6" id="usm-psychology-teaching-professor-2026" data-tags="job faculty teaching-track psychology neuroscience developmental counseling usm mississippi">
      <div class="title"><b>Assistant Teaching Professor — School of Psychology (University of Southern Mississippi)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Institution</div><div>University of Southern Mississippi (USM)</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Hattiesburg, Mississippi</div></div>
        <div class="meta-row"><div class="meta-label">Start Date</div><div>August 2026</div></div>
        <div class="meta-row"><div class="meta-label">Positions</div><div>Two full-time, 9-month Assistant Teaching Professor appointments</div></div>
        <div class="meta-row"><div class="meta-label">Area</div><div>Behavioral Neuroscience and Developmental Psychology (open specialization; applied fields encouraged)</div></div>
        <div class="meta-row"><div class="meta-label">Overview</div>
          <div>Doctoral-level graduates with strong teaching ability are invited to apply for teaching-track faculty roles supporting undergraduate and graduate programs at USM’s Hattiesburg campus. Positions are permanent with eligibility for promotion.</div>
        </div>
        <div class="meta-row"><div class="meta-label">Highlights</div>
          <div>
            <ul class="notes-list">
              <li>Opportunities for practicum supervision for license-eligible applicants</li>
              <li>Leadership potential within the Master’s Counseling Psychology program</li>
              <li>Peer supervision available during licensure process</li>
              <li>Visa sponsorship not available</li>
            </ul>
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Application Details</div>
          <div>
            <ul class="notes-list">
              <li>Review begins October 1, 2025, and continues until filled</li>
              <li>Submit: cover letter, CV, 3 references, academic transcripts, and teaching evaluations (if applicable)</li>
              <li>Apply online: <a href="https://usm.csod.com/ux/ats/careersite/1/home/requisition/4902?c=usm&sq=req4902">USM Job Portal</a></li>
              <li>Contact: <a href="mailto:kristy.mcraney@usm.edu">Dr. Kristy McRaney</a> (Search Chair)</li>
            </ul>
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Department Info</div>
          <div>The School of Psychology serves ~600 undergraduates and 125 graduate students across counseling, clinical, school, and experimental psychology programs. Visit <a href="http://www.usm.edu/psychology">usm.edu/psychology</a> for details.</div>
        </div>
      </div>
    </div>


    <div class="card md-6" id="ai-chairs-enact-strasbourg-2026" data-tags="job faculty ai chair research france enact healthcare">
      <div class="title"><b>AI Chair Positions — ENACT Cluster (University of Strasbourg)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Institution</div><div>University of Strasbourg • AI Cluster ENACT • IHU Strasbourg • ICube Laboratory</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Strasbourg, France</div></div>
        <div class="meta-row"><div class="meta-label">Program</div>
          <div>ENACT — a France 2030–funded initiative uniting AI research and education partners across Eastern France. </div>
        </div>
        <div class="meta-row"><div class="meta-label">Focus Areas</div>
          <div>Artificial Intelligence for Healthcare, in collaboration with IHU Strasbourg and the ICube Laboratory. </div>
        </div>
        <div class="meta-row"><div class="meta-label">Application Info</div>
          <div>
            <a href="https://cluster-ia-enact.ai/">Program website</a> •
            <a href="https://cluster-ia-enact.ai/wp-content/uploads/2025/10/External-Chairs-Call-for-Proposals-ENG-v1.pdf">Call for Proposals (PDF)</a>
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Contact</div>
          <div><a href="mailto:npadoy@unistra.fr">Prof. Nicolas Padoy</a> (IHU Strasbourg / CAMMA Group)</div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>Multiple Chair positions available; strong support package and collaboration opportunities with regional research institutes. </div>
        </div>
      </div>
    </div>



    <!-- Faculty: Vanderbilt University — Computation & Psychology -->
    <div class="card md-6" data-tags="jobs faculty computation psychology ai vanderbilt nashville">
      <div class="title"><b>Open-Rank Faculty Search — Computation &amp; Psychology (Vanderbilt University)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>Vanderbilt University — College of Connected Computing</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Nashville, Tennessee, USA</div></div>
        <div class="meta-row"><div class="meta-label">Rank</div><div>Assistant (tenure-track), Associate, or Full Professor</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>Application review begins November&nbsp;5,&nbsp;2025</div></div>
        <div class="meta-row"><div class="meta-label">Start</div><div>Fall&nbsp;2026 (anticipated)</div></div>
        <div class="meta-row"><div class="meta-label">Fields</div>
          <div>
            Computational models of cognition · Computational social psychology · Affective computing · Clinical informatics · Computational models of development · Machine learning &amp; psychometrics · Computational neuroscience · NLP · Agentic &amp; generative AI
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Qualifications</div>
          <div>
            Ph.D. in computer science, psychology, neuroscience, data science, or related field; strong publication record in computational methods (e.g., ML, modeling, AI); collaborative, interdisciplinary research experience.
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Application</div>
          <div>
            Apply via Interfolio:
            <a href="https://apply.interfolio.com/175958">Assistant (TT)</a> ·
            <a href="https://apply.interfolio.com/175978">Associate/Full</a>
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:hansen.schwartz@vanderbilt.edu">Andy&nbsp;Schwartz</a> (Search&nbsp;Chair)</div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Part of Vanderbilt’s new College of Connected Computing, integrating psychological theory with computation and AI.</li>
              <li>Opportunities for secondary/joint appointments with Psychology and related departments.</li>
              <li>Emphasis on building interdisciplinary graduate and academic programs in computational psychology.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>



    <div class="card md-6">
      <div class="title"><b>New York University Abu&nbsp;Dhabi — Psychology Professor (Tenured/Tenure-Track)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Institution</div><div>New York University Abu&nbsp;Dhabi (Division of Science)</div></div>
        <div class="meta-row"><div class="meta-label">Position</div><div>Assistant, Associate, or Full Professor — Psychology</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>November&nbsp;30,&nbsp;2025</div></div>
        <div class="meta-row"><div class="meta-label">Areas</div>
          <div>Perception, Cognition, Cognitive&nbsp;Development, Social&nbsp;Psychology, Developmental&nbsp;Approaches</div>
        </div>
        <div class="meta-row"><div class="meta-label">Website</div>
          <div><a href="https://apply.interfolio.com/175417">apply.interfolio.com/175417</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Open to early-career and established scholars with demonstrated excellence in research and leadership.</li>
              <li>Strong research environment with interdisciplinary opportunities across NYU’s global network.</li>
              <li>Access to world-class facilities including a Siemens Prisma 3T MRI, MEG systems (SQUID &amp; OPM), multimodal EEG, VR, and child observation labs.</li>
              <li>For inquiries: <a href="mailto:nyuad.academicrecruitment@nyu.edu">nyuad.academicrecruitment@nyu.edu</a></li>
              <li>Equal-opportunity employer; applications from diverse candidates encouraged.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>


    <div class="card md-6">
      <div class="title"><b>Assistant Professor — Cognitive &amp; Brain Sciences (UNR)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Organization</div><div>University of Nevada, Reno — Department of Psychology</div></div>
        <div class="meta-row"><div class="meta-label">Start</div><div>July 1, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Reno, Nevada (USA)</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>Review begins December 1, 2025</div></div>
        <div class="meta-row"><div class="meta-label">Website</div>
          <div><a href="https://nshe.wd1.myworkdayjobs.com/UNR-external/job/University-of-Nevada-Reno---Main-Campus/Assistant-Professor--Cognitive-and-Brain-Sciences_R0149374">Apply (Workday)</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Contact</div>
          <div><a href="mailto:shaigh@unr.edu">Sarah Haigh</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Focus: perception, attention, memory, language, decision-making, cognitive control, or neural mechanisms of cognition.</li>
              <li>Methods may include fMRI, EEG, fNIRS, computational modeling / ML, TMS / tDCS, adaptive optics.</li>
              <li>Department of Psychology (CBS program) + Institute for Neuroscience with shared EEG, 3 T MRI, fNIRS and modeling facilities.</li>
              <li>Application materials: CV, cover letter, research and teaching statements, three references.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>


    <!-- Faculty (Teaching Track): University of Chicago — Instructional Professor, Psychology -->
    <div class="card md-6" data-tags="jobs faculty teaching-track psychology uchicago instructional professor">
      <div class="title"><b>University of Chicago — Instructional Professor (Assistant/Associate/Full), Psychology</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Rank</div><div>Instructional Professor (Assistant/Associate/Full)</div></div>
        <div class="meta-row"><div class="meta-label">Start</div><div>On or after July&nbsp;1,&nbsp;2026</div></div>
        <div class="meta-row"><div class="meta-label">Teaching</div><div>6 quarter-long undergraduate courses per year</div></div>
        <div class="meta-row"><div class="meta-label">Areas</div>
          <div>Broad coverage in perception, action, cognition; advanced research methods; experience conducting psychological research.</div>
        </div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Chicago, IL (USA)</div></div>
        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div>
            <a href="https://apply.interfolio.com/173929">Apply via Interfolio</a> •
            <a href="https://psychology.uchicago.edu/">Department of Psychology</a>
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>Review begins November&nbsp;3,&nbsp;2025 (rolling until filled)</div></div>
        <div class="meta-row"><div class="meta-label">Contact</div>
          <div><a href="mailto:mdrosenberg@uchicago.edu">Monica&nbsp;Rosenberg</a> (search contact)</div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>Full-time, career-track teaching position (3–5 year renewable term; promotion within IP track). Union-covered role; benefits per University policy.</div>
        </div>
      </div>
    </div>


    <!-- Faculty: University of California, Irvine — Cognitive Sciences -->
    <div class="card md-6" data-tags="jobs faculty cognitive-science uci california ai modeling perception">
      <div class="title"><b>University of California, Irvine — Assistant Professor, Cognitive Sciences</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Rank</div><div>Tenure-track Assistant Professor</div></div>
        <div class="meta-row"><div class="meta-label">Start</div><div>July 1, 2026</div></div>
        <div class="meta-row">
          <div class="meta-label">Areas</div>
          <div>
            Empirical human cognition (perception, memory, decision-making) combined with computational modeling,
            cognitive modeling, or data science. Interest in AI approaches and human–AI interaction encouraged.
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Irvine, California (USA)</div></div>

        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div>
            <a href="https://recruit.ap.uci.edu/JPF09896">Application portal</a> •
            <a href="https://www.cogsci.uci.edu/">Department of Cognitive Sciences</a>
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>November&nbsp;15,&nbsp;2025 (priority review)</div></div>
        <div class="meta-row">
          <div class="meta-label">Contact</div>
          <div><a href="mailto:zpizlo@uci.edu">Zygmunt&nbsp;Pizlo</a></div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Notes</div>
          <div>
            Research may bridge cognitive science, neuroscience, computer science, and AI.
            UCI offers strong interdisciplinary collaborations across cognitive science, engineering, and statistics.
          </div>
        </div>
      </div>
    </div>



    <div class="card md-6">
      <div class="title"><b>Assistant Professor in Cognitive &amp; Psychological Sciences (AI &amp; the Mind)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>Start July 1, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Brown University — Department of Cognitive &amp; Psychological Sciences (CoPsy), Providence, RI</div></div>
        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div><a href="https://apply.interfolio.com/173939">apply.interfolio.com/173939</a></div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Area: AI &amp; the Mind; intersections of AI and cognition</li>
              <li>Examples: AI models of cognition, BCI, cognitive enhancement, human–AI interaction, algorithmic bias</li>
              <li><b>Full-review deadline:</b> November 8, 2025</li>
              <li>Materials: CV; 3 pubs/preprints; research (≤2 pp) &amp; teaching statements (≤1 pp); 3 letters via Interfolio</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    
    <div class="card md-6">
      <div class="title"><b>Assistant Professor in Psychology (Cognitive Neuroscience)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>Start Aug 7, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>University of South Florida, Tampa, FL</div></div>
        <div class="meta-row"><div class="meta-label">Website</div>
          <div><a href="https://fa-ewkd-saasfaprod1.fa.ocs.oraclecloud.com/hcmUI/CandidateExperience/en/sites/USF/job/41553/?utm_medium=jobshare&utm_source=External+Job+Share">USF Job 41553</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Focus on human cognition</li>
              <li>Screening begins Oct 17, 2025</li>
              <li>Contact: <a href="mailto:ratchley@usf.edu">ratchley@usf.edu</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>Senior Lecturer in Psychology</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>Start Aug 16, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Vanderbilt University, Nashville, TN</div></div>
        <div class="meta-row"><div class="meta-label">Website</div>
          <div><a href="https://apply.interfolio.com/173008">apply.interfolio.com/173008</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Non-tenure-track instructional faculty (3-year renewable)</li>
              <li>Teach General Psychology plus quantitative/experimental and elective courses</li>
              <li>PhD required by Aug 1, 2026</li>
              <li>Screening begins Dec 2025</li>
              <li>Contact: <a href="mailto:isabel.gauthier@vanderbilt.edu">isabel.gauthier@vanderbilt.edu</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>Senior Faculty Position — Psychological &amp; Brain Sciences</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>Start date TBD</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>University of Iowa — Department of Psychological &amp; Brain Sciences</div></div>
        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div><a href="https://psychology.uiowa.edu/about/job-openings">psychology.uiowa.edu/about/job-openings</a></div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Rank: Associate or Full Professor</li>
              <li>Research area open; search details via department website</li>
              <li>No fixed application deadline listed</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>Assistant Professor — Computational Cognitive Neuroscience</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Category</div><div>Faculty (Tenure-Track)</div></div>
        <div class="meta-row"><div class="meta-label">Deadline</div><div>Open until filled</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>University at Albany (SUNY) — Department of Psychology, Albany, NY</div></div>
        <div class="meta-row">
          <div class="meta-label">Website</div>
          <div><a href="https://albany.interviewexchange.com/jobofferdetails.jsp?JOBID=193092">albany.interviewexchange.com/jobofferdetails.jsp?JOBID=193092</a></div>
        </div>
        <div class="meta-row">
          <div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Focus: human cognition &amp; brain with computational approaches (AI/ML/neuromorphic)</li>
              <li>Intersects modern neuroscience methods; departmental appointment in Psychology</li>
              <li>Start date per posting; standard materials via portal</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

  </div> <!-- end cards: Faculty Positions -->


<!-- Funding -->
<h3 id="funding">Funding</h3>
<p class="section-desc">
  Travel awards, fellowships, and research support opportunities with typical timelines and eligibility notes.
</p>

<!-- ====================== -->
<!-- Travel & Networking    -->
<!-- ====================== -->
<h4>Travel &amp; Networking Awards</h4>
<div class="cards">

  <div class="card md-6" data-tags="funding travel-award networking vss diversity early-career recurring">
    <div class="title"><b>Females of Vision et al. Travel and Networking Award</b></div>
    <div class="meta">
      <div class="meta-row">
        <div class="meta-label">Dates</div>
        <div>Deadline TBD (typically March; check site for current call)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Location</div>
        <div>Vision Sciences Society Annual Meeting</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div>
          <a href="http://www.foveavision.org/awards/accepting-applications">
            http://www.foveavision.org/awards/accepting-applications
          </a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>$1,000 each, up to 8 awards</li>
            <li>Open to women, transgender, intersex, or non-binary graduate students, postdocs, or early-career vision scientists</li>
            <li>Contact: <a href="mailto:fovea.award@gmail.com">fovea.award@gmail.com</a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>

</div> <!-- end cards: Travel & Networking Awards -->


<!-- ================================ -->
<!-- Fellowships & Career Development -->
<!-- ================================ -->
<h4>Fellowships &amp; Career Development</h4>
<div class="cards">

  <div class="card md-6" data-tags="funding fellowship postdoc neuroscience brain allen-institute uw seattle 2026">
    <div class="title"><b>Shanahan Foundation Fellowship (Allen Institute + University of Washington)</b></div>
    <div class="meta">
      <div class="meta-row">
        <div class="meta-label">Dates</div>
        <div>Application deadline: December 1, 2025 · Start date: Fall 2026</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Location</div>
        <div>Seattle, WA (Allen Institute &amp; University of Washington)</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div>
          <a href="https://alleninstitute.org/what-we-do/brain-science/careers/shanahan-foundation-fellowship/"
             aria-label="Shanahan Foundation Fellowship page">
            Allen Institute – Shanahan Fellowship
          </a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>Prestigious 3-year postdoctoral fellowship</li>
            <li>Joint mentorship across Allen Institute for Brain Science and the University of Washington</li>
            <li>Supports independent, high-risk/high-reward neuroscience research</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

</div> <!-- end cards: Fellowships & Career Development -->



<!-- Competitions and Prizes -->
<h3 id="competitions">Competitions and Prizes</h3>
<p class="section-desc">
  Community contests and prizes highlighting creativity and achievement (e.g., Illusion of the Year).
</p>

<h4>Awards &amp; Prizes</h4>
<div class="cards">

<!-- VSS 2026 – Society Awards (Nakayama, Davida Teller, Elsevier/VSS YI) -->
<div class="card md-6" id="vss-2026-awards"
     data-tags="awards prizes vss vision-science nakayama davida-teller early-career young-investigator">
  <div class="title">
    <b>VSS 2026 – Call for Award Nominations</b>
  </div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Society</div>
      <div>Vision Sciences Society (VSS)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Awards</div>
      <div>
        <b>Ken Nakayama Medal for Excellence in Vision Science</b><br>
        <b>Davida Teller Award</b><br>
        <b>Elsevier/VSS Young Investigator Award</b>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Schedule</div>
      <div>
        Nominations open: <b>November 24, 2025</b><br>
        Nominations close: <b>January 22, 2026</b><br>
        Recipients announced: <b>March 2026</b><br>
        Awards presented at <a href="#vss">VSS 2026</a>, May 15–19, 2026 (St. Pete Beach, Florida).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Ken Nakayama Medal</div>
      <div>
        Honors a senior investigator (≈25+ years post-terminal degree) whose fundamental, clinical, or applied
        work has made exceptional, lasting contributions to vision science.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Davida Teller Award</div>
      <div>
        Recognizes an outstanding mid-career vision scientist, with attention to impactful research,
        mentorship, and contributions to diversity and inclusion in the field.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Elsevier/VSS Young Investigator</div>
      <div>
        Early-career award sponsored by <i>Vision Research</i>, for outstanding fundamental, clinical, or applied work.
        Selection emphasizes significance, originality, and long-range impact; the awardee writes a review article for
        <i>Vision Research</i>.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Nomination Info</div>
      <div>
        Full nomination instructions and criteria are available on the VSS website:<br>
        <a href="https://www.visionsciences.org/vss-awards/" rel="noopener">
          visionsciences.org – VSS Awards
        </a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        Nominations should highlight scientific contributions, service to the community, and—where relevant—
        mentoring and efforts to broaden participation in vision science.
      </div>
    </div>

  </div>
</div>


  <!-- AWARD: ICVS 2026 Early Career Investigator Award -->
  <div class="card md-6" id="icvs-earlycareer-2026"
       data-tags="award competition icvs colour-vision early-career 2026">
    <div class="title"><b>ICVS 2026 Early Career Investigator Award</b></div>
    <div class="meta">
      <div class="meta-row">
        <div class="meta-label">Deadline</div>
        <div>January 16, 2026</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Presented At</div>
        <div>ICVS Symposium 2026 · Brighton, UK</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Eligibility</div>
        <div>
          Researchers ≤ 10 years post-PhD (extensions possible with justification). Recognizes exceptional
          achievement in colour vision research.
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Nomination Materials</div>
        <div>
          Letter of nomination + candidate CV to
          <a href="mailto:anya.hurlbert@newcastle.ac.uk">anya.hurlbert@newcastle.ac.uk</a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">More Info</div>
        <div><a href="https://www.icvs.info/">icvs.info</a></div>
      </div>
    </div>
  </div>

</div> <!-- end cards: Competitions and Prizes -->

<p style="text-align: center; margin: 24px 0;">
    CVNet is supported by the
    <a href="https://www.ski.org/">Smith-Kettlewell Eye Research Institute</a>
  </p>
    
<footer class="site-footer" role="contentinfo">
  <div class="footer-inner">
    <div class="footer-left">
      <small>
        © <span id="cal-year"></span> CVNet &nbsp;•&nbsp; This page is maintained by
       David Peterzell and Michaela Tedesco <span id="cal-updated"></span>
      </small>
    </div>

    <!-- Right: add the resource-style quick actions -->
    <nav class="footer-nav" aria-label="Footer">
      <a href="#top" aria-label="Back to top">Back to top ↑</a>
<a href="https://info.cvnet.org/" aria-label="Back to CVNet">CVNet</a>
      <a href="resources.html">Online Resources</a>
      <a href="./journals%20list.html">Journals</a>
      <a href="https://cvnet.org/mailman/listinfo/cvnet">CVNet Archive</a>
      <a href="http://visionscience.com/pipermail/visionlist_visionscience.com">VisionList Archive</a>
    </nav>
  </div>

   <div class="footer-sub">
    This calendar is community-curated. Send additions or corrections to  <a href="mailto:davidpeterzell@mac.com">davidpeterzell@mac.com</a> &
        <a href="mailto:mtedesco12@gmail.com">mtedesco12@gmail.com</a>.

  </div>
</footer>


<script>
  function walk(node, regex) {
    // Skip excluded containers entirely (header/nav or any [data-no-highlight] region)
    if (node.nodeType === 1) {
      if (node.closest && node.closest('.header-fixed,[data-no-highlight]')) return;
    }

    if (node.nodeType === 3) {
      const match = node.nodeValue.match(regex);
      if (match) {
        const span = document.createElement('span');
        const html = node.nodeValue.replace(regex, '<mark class="cvnet-highlight">$1</mark>');
        span.innerHTML = html;
        node.parentNode.replaceChild(span, node);
      }
    } else if (
      node.nodeType === 1 &&
      node.childNodes &&
      !['SCRIPT', 'STYLE', 'MARK'].includes(node.tagName)
    ) {
      for (let i = 0; i < node.childNodes.length; i++) {
        walk(node.childNodes[i], regex);
      }
    }
  }

  function removeHighlights() {
    document.querySelectorAll('mark.cvnet-highlight').forEach(mark => {
      const parent = mark.parentNode;
      parent.replaceChild(document.createTextNode(mark.textContent), mark);
      parent.normalize();
    });
  }

  function setHeaderOffset() {
    const fixed = document.querySelector('.header-fixed');
    if (!fixed) return;
    const h = fixed.offsetHeight || 0;
    document.body.style.paddingTop = (h + 10) + 'px';
    document.querySelectorAll('h3[id], h4[id]').forEach(el => {
      el.style.scrollMarginTop = (h + 10) + 'px';
    });
  }

function debounce(fn, ms) {
  let t;
  return function (...args) {
    clearTimeout(t);
    t = setTimeout(() => fn.apply(this, args), ms);
  };
}




  document.addEventListener('DOMContentLoaded', function () {
    const input = document.getElementById('search-input');

    setHeaderOffset();

    let searchTimer = null;
    let lastQuery = '';
    function highlightFilter(value) {
      removeHighlights();
      if (!value || value.length < 2 || value === lastQuery) {
        if (!value) lastQuery = '';
        return;
      }
      lastQuery = value;
      const regex = new RegExp('(' + value.replace(/[.*+?^${}()|[\]\\]/g, '\\$&') + ')', 'gi');
      // Only search within main content areas, never in the fixed header
      document.querySelectorAll('.page').forEach(root => {
        walk(root, regex);
      });
    }

    input.addEventListener('input', () => {
      clearTimeout(searchTimer);
      searchTimer = setTimeout(() => {
        const q = input.value.trim();
        if (!q) { lastQuery = ''; removeHighlights(); return; }
        highlightFilter(q);
      }, 180);
    });
  });

  window.addEventListener('resize', debounce(setHeaderOffset, 100));
  // Also recalc after full assets load to avoid initial anchor "peek"
  window.addEventListener('load', setHeaderOffset);
</script>


<script>
(() => {
  const month = {jan:0,feb:1,mar:2,apr:3,may:4,jun:5,jul:6,aug:7,sep:8,sept:8,oct:9,nov:10,dec:11};
  const re = /\b(January|February|March|April|May|June|July|August|September|Sept\.?|October|November|December|Jan\.?|Feb\.?|Mar\.?|Apr\.?|Jun\.?|Jul\.?|Aug\.?|Oct\.?|Nov\.?|Dec\.?)\s+(\d{1,2}),\s*(\d{4})\b/g;
  const today = new Date(); today.setHours(0,0,0,0);

  document.querySelectorAll('ul.notes-list li').forEach(li => {
    const t = li.textContent;
    let m, future = false;
    re.lastIndex = 0;
    while ((m = re.exec(t))) {
      const mm = month[m[1].toLowerCase().replace('.', '').slice(0,3)];
      const dd = +m[2], yy = +m[3];
      if (mm == null) continue;
      const d = new Date(yy, mm, dd); d.setHours(0,0,0,0);
      if (d >= today) { future = true; break; }
    }
    if (future) {
      li.classList.add('deadline-open');
      if (!li.querySelector('.visually-hidden')) {
        const sr = document.createElement('span');
        sr.className = 'visually-hidden';
        sr.textContent = ' (deadline open)';
        li.appendChild(sr);
      }
    }
  });
})();
</script>
<script>
(function () {
  const $input = document.getElementById('search-input');
  if (!$input) return;

  const $status = document.getElementById('search-status');
  const cards = Array.from(document.querySelectorAll('.cards .card'))
    .filter(el => !el.classList.contains('intro')); // keep the intro card untouched

  // Precompute searchable text for speed
  const haystacks = new Map();
  const norm = s => (s || '').normalize('NFKD').toLowerCase();
  const clean = s => s.replace(/\s+/g, ' ').trim();

  function cardText(card) {
    const title = card.querySelector('.title')?.textContent || '';
    const meta = card.querySelector('.meta')?.textContent || '';
    const tags = card.getAttribute('data-tags') || '';
    const id   = card.id || '';
    return norm([title, meta, tags.replace(/[-_]/g, ' '), id].join(' '));
  }
  function ensureHaystack(card) {
    if (!haystacks.has(card)) haystacks.set(card, cardText(card));
    return haystacks.get(card);
  }

  // Highlight utility (title only, for readability)
  function unhighlight(card) {
    const t = card.querySelector('.title');
    if (!t) return;
    // remove any previous <mark>
    t.innerHTML = t.textContent;
  }
  function highlight(card, tokens) {
    const t = card.querySelector('.title');
    if (!t) return;
    let text = t.textContent;
    for (const tok of tokens) {
      if (!tok) continue;
      const esc = tok.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
      text = text.replace(new RegExp(`(${esc})`, 'ig'), '<mark class="search-hit">$1</mark>');
    }
    t.innerHTML = text;
  }

  let debounceTimer = null;
  function onSearch() {
    const q = clean($input.value);
    const tokens = q.split(' ').filter(Boolean).map(norm);

    let visibleCount = 0;
    for (const card of cards) {
      unhighlight(card);
      if (tokens.length === 0) {
        card.classList.remove('is-filtered-out');
        continue;
      }
      const hay = ensureHaystack(card);
      const matchAll = tokens.every(tok => hay.includes(tok));
      card.classList.toggle('is-filtered-out', !matchAll);
      if (matchAll) {
        highlight(card, tokens);
        visibleCount++;
      }
    }
    if ($status) {
      $status.textContent = tokens.length
        ? `${visibleCount} result${visibleCount === 1 ? '' : 's'}`
        : `Showing all`;
    }
  }

  function debounced() {
    clearTimeout(debounceTimer);
    debounceTimer = setTimeout(onSearch, 120);
  }

  // Clear on Escape
  $input.addEventListener('keydown', (e) => {
    if (e.key === 'Escape' && $input.value) {
      $input.value = '';
      debounced();
      $input.blur();
    }
  });

  $input.addEventListener('input', debounced);

  // If the page loads with a query param ?q=..., prefill and search
  const params = new URLSearchParams(location.search);
  const preset = params.get('q');
  if (preset) {
    $input.value = preset;
    onSearch();
  }
})();
</script>
</body>
</html>
