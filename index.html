<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QMCHQQ8STW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-QMCHQQ8STW');
</script>  
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CVNet - Community Calendar</title>
<meta name="theme-color" content="#FFD12D"/>
<style>

/* Search filtering + highlight */
.is-filtered-out { display: none !important; }
mark.search-hit { background: #fff3b0; padding: 0 .05em; border-radius: 2px; }


.section-desc {
  margin: 6px 0 8px;        /* lowered bottom so the +* can matter */
  color: #2f3a56;
  font-size: 0.95rem;
}

/* adds a little breathing room to whatever follows the blurb */
.section-desc + * {
  margin-top: 8px;
}


  /* Added: predictably size elements and improve wrapping */
  *, *::before, *::after { box-sizing: border-box; }

  :root{
  --brand-yellow:#FFD12D;
  --brand-blue:#000066;
  --text:#000;
  --muted:#f5f7fb;
  --border:#dfe3ea;
  --card-bg:#ffffff;
  --card-shadow: 0 6px 18px rgba(0,0,0,.08), 0 2px 8px rgba(0,0,0,.06);
  --card-shadow-hover: 0 10px 26px rgba(0,0,0,.10), 0 6px 14px rgba(0,0,0,.08);
  /* Helps anchor jumps clear the fixed header */
  scroll-padding-top: 90px;
}

html { scroll-behavior: smooth; }
  body {
    font-family: Arial, sans-serif;
    color: var(--text);
    background-color: var(--muted);
    margin: 0;
    line-height: 1.6;
    font-size: 16px;
    -webkit-text-size-adjust: 100%;
    text-size-adjust: 100%;
  }

  /* Page container margins for content */
  .page {
    margin: 20px;
  }

  /* ===== Fixed header container (covers top edge; content scrolls beneath) ===== */
  .header-fixed {
    position: fixed;
    top: 0; left: 0; right: 0;
    z-index: 1000;
    background: #fff; /* prevents content showing through above header */
    box-shadow: 0 1px 0 rgba(0,0,0,.05);
  }
  /* Inner header respects page side margins and leaves space above yellow banner */
  .site-header {
    margin: 0 20px;         /* aligns with body’s side margin */
    padding-top: 16px;      /* gap above yellow banner */
  }

  /* Title banner */
  .title-banner{
    background: var(--brand-yellow);
    border-radius: 10px;
    padding: 16px 20px;
    text-align:center;
  }
  .title-banner h2{
    margin: 0;
    /* Added: allow long page titles to wrap safely on small screens */
    overflow-wrap:anywhere;
    word-break: break-word;
  }
  .title-banner .subtitle{
    margin-top: 6px;
    font-size: 14px;
  }

  /* Search and nav links */
  .topbar{
    display:flex;
    flex-direction: column;      /* stack main row + back link */
    align-items:flex-start;
    gap: 8px;
    margin: 14px 0 16px 0;
  }
  .topbar-row{
    display:flex;
    width: 100%;
    justify-content: space-between;
    align-items: center;
    gap: 12px;
    flex-wrap: wrap;
  }
  .quick-links{
    display:flex;
    gap:14px;
    flex-wrap: wrap;
  }
  .quick-links a{
    color: var(--brand-blue);
    text-decoration:none;
    font-weight: bold;
    background: rgba(0,0,102,.06);
    padding: 6px 10px;
    border-radius: 999px;
    border: 1px solid rgba(0,0,102,.12);
    white-space: nowrap; /* keep single-line labels */
  }
  .quick-links a:hover{ text-decoration: underline; }

.cv-conference-links{
  display:flex;
  flex-wrap:wrap;
  gap:.4rem;
  margin-bottom:.8rem;
}

  .topbar .back-link{
    margin: 0;
  }

  .search-wrap{
    margin-left:auto;
    display:flex;
    align-items:center;
    gap:8px;
  }
  .search-wrap .icon{ font-size:18px; line-height:1; }
  #search-input{
    width: 280px;
    max-width: 50vw;
    padding: 10px 12px;
    border:1px solid #cfd6e4;
    border-radius:999px;
    background:#fff;
  }

  h3 {
    color: var(--brand-blue);
    border-bottom: 3px solid var(--brand-yellow);
    padding-bottom: 6px;
    margin: 28px 0 14px;
  }
  h4 { margin: 18px 0 8px; color: var(--brand-blue); }

  a {
    color: #003366; text-decoration: none;
    /* Added: ensure very long URLs or emails wrap instead of overflowing */
    overflow-wrap:anywhere;
    word-break: break-word;
  }
  a:hover { text-decoration: underline; }

  mark.cvnet-highlight {
    background-color: yellow;
    color: black;
  }

/* ===== Card layout ===== */
.cards {
  display: grid;
  grid-template-columns: repeat(12, 1fr);
  gap: 14px;
  margin-bottom: 24px;
}

.card {
  grid-column: span 12; /* full width by default (mobile-first) */
  background: var(--card-bg);
  border: 1px solid var(--border);
  border-radius: 12px;
  box-shadow: var(--card-shadow);
  padding: 14px;
  transition: box-shadow .2s ease, transform .2s ease;
  /* Added: make sure content inside cards can wrap and never overflow */
  overflow-wrap: anywhere;
  word-break: break-word;
}

.card:hover {
  box-shadow: var(--card-shadow-hover);
  transform: translateY(-1px);
}

.card .title {
  font-weight: 700;
  color: #001652;
  margin-bottom: 6px;
  line-height: 1.35;
  /* Added: prevent long titles from causing overflow */
  overflow-wrap: anywhere;
  word-break: break-word;
}

.meta {
  display: grid;
  grid-template-columns: 1fr;
  gap: 8px;
}

.meta-row {
  display: grid;
  grid-template-columns: 160px minmax(0,1fr);
  gap: 10px;
  align-items: start;
}

.meta-label {
  font-weight: 700;
  color: var(--brand-blue);
}

.notes-list {
  margin: 0;
  padding-left: 1.1em;
}

.notes-list li {
  margin: 2px 0;
}

/* Added: guarantee the value column actually shrinks and wraps on narrow screens */
.meta-row > :last-child {
  min-width: 0;
  overflow-wrap: anywhere;
  word-break: break-word;
}

/* Section intro paragraph card */
.intro {
  background: #fffaf0;
  border: 1px solid #ffe3a3;
  color: #3a2a00;
}

/* Responsive columns */
@media (min-width: 720px) {
  .card.sm-6 { grid-column: span 6; }
  .card.sm-4 { grid-column: span 4; }
  .card.md-6 { grid-column: span 6; } /* ensure half width at medium+ */
}

/* Mobile niceties & quick-links scroll */
@media (max-width: 700px) {
  .site-header { margin: 0 12px; padding-top: 12px; }
  .topbar-row { gap: 8px; }
  .quick-links {
    flex-wrap: nowrap;
    overflow-x: auto;
    -webkit-overflow-scrolling: touch;
    gap: 10px;
    scroll-snap-type: x mandatory;
    padding-bottom: 6px;
  }
  .quick-links a {
    scroll-snap-align: start;
  }
  .search-wrap { width: 100%; margin-left: 0; }
  #search-input { width: 100%; max-width: 100%; }

  /* Slightly narrower label on phones; allow label wrapping */
  .meta-row { grid-template-columns: minmax(90px,140px) minmax(0,1fr); }
  .meta-label { overflow-wrap:anywhere; }
}

/* ✅ Accessible focus outlines for keyboard users */
a:focus-visible,
button:focus-visible,
input:focus-visible,
select:focus-visible,
textarea:focus-visible {
  outline: 2px solid var(--brand-yellow);
  outline-offset: 2px;
  border-radius: 6px;
}

/* ✅ Improve search input UX */
input[type="search"] {
  -webkit-appearance: textfield;
  accent-color: var(--brand-blue);
  padding-right: 2.2em;
}
input[type="search"]::-webkit-search-cancel-button { cursor: pointer; }

/* ✅ Skip link for keyboard users */
.sr-skip {
  position:absolute; left:-9999px; top:auto; width:1px; height:1px; overflow:hidden;
}
.sr-skip:focus {
  position:fixed; left:16px; top:16px; width:auto; height:auto; padding:8px 12px;
  background:#fff; border:2px solid var(--brand-yellow); border-radius:8px; z-index:1001;
}

/* ✅ Respect reduced-motion preferences */
@media (prefers-reduced-motion: reduce) {
  html { scroll-behavior: auto; }
  .card { transition: none; }
  .card:hover { transform: none; }
}

/* === Footer (calendar + resources hybrid) ================================ */
.site-footer{
  background:#fff;
  border-top:1px solid var(--border);
  margin-top:32px;
}
.footer-inner{
  margin:0 20px;
  padding:18px 0;
  display:flex;
  flex-wrap:wrap;
  gap:12px;
  justify-content:space-between;
  align-items:center;
}
.footer-left small{ color:#2f3a56; }
.footer-nav{
  display:flex;
  gap:14px;
  flex-wrap:wrap;
}
.footer-nav a{
  color:var(--brand-blue);
  text-decoration:none;
  font-weight:bold;
  border:1px solid rgba(0,0,102,.12);
  background:rgba(0,0,102,.06);
  padding:6px 10px;
  border-radius:999px;
}
.footer-nav a:hover{ text-decoration: underline; }

/* optional micro-row if you ever want a second line */
.footer-sub{
  margin:0 20px;
  padding:0 0 16px;
  color:#56607a;
  font-size:.9rem;
}

.deadline-open{
  font-weight: 600;
  color: var(--brand-blue, #000066);
  text-decoration: underline;
  text-underline-offset: 2px;
}
.visually-hidden{
  position:absolute;left:-9999px;width:1px;height:1px;overflow:hidden;
}


</style>

<!-- ✅ External links: open safely in a new tab (robust) -->
<script>
  document.addEventListener('DOMContentLoaded', () => {
    const { host } = location;
    document.querySelectorAll('a[href]').forEach(a => {
      try {
        const url = new URL(a.getAttribute('href'), location.origin);
        const isHttp = url.protocol === 'http:' || url.protocol === 'https:';
        const external = isHttp && url.host !== host;
        if (external) { a.target = '_blank'; a.rel = 'noopener noreferrer'; }
      } catch (e) {}
    });
  });
</script>


</head>
<body>
<a href="#conferences" class="sr-skip">Skip to main content</a>
<!-- Fixed header container that spans the very top -->
<div class="header-fixed">
  <header class="site-header">
    <div class="title-banner">
      <h2>CVNet Community Calendar</h2>
      <div class="subtitle">Last updated: 24 November, 2025</div> 
    </div>

    <div class="topbar">
      <div class="topbar-row">
        <nav class="quick-links" aria-label="Quick section links">
  <a href="#conferences">Conferences</a>
  <a href="#talks">Online Seminars/Clubs</a>
  <a href="#calls">Special/Feature Issues</a>
  <a href="#education">Education</a>
  <a href="#gradprograms">Grad Programs</a>
  <a href="#jobs">Jobs</a>
  <a href="#funding">Funding</a>
  <a href="#competitions">Competitions</a>
</nav>
        <div class="search">
  <input id="search-input" type="search" placeholder="Search..." />
</div>

<!-- Search result counter (for accessibility + user feedback) -->
<div id="search-status" class="visually-hidden" aria-live="polite"></div>

      </div>
      <!-- Back link under main nav buttons -->
      <p class="back-link">
        <a href="https://info.cvnet.org/">← Back to main CVNet info page</a>
      </p>
    </div>
  </header>
</div>

<div class="page" id="top"></div>

<div class="page">
  <div class="card intro">
    This page lists upcoming <strong>conferences, courses, talks, jobs</strong>, and <strong>funding opportunities</strong> for the color &amp; vision research community. Send additions or corrections to
    <a href="mailto:davidpeterzell@mac.com">davidpeterzell@mac.com</a> and
    <a href="mailto:mtedesco12@gmail.com">mtedesco12@gmail.com</a>. Additional opportunities can be found in the
    <a href="https://cvnet.org/mailman/listinfo/cvnet">CVNet Archive</a> and the
    <a href="http://visionscience.com/pipermail/visionlist_visionscience.com">VisionList Archive</a>.
  </div>

 <!-- Conferences -->
<h3 id="conferences">Conferences</h3>
<p class="section-desc">
  Major academic and industry meetings in vision science, neuroscience, imaging, color, and related fields. Includes key dates for abstracts, registration, and workshops. Conferences are listed in chronological order by event date (not by submission deadlines).
</p>
<!-- Conference Quick Links -->
<nav class="cv-conference-links quick-links" aria-label="Conference quick links">
  <a href="#apa" class="cv-btn">APA</a>
  <a href="#apcv" class="cv-btn">APCV</a>
  <a href="#aps" class="cv-btn">APS</a>
  <a href="#arvo" class="cv-btn">ARVO</a>
  <a href="#ava" class="cv-btn">AVA</a>
  <a href="#bavrd" class="cv-btn">BAVRD</a>
  <a href="#ecvp" class="cv-btn">ECVP</a>
  <a href="#gruppo-del-colore" class="cv-btn">Gruppo del Colore</a>
  <a href="#hvei" class="cv-btn">HVEI</a>
  <a href="#icvs" class="cv-btn">ICVS</a>
  <a href="#modvis" class="cv-btn">MODVIS</a>
  <a href="#optica-fall-vision" class="cv-btn">Optica Fall Vision</a>
  <a href="#psychonomics" class="cv-btn">Psychonomics</a>
  <a href="#sfn" class="cv-btn">SfN</a>
  <a href="#vsac" class="cv-btn">VSAC</a>
  <a href="#vss" class="cv-btn">VSS</a>
</nav>
<div class="cards">


    <!-- Each conference as a card -->    
<div class="card md-6">
  <div class="title"><b>CVMP 2025 — ACM SIGGRAPH European Conference on Visual Media Production</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual Conference</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>December 3–4, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>BFI Southbank, London, UK</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.cvmp-conference.org/2025/">cvmp-conference.org/2025</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Deadline for short papers &amp; demos:</b> September 19, 2025 (extended)</li>
          <li>Bring together researchers &amp; industry in video processing, imaging, graphics, and production practice</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="card md-6" id="ava-xmas-2025" data-tags="conference ava uk vision-science optometry perception aston birmingham">
  <div class="title"><b>AVA Xmas Meeting 2025</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Date</div><div>December 15, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Aston University, Birmingham, UK</div></div>
    <div class="meta-row"><div class="meta-label">Organizer</div><div>Applied Vision Association (AVA)</div></div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>Annual end-of-year meeting of the AVA community, featuring talks and posters across vision science and perception research. Especially supportive of early-career and student presenters. Abstracts published in <i>Perception</i> (SAGE).</div>
    </div>
    <div class="meta-row"><div class="meta-label">Abstract Deadline</div><div>November 13, 2025 (5:00 PM UK)</div></div>
    <div class="meta-row"><div class="meta-label">Registration</div><div><a href="https://forms.office.com/e/z0Ci74AY9V">Initial registration form</a> (collects attendee info; payment link to follow)</div></div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Encourages submissions from PhD and Master’s students</li>
          <li>Abstracts follow Perception format; re-publication opt-out available</li>
          <li>Organized by Dr. Samantha Strong (Aston University)</li>
          <li>Announcement posted via AVA Listserv on Oct 31, 2025</li>
        </ul>
      </div>
    </div>
  </div>
</div>

    <!-- 2026 -->

<div class="card md-6">
  <div class="title"><b>AAAI 2026 Workshop — Neuro for AI &amp; AI for Neuro: Towards Multi-Modal Natural Intelligence</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Workshop @ AAAI</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>January 27, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Singapore</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://neuroai-multimodal-workshop.github.io/">Workshop site</a> •
        <a href="https://openreview.net/group?id=AAAI.org/2026/Workshop/NeuroAI">OpenReview submissions</a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Submission deadline:</b> October 30, 2025</li>
          <li>Topics: Neuro&nbsp;→&nbsp;AI &amp; AI&nbsp;→&nbsp;Neuro; multimodal natural intelligence</li>
          <li>Invited speakers include Chklovskii, Tolias, Schrimpf, Chen, Razi, Shou</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<div class="card md-6" id="innovations-in-neuroimaging-methods-nyuad-2026" data-tags="conference neuroimaging methods cognitive-neuroscience tutorials posters panels uae nyuad">
  <div class="title"><b>Innovations in Neuroimaging Methods — NYU Abu Dhabi 2026</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>February 9–11, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>NYU Abu Dhabi, United Arab Emirates</div></div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>Conference bringing together an international cohort to assess the state of human cognitive neuroimaging and chart future directions, including tutorials for students and junior researchers.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Program</div>
      <div>Research talks • Poster sessions • Panels: “Women in Neuroscience”, “Neuroimaging in the UAE &amp; MENA” • Hands-on tutorials.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Confirmed Speakers</div>
      <div>Yanchao Bi • Olivia Cheung • Olivier Collignon • Ida Gobbini • Clayton Hickey • Angelika Lingnau • Liuba Papeo • Marius Peelen • Nathan Weisz</div>
    </div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://nyuad.nyu.edu/en/events/2026/february/innovations-in-neuroimaging-methods.html">NYUAD event page</a></div></div>
    <div class="meta-row"><div class="meta-label">Organizers</div><div>David Melcher (NYU Abu Dhabi) • Olivia Cheung (United Arab Emirates University)</div></div>
    <div class="meta-row"><div class="meta-label">In Collaboration With</div><div>NYUAD Center for Brain and Health • NYU Abu Dhabi Institute</div></div>
  </div>
</div>


<div class="card md-6" id="hvei">
      <div class="title"><b>Human Vision and Electronic Imaging (HVEI)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>March 1–5, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Hyatt Regency San Francisco Airport, Burlingame, California</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.imaging.org/IST/IST/Conferences/EI/EI2026/Conference/C_HVEI.aspx?23416eb0dbe7=3">imaging.org • HVEI 2026</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Extended submission deadline: September 12, 2025</li>
              <li>Author notifications: October 10, 2025</li>
              <li>Journal-first: Final manuscripts due October 31, 2025</li>
              <li>FastTrack (early online): January 9, 2026</li>
              <li>Non-FastTrack (post-symposium): March 23, 2026</li>
              <li>Demo application deadline: January 15, 2026, 09:00 (NY)</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

<div class="card md-6">
  <div class="title"><b>Winter Conference on Applications of Computer Vision (WACV)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>March 6–10, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>JW Marriott Starr Pass, Tucson, Arizona</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://wacv.thecvf.com/">wacv.thecvf.com</a></div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Call for Tutorial Proposals</b> — proposals due <b>October 27, 2025 (11:59 PM PDT)</b>; notifications by <b>November 17, 2025</b>.</li>
          <li>Tutorials will be held in person on <b>Friday, March 6</b> or <b>Saturday, March 7, 2026</b>.</li>
          <li>Submission: <a href="https://openreview.net/group?id=thecvf.com/WACV/2026/Tutorials">OpenReview — WACV 2026 Tutorials</a>. Chairs: Marta Gomez-Barrero &amp; Vishal M. Patel.</li>
          <li>Round 1: Reviews issued September 3, 2025</li>
          <li>Round 1: Camera-ready (accepted) September 18, 2025</li>
          <li>Round 2: New paper registration September 12, 2025</li>
          <li>Round 2: Submission September 19, 2025</li>
          <li>Final decisions: November 5, 2025</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- VisionDocs @ WACV 2026 -->
<div class="card md-6" id="visiondocs-wacv2026" data-tags="workshop wacv2026 document-analysis recognition computer-vision">
  <div class="title"><b>VisionDocs @ WACV 2026 — Computer Vision Systems for Document Analysis and Recognition</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Workshop (in conjunction with WACV 2026)</div></div>
<div class="meta-row"><div class="meta-label">Dates</div><div>March 6 or 7, 2026 (one-day workshop; exact day TBD)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>JW Marriott Starr Pass, Tucson, Arizona</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://sites.google.com/view/avml-lab-visiondocs-wacv2026/home">VisionDocs @ WACV 2026</a></div></div>
    <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:visiondocs.organizers@gmail.com">visiondocs.organizers@gmail.com</a></div></div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>
        3rd workshop on advancing document analysis and recognition through computer vision — tackling heterogeneous document classes, low-data environments, and multi-modal fusion strategies for improved accuracy.
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Topics</div>
      <div>
        Document image processing • Layout and handwriting recognition • Document forensics • Table and formula recognition • Multimedia and multi-modal document analysis • Graphics recognition • Structured document generation • Historical documents • Datasets and benchmarks.
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Key Dates (PST)</div>
      <div>
        <ul class="notes-list">
          <li><b>Paper submissions:</b> Dec 1, 2025 (11:59 PM)</li>
          <li><b>Notifications:</b> Jan 2, 2026 (11:59 PM)</li>
          <li><b>Camera-ready:</b> Jan 9, 2026 (11:59 PM)</li>
        </ul>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Organizers</div><div>Axel De Nardin, Silvia Zottin, Silvia Cascianelli, Claudio Piciarelli, Gian Luca Foresti</div></div>
  </div>
</div>


<!-- Workshop: Physical Retail AI Workshop (PRAW) @ WACV 2026 -->
<div class="card md-6" id="praw-wacv2026" data-tags="workshop wacv2026 retail ai computer-vision grocery datasets challenges">
  <div class="title"><b>3rd Physical Retail AI Workshop (PRAW) @ WACV 2026</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Workshop (in conjunction with WACV 2026)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>March 6–7, 2026</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Tucson, Arizona, USA</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://grocery-vision.github.io">grocery-vision.github.io</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        3rd edition of the Physical Retail AI Workshop at WACV, focusing on computer vision and AI
        for physical retail environments, with shared public datasets (Grocery Vision, RetailAction)
        and associated challenges in the grocery/retail domain.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Workshop Papers</div>
      <div>
        <ul class="notes-list">
          <li><b>Submission deadline:</b> December 19, 2025 (23:59, anywhere on earth)</li>
          <li><b>Notification of acceptance:</b> December 26, 2025</li>
          <li><b>Camera-ready deadline:</b> January 9, 2026</li>
        </ul>
        <div>Submission site: <a href="https://openreview.net/group?id=thecvf.com/WACV/2026/Workshop/PRAW">OpenReview (PRAW)</a></div>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Grocery Vision Challenges (GV26)</div>
      <div>
        <ul class="notes-list">
          <li><b>Tracks:</b> 
            Track 1 — Shopping Cart Video Temporal/Spatio-Temporal Action Localization (SC-TAL / SC-STAL); 
            Track 2 — Multi-modal Product Retrieval (MPR); 
            Track 3 — Top-View Spatio-Temporal Action Localization (TV-STAL, hosted on Kaggle).
          </li>
          <li><b>Registration &amp; training data release:</b> December 12, 2025</li>
          <li><b>Results submission deadline:</b> January 16, 2026</li>
          <li><b>Winner notification:</b> January 30, 2026</li>
        </ul>
        <div>Challenge details: <a href="https://grocery-vision.github.io">Grocery Vision Challenge (GV26)</a></div>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Organizers</div>
      <div>
        Rocco Pietrini (Mercatorum University) • Bruno Abbate • David Woollard • Davide Mazzini (Standard AI) •
        Quanfu Fan • Sean Ma • Shun Miao • Weijian Li (Amazon Worldwide Grocery Stores)
      </div>
    </div>

  </div>
</div>


<!-- Vision and Depiction 2026 -->
<div class="card md-6" id="vision-depiction-2026">
  <div class="title"><b>Vision and Depiction 2026 — Delft</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>February 4–6, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Delft, Netherlands</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div>
        <a href="https://visionanddepiction.github.io/">visionanddepiction.github.io</a> &nbsp;|&nbsp;
        <a href="https://visionanddepiction.github.io/program2024">Archive (2024)</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Call for abstracts open</b>; submission deadline: <b>CLOSED</b></li>
          <li>Scope: formal elements bridging science &amp; art (texture, colour, light, shape, space, material, motion, etc.)</li>
          <li>Organiser: Maarten Wijntjes (TU Delft)</li>
          <li>Contact if unsure about fit; selection, timeline, and registration on the website</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<div class="card md-6">
  <div class="title"><b>Electronic Imaging Symposium (EI 2026)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual (multi-conference symposium)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>March 1–5, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Hyatt Regency San Francisco Airport, Burlingame, CA</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://imaging.org/IST/IST/Conferences/EI/EI2026/Program.aspx">imaging.org – EI 2026</a></div></div>
    <div class="meta-row"><div class="meta-label">Notes</div><div>
      <ul class="notes-list">
        <li><b>Abstract submission deadline: October 15, 2025</b> (extended)</li>
        <li>Includes 17 technical conferences (e.g., HVEI)</li>
      </ul>
    </div></div>
  </div>
</div>


<!-- COSYNE Main Conference -->
<div class="card md-6" id="cosyne">
  <div class="title"><b>COSYNE — Computational &amp; Systems Neuroscience</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>Main: March 12–15, 2026 • Workshops: March 16–17, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Lisbon Congress Centre (Lisbon) &amp; Hotel Cascais Miragem (Cascais), Portugal</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.cosyne.org/">cosyne.org</a></div></div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Abstract submission deadline: October 16, 2025 (11:59 PM PST)</li>
          <li>Author notifications: December 2025</li>
          <li>Workshop proposals due: October 23, 2025 (11:59 PM PST)</li>
          <li>Undergraduate travel grant deadline: November 12, 2025 (11:59 PM PST)</li>
          <li>Travel grants: at least &euro;500 (larger possible); priority for first-time attendees, under-represented groups, mentor–student pairs, undergrads, and authors of submitted abstracts</li>
          <li>Student volunteer applications due: December 31, 2025 (free registration; limited slots)</li>
          <li>Contact: <a href="mailto:meeting@cosyne.org">meeting@cosyne.org</a></li>
          <li>Associated event: <a href="#cosyne-workshops">COSYNE Workshops</a>, March 16–17, 2026 (Hotel Cascais Miragem)</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- COSYNE Workshops -->
<div class="card md-6" id="cosyne-workshops">
  <div class="title"><b>COSYNE Workshops</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual (part of <a href="#cosyne">COSYNE 2026</a>)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>March 16–17, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Hotel Cascais Miragem, Cascais, Portugal</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.cosyne.org/">cosyne.org</a></div></div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Call for workshop proposals open</li>
          <li>Submission deadline: October 23, 2025 (11:59 PM PST)</li>
          <li>Notifications: late November–early December 2025</li>
          <li>One free registration per organizer (others pay)</li>
          <li>Related main meeting: <a href="#cosyne">COSYNE 2026</a> (Lisbon Congress Centre), March 12–15, 2026</li>
        </ul>
      </div>
    </div>
  </div>
</div>


    <div class="card md-6">
      <div class="title"><b>German Conference on Medical Image Computing (BVM)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>March 15–17, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Lübeck, Germany</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.bvm-conf.org/">https://www.bvm-conf.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Long papers &amp; abstracts: October 24, 2025</li>
              <li>Author notifications: November 28, 2025</li>
              <li>Camera-ready: January 4, 2026</li>
              <li>BVM Award applications: January 31, 2026</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>European Sensory Science Society (E3S)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>March 26–27, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Zurich, Switzerland</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://e3sensory.eu/events/">https://e3sensory.eu/events/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Deadline: March 13, 2026</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="aps">
      <div class="title"><b>Association for Psychological Science (APS)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>April 17–19, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Barcelona, Spain</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.psychologicalscience.org/conventions/annual">https://www.psychologicalscience.org/conventions/annual</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Symposiums due: November 15, 2025</li>
              <li>Poster &amp; paper abstracts due: December 5, 2025</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

<div class="card md-6" id="modvis">
      <div class="title"><b>Models in Vision Science (MODVIS) Conference</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>May 2026 – TBD</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>St. Pete Beach, Florida</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.purdue.edu/conferences/events/modvis/">https://www.purdue.edu/conferences/events/modvis/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD 2026</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="arvo">
      <div class="title"><b>Association for Research in Vision and Ophthalmology (ARVO)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>May 3–7, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Colorado Convention Center, Denver, Colorado</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.arvo.org/annual-meeting/">https://www.arvo.org/annual-meeting/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Abstract deadline: December 5, 2025</li></ul></div></div>
      </div>
    </div>

<div class="card md-6">
  <div class="title"><b>SAND Challenge @ IEEE ICASSP 2026</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>May 4–8, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Barcelona, Spain (with ICASSP 2026)</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://sand.icar.cnr.it" aria-label="SAND Challenge website">sand.icar.cnr.it</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Registration &amp; dataset release:</b> September 1, 2025</li>
          <li><b>Submission closes:</b> November 20, 2025</li>
          <li><b>Results:</b> December 2, 2025</li>
          <li>Test set available; submissions open</li>
          <li>Top teams present at ICASSP 2026 (Barcelona); selected works in proceedings</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<div class="card md-6">
  <div class="title"><b>IECMA 2026 – 3rd International Electronic Conference on Machines &amp; Applications</b></div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Biennial</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>May 12–14, 2026</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Online</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://sciforum.net/event/IECMA2026" aria-label="IECMA 2026 event page">sciforum.net – IECMA 2026</a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Abstract submission deadline: January 9, 2026</li>
          <li>Free registration deadline: May 6, 2026</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- CONFERENCE UPDATE: VSS 2026 — Vision Sciences Society -->
<div class="card md-6" id="vss" data-tags="conference 2026 vision-science vss symposia florida virtual hybrid">
  <div class="title"><b>VSS 2026 — Vision Sciences Society Annual Meeting</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>May 15 – 19, 2026</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>TradeWinds Island Resorts, St. Pete Beach, Florida · and Online</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Symposia</div>
      <div>
        November 20, 2025 – Symposium Submission Deadline <b>(closed)</b>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Abstracts</div>
      <div>
        December 4, 2025 – Abstract Submission Deadline<br>
        February 9, 2026 – Abstract Decision Notifications
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Other Key Dates</div>
      <div>
        November 13, 2025 – T-shirt Design Competition Opens<br>
        November 20, 2025 – Satellite &amp; Social Event Applications Open<br>
        November 24, 2025 – VSS Award Nominations Open<br>
        December 16, 2025 – Registration Opens<br>
        January 9, 2026 – Satellite &amp; Social Event Applications Deadline<br>
        January 15, 2026 – T-shirt Design Submission Deadline<br>
        February 1, 2026 – Winning T-shirt Design Announced
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">New Features</div>
      <div>
        <ul class="notes-list">
          <li>Expanded online symposium series for members throughout the year, recorded and published in the <i>Journal of Vision</i>.</li>
          <li>Limited remote presentation options (talks and posters) for those unable to attend in person.</li>
        </ul>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">More Info</div>
      <div><a href="https://www.visionsciences.org/">visionsciences.org</a></div>
    </div>

  </div>
</div>


<!-- Conference: Applied Vision Science (AVS) Meeting -->
<div class="card md-6" data-tags="conference 2026 applied-vision vision-sciences avs vss satellite">
  <div class="title"><b>Applied Vision Science (AVS) Meeting</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Annual (inaugural year in 2026)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Date</div>
      <div>May 20, 2026 (Wednesday following VSS 2026)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>TradeWinds Island Grand, St. Pete Beach, Florida, USA</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Organizers</div>
      <div>Andrew Watson &amp; Laurie Wilcox (in collaboration with the Vision Sciences Society)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.visionsciences.org/">visionsciences.org</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Fees</div>
      <div>Student $50 • Postdoc $50 • Faculty $100 • Industry $200</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Important Dates</div>
      <div>
        Nov 13, 2025 – T-shirt Design Competition Opens<br>
        Nov 14, 2025 – Symposia Submission Deadline <b>(closed)</b><br>
        Nov 20, 2025 – Satellite Event Applications Open<br>
        Dec 4, 2025 – Abstract Submission Deadline<br>
        May 15–19, 2026 – VSS 2026 (same venue; AVS follows on May 20)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Inaugural one-day meeting highlighting high-quality applied vision research.</li>
          <li>Focus on the science underlying practical applications, not just the applications themselves.</li>
          <li>Example topics: display &amp; optical design, AR/VR/MR, image &amp; video coding/compression,
              computational photography, stereoscopic capture and display, color &amp; HDR imaging, user interface design, computer graphics.</li>
          <li>Includes lecture and poster presentations; registration and abstract submission handled through VSS.</li>
          <li>Corporate sponsorship opportunities available.</li>
        </ul>
      </div>
    </div>

  </div>
</div>



<!-- Conference: IASAS 2026 — Synesthesia: Interdisciplinary Research in a Multisensory World -->
<div class="card md-6" data-tags="conference synesthesia multisensory iasas perception cognition art psychology music design">
  <div class="title"><b>IASAS 2026 — Synesthesia: Interdisciplinary Research in a Multisensory World</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial Symposium</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>May 27–29, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>University of Amsterdam &amp; The Orgelpark, Amsterdam, NL</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="mailto:2026amsterdam@gmail.com">2026amsterdam@gmail.com</a></div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>December 1, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Notification</div><div>January 10, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Keynote</div><div>Professor Anina Rich (Macquarie University, Australia)</div></div>
    <div class="meta-row"><div class="meta-label">Chairs</div>
      <div>
        Tessa van Leeuwen (Tilburg University) &amp; Romke Rouw (University of Amsterdam)
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Topics</div>
      <div>
        Synaesthesia and multisensory integration · Synaesthetic art across media · Lived experiences of synaesthetes
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Hosted by the International Association of Synaesthetes, Artists, and Scientists (IASAS).</li>
          <li>Interdisciplinary symposium welcoming both research papers and creative works exploring synaesthetic experience.</li>
          <li>Abstracts ≤500 words; accepted formats: PDF, DOC, or RTF.</li>
          <li>Include name, affiliation, and contact details in submission.</li>
        </ul>
      </div>
    </div>
  </div>
</div>



<div class="card md-6">
  <div class="title"><b>CRV 2026 — 23rd Conference on Robots and Vision</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>May 25–28, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Vancouver, British Columbia, Canada</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.computerrobotvision.org/" rel="noopener">computerrobotvision.org</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Submission</div>
      <div>
        <a href="https://openreview.net/group?id=computerrobotvision.org/CRV/2026/Conference" rel="noopener">
          OpenReview — CRV 2026
        </a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Important dates</div>
      <div>
        <ul class="notes-list">
          <li><b>Paper submission:</b> Feb 3, 2026</li>
          <li><b>Notifications:</b> Mar 27, 2026</li>
          <li><b>Camera-ready:</b> Apr 24, 2026</li>
        </ul>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Full, in-person conference with keynotes, oral presentations, posters.</li>
          <li>IEEE format; standalone 4–8 page submissions (references excluded).</li>
          <li>Original work only; no duplicate submissions.</li>
        </ul>
      </div>
    </div>
  </div>
</div>


    <div class="card md-6">
  <div class="title"><b>IEEE FG 2026 – International Conference on Automatic Face and Gesture Recognition</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>May 25–29, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Kyoto, Japan</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://fg2026.ieee-biometrics.org/" aria-label="IEEE FG 2026 conference page">fg2026.ieee-biometrics.org</a><br/>
        <a href="https://cmt3.research.microsoft.com/FG2026/" aria-label="IEEE FG 2026 submission portal">Submission portal</a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li><b>Round 1:</b> Abstract Sept 25, 2025 • Full Paper Oct 2, 2025 • Notification Dec 11, 2025</li>
          <li><b>Round 2:</b> Abstract Jan 9, 2026 • Full Paper Jan 15, 2026 • Notification Apr 2, 2026 • Camera-ready Apr 21, 2026</li>
          <li><b>Workshops:</b> Proposal deadline Nov 13, 2025</li>
          <li>Calls also open for tutorials and demos</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Conference — Athens Institute (ATINER) Psychology 2026 -->
<div class="card md-6">
  <div class="title"><b>20th Annual International Conference on Psychology</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>May 25–29, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Athens, Greece</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 21, 2025</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://www.atiner.gr/psychology" rel="noopener">atiner.gr/psychology</a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Organizer: Athens Institute for Education and Research (ATINER)</li>
          <li>Academic leads: Dr. Jim Clark (University of Winnipeg), Dr. Thanos Patelis (University of Kansas)</li>
          <li>Call for papers and participation details available on the ATINER site.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="card md-6" id="etra-2026" data-tags="conference acm eye-tracking research applications hci cg it data-analysis gaze interaction perception cognition marrakech">
  <div class="title"><b>ACM Symposium on Eye Tracking Research &amp; Applications (ETRA 2026)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>June 1–4, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Marrakech, Morocco</div></div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>Premier international meeting bridging eye tracking research and applied domains across computer science, psychology, perception, and human–computer interaction. Hosted by ACM SIGCHI and SIGGRAPH.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Key Dates (AoE)</div>
      <div>
        <ul class="notes-list">
  <li><b>Abstract submission (mandatory):</b> <span class="status closed">CLOSED</span> — Nov 3, 2025 (AoE)</li>
  <li><b>Full paper submission:</b> November 10, 2025 (AoE)</li>
  <li><b>1st review notifications:</b> January 8, 2026</li>
  <li><b>Revisions due:</b> February 16, 2026</li>
  <li><b>2nd review notifications:</b> March 13, 2026</li>
  <li><b>Camera-ready:</b> March 30, 2026</li>
</ul>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Ethics Requirement</div>
      <div>Each submission must include a brief privacy and ethics statement (2–3 sentences) addressing potential societal risks, fairness, or broader impacts of the work.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Publication</div>
      <div>Accepted papers appear in <i>PACM on CGIT</i> or <i>PACM on HCI</i> and are indexed in the ACM Digital Library.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Topics</div>
      <div>Eye-tracking systems, gaze prediction, visualization, perception and cognition, gaze-based interaction, and applied methods in real-world contexts.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://etra.acm.org/2026/">etra.acm.org/2026</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div>
      <div><a href="mailto:etra.conference@gmail.com">etra.conference@gmail.com</a> • Nour Aburaed (Publicity Chair)</div>
    </div>
  </div>
</div>


    <div class="card md-6">
      <div class="title"><b>AREADNE 2026 (Research in Encoding and Decoding of Neural Ensembles)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>June 23–27, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Eliopoulos Conference Center, Milos, Greece</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://areadne.org">https://areadne.org</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>


<div class="card md-6" id="apcv">
  <div class="title"><b>EPC–APCV 2026 — Joint Meeting of the Australasian Society for Experimental Psychology &amp; Asia Pacific Conference on Vision</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Joint Meeting (Biennial)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>July 1–4, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>University of Auckland, New&nbsp;Zealand</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://visualneuroscience.auckland.ac.nz/epc-apcv-2026/">visualneuroscience.auckland.ac.nz/epc-apcv-2026</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Hosted jointly by the <b>Australasian Society for Experimental Psychology (EPC)</b> and the <b>Asia Pacific Conference on Vision (APCV)</b>.</li>
          <li><b>Member-initiated symposia:</b> 90-minute sessions (3–4 speakers + discussant); submission deadline <b>January 31, 2026</b>.</li>
          <li><b>General abstract submissions:</b> open January 2026, accepted on a rolling basis until April 2026.</li>
          <li>Topics welcome from all areas of experimental psychology and vision science.</li>
          <li>Contact: <a href="mailto:d.arnold@psy.uq.edu.au">d.arnold@psy.uq.edu.au</a></li>
          <li>Further details on keynotes and satellite workshops to follow.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Conference: Eurohaptics 2026 — Siena, Italy -->
<div class="card md-6"
  data-tags="conference 2026 haptics touch perception multisensory vr ar robotics interfaces biomechanics eurohaptics italy">

  <div class="title"><b>Eurohaptics 2026 — Siena, Italy</b></div>

  <div class="meta">

    <div class="meta-row"><div class="meta-label">Dates</div><div>July 6 – 9, 2026</div></div>

    <div class="meta-row"><div class="meta-label">Location</div><div>Siena, Italy</div></div>

    <div class="meta-row"><div class="meta-label">Themes</div>
      <div>
        Haptic perception · neuroscience of touch · tactile interfaces · sensors &amp; actuators · 
        teleoperation · VR/AR haptics · human-computer interaction · robotics · health &amp; wellness applications.
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Contribution Types</div>
      <div>
        Conference papers · Workshops · Published Research Track · Work-in-Progress · Hands-on Demonstrations.
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Deadlines</div>
      <div>
        Jan 19, 2026 – Conference Papers<br>
        Mar 12, 2026 – Workshop Proposals<br>
        Mar 15, 2026 – Published Research Track<br>
        Apr 12, 2026 – Work-in-Progress<br>
        Apr 16, 2026 – Demonstrations
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://2026.eurohaptics.org/">2026.eurohaptics.org</a></div>
    </div>

  </div>
</div>




    <div class="card md-6" id="apa">
      <div class="title"><b>American Psychological Association (APA)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>August 6–8, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Washington, DC</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://convention.apa.org/attend/future-conventions">Future conventions</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="icvs">
      <div class="title"><b>International Colour Vision Society (ICVS) Conference</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>August 14-18, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Brighton, UK</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.icvs2026.org/" rel="noopener">icvs2026.org (local 2026 site)</a><br>
        <a href="https://icvs.info/meetings" rel="noopener">icvs.info/meetings (main ICVS page)</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="spatial-cognition-2026" data-tags="conference spatial-cognition psychology neuroscience cs linguistics navigation">
  <div class="title"><b>Spatial Cognition 2026 — University of Glasgow</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>August 25–28, 2026</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>University of Glasgow, United Kingdom</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.gla.ac.uk/research/az/spatialreasoning/sc26/">Conference page</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        Hosted by the Turner Kirk Centre for Spatial Reasoning.  
        Focus on the acquisition, development, representation, and use of spatial knowledge in real, virtual, and hybrid environments by humans and artificial agents.  
        Interdisciplinary scope across cognitive & developmental psychology, computer science, linguistics, geography, cartography, philosophy, neuroscience, education, and robotics.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Program</div>
      <div>
        Single-track, refereed conference featuring invited talks, oral presentations, poster sessions, workshops, and symposia.  
        In-person only.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div><a href="mailto:jack.parkinson@glasgow.ac.uk">jack.parkinson@glasgow.ac.uk</a></div>
    </div>

  </div>
</div>


    <div class="card md-6">
      <div class="title"><b>European Working Memory Symposium (EWOMS)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>August 26–28, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Lyon, France</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.escop.eu/news/news/conference-news/ewoms-european-working-memory-symposium">ESCOP • EWOMS</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Abstract deadline: March 13, 2026</li>
              <li>Registration deadline: June 20, 2026</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

<!-- CONFERENCE: ECEM 2026 — European Conference on Eye Movements -->
<div class="card md-6" id="ecem2026" data-tags="conference europe eye-movements vision cognition neuroscience ulm germany 2026">
  <div class="title"><b>23rd European Conference on Eye Movements (ECEM 2026)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>August 30 – September 3 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Ulm University, Germany</div></div>
    <div class="meta-row"><div class="meta-label">Symposia Deadline</div><div>January 31 2026</div></div>
    <div class="meta-row"><div class="meta-label">Abstract / Poster Deadline</div><div>March 31 2026</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.uni-ulm.de/in/ecem2026/">uni-ulm.de/in/ecem2026</a> • <a href="https://www.conftool.com/ecem2026/">Submit via Conftool</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>World’s largest meeting devoted to eye-movement research, uniting neuroscience, psychology, linguistics, computer science, and applied domains such as XR, education, and clinical vision science.</div>
    </div>
  </div>
</div>


<!-- CONFERENCE: Progress in Colour Studies (PICS 2026) -->
<div class="card md-6" id="pics2026" data-tags="conference colour-studies interdisciplinary linguistics humanities poland 2026">
  <div class="title"><b>Progress in Colour Studies (PICS 2026)</b></div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Frequency</div>
      <div>Recurring conference in the Progress in Colour Studies series</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>September 10–12, 2026</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Wrocław, Poland</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://phc.uni.wroc.pl/PICS2026/">phc.uni.wroc.pl/PICS2026</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Key Dates</div>
      <div>
        <ul class="notes-list">
          <li><b>Abstract submission:</b> February 28, 2026</li>
          <li><b>Notification of acceptance:</b> April 15, 2026</li>
          <li><b>Registration &amp; payment:</b> April 30 – May 31, 2026</li>
          <li><b>Conference:</b> September 10–12, 2026</li>
          <li><b>Paper submission for publication:</b> February 28, 2027</li>
        </ul>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Chairs</div>
      <div>Adam Pawłowski &amp; Danuta Stanulewicz (University of Wrocław / University of Gdańsk)</div>
    </div>
  </div>
</div>



    <div class="card md-6">
      <div class="title"><b>Asian Conference on Computer Vision (ACCV)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>December 8–12, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://accv2022.org/">https://accv2022.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

    <!-- 2026 TBD / Unknown Dates -->
    
<div class="card md-6">
  <div class="title"><b>Trieste “Spring Congressino”</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual (Spring)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>Spring 2026 — TBD</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Trieste, Italy</div></div>
   <div class="meta-row">
  <div class="meta-label">Website</div>
  <div><a href="https://portale.units.it/en/events/calendar">https://portale.units.it/en/events/calendar</a></div>
</div>
    <div class="meta-row"><div class="meta-label">Notes</div><div>
      <ul class="notes-list">
      </ul>
    </div></div>
  </div>
</div>

<div class="card md-6" id="optica-fall-vision">
      <div class="title"><b>Optica Fall Vision Meeting</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div> TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.osafallvisionmeeting.org/">https://www.osafallvisionmeeting.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Submissions TBD</li></ul></div></div>
      </div>
    </div>
    
<div class="card md-6">
      <div class="title"><b>ACM Symposium on Applied Perception (SAP)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://sap.acm.org/">https://sap.acm.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="ava">
      <div class="title"><b>Applied Vision Association (AVA)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Various</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Brisbane, Australia</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.theava.net/meetings.php">https://www.theava.net/meetings.php</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>International Multisensory Research Forum (IMRF)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://imrf2025.sciencesconf.org/">https://imrf2025.sciencesconf.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="sfn">
  <div class="title"><b>Society for Neuroscience (SfN)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>

    <!-- Last known meeting info -->
    <div class="meta-row"><div class="meta-label">Next Meeting</div>
      <div>TBD 2026 (details pending from SfN)</div>
    </div>

    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.sfn.org/meetings">https://www.sfn.org/meetings</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Official 2026 dates and location have not yet been announced.</li>

          <li>
            <b>Reception (2025):</b> Academy of Neuroscience for Architecture — 
            Monday <b>November 17, 2025</b>, 7–9 PM, Marriott Marquis San Diego Marina 
            (Rancho Santa Fe Rooms 1–3, North Tower), 
            <a href="https://anfarch.org">anfarch.org</a>. Food & drinks provided.
          </li>

          <li>Hosted by SfN during the 53rd Annual Meeting (2025).</li>
        </ul>
      </div>
    </div>
  </div>
</div>


    <div class="card md-6">
      <div class="title"><b>16th Pangborn Sensory Science Symposium</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.pangbornsymposium.com/">https://www.pangbornsymposium.com/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>


<div class="card md-6" id="vsac">
      <div class="title"><b>Visual Science Art Conference (VSAC)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://2025.vsac.eu/">https://2025.vsac.eu/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="ecvp">
      <div class="title"><b>European Conference on Visual Perception (ECVP)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>August 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Bournemouth, UK</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="http://ecvp.org/future.html">http://ecvp.org/future.html</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>European Society for Cognitive Psychology (ESCOP)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.escop2025.com/">https://www.escop2025.com/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>EuroGraphics 3D Object Retrieval Workshop (3DOR)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><em>TBD</em></div></div>
<div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Official site/link pending</li></ul></div></div>
      </div>
    </div>

<div class="card md-6" id="gruppo-del-colore">
    <div class="title"><b>Gruppo del Colore — Associazione Italiana Colore (Annual Meeting)</b></div>
<div class="meta">
  <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
  <div class="meta-row"><div class="meta-label">Dates</div><div>September 3–4, 2026</div></div>
  <div class="meta-row"><div class="meta-label">Location</div><div>Florence, Italy</div></div>
  <div class="meta-row"><div class="meta-label">Website</div>
    <div><a href="https://www.aic2026.org/welcome-message/">aic2026.org/welcome-message</a></div>
  </div>
  <div class="meta-row"><div class="meta-label">Deadline</div><div>Abstracts due December 15, 2025</div></div>
  <div class="meta-row"><div class="meta-label">Notes</div>
    <div>
      <ul class="notes-list">
        <li>Co-located with AIC 2026; joint Florence dates and venue.</li>
      </ul>
    </div>
  </div>
</div>
    </div>

<!-- Conference: AIC 2026 — International Colour Association -->
<div class="card md-6" id="aic2026" data-tags="conference color aic florence italy 2026">
  <div class="title"><b>AIC 2026 — International Colour Association</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>September 3–4, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Florence, Italy</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.aic2026.org/welcome-message/">aic2026.org/welcome-message</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Abstracts due December 15, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Co-located with Gruppo del Colore – Associazione Italiana Colore.</li>
          <li>Applied &amp; basic colour/vision topics; strong overlap with the CVNet community.</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<div class="card md-6" id="bavrd">
      <div class="title"><b>Bay Area Vision Research Day (BAVRD)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>2026 TBD</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>International House, UC Berkeley, Berkeley, CA</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://vision.berkeley.edu/events/bavrd">https://vision.berkeley.edu/events/bavrd</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list"><li>TBD</li></ul>
          </div>
        </div>
      </div>
    </div>

   <div class="card md-6">
      <div class="title"><b>7th Instance-Level Recognition and Generation Workshop (ILR) @ ICCV</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial (with ICCV)</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2027</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://ilr-workshop.github.io/ICCVW2025/">https://ilr-workshop.github.io/ICCVW2025/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

<!-- Conference: ACM SUI 2025 → roll forward -->
<div class="card md-6" data-tags="conference hci spatial interaction acm sui montreal">
  <div class="title"><b>ACM Symposium on Spatial User Interaction (SUI)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://sui.acm.org/2025/overview/">Program Overview</a> •
        <a href="https://sui.acm.org/2025/keynotes/">Keynotes</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Keynotes</div><div>TBD</div></div>
    <div class="meta-row"><div class="meta-label">Notes</div><div>TBD</div></div>
  </div>
</div>


    <div class="card md-6">
  <div class="title"><b>Fechner Day (International Society for Psychophysics)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://fechner.fonae.org/">fechner.fonae.org</a></div></div>
    <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
  </div>
</div>


    <div class="card md-6" id="aps">
  <div class="title"><b>Association for Psychological Science (APS)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.psychologicalscience.org/conventions/annual">https://www.psychologicalscience.org/conventions/annual</a></div></div>
    <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
  </div>
</div>

    <div class="card md-6">
      <div class="title"><b>Brain Conference: Principles of the Adaptive Mind</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Periodic</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.fens.org/news-activities/fens-and-societies-calendar/meeting-event/principles-of-the-adaptive-mind">FENS event page</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Submissions TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>Color and Imaging Conference (CIC 33)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.imaging.org/IST/IST/Conferences/CIC/CIC2025/CIC_Home.aspx">imaging.org – CIC 2025</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Submissions TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
  <div class="title"><b>SCAIR 2025 — Southern California AI &amp; Robotics Symposium</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="http://www.scair2025.org/">scair2025.org</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>TBD</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="card md-6">
  <div class="title"><b>Kanizsa Lecture / Trieste Symposium — 33rd Kanizsa Lecture</b></div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>TBD 2026</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>University of Trieste, Trieste, Italy</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="mailto:trieste.symposium@gmail.com">trieste.symposium@gmail.com</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>TBD</li>
        </ul>
      </div>
    </div>
  </div>
</div>

  <div class="card md-6">
      <div class="title"><b>Asian Conference on Pattern Recognition (ACPR)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2027</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.mva-org.jp/acpr2025/">https://www.mva-org.jp/acpr2025/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>International Symposium on Visual Computing (ISVC)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.isvc.net/">https://www.isvc.net/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              TBD
            </ul>
          </div>
        </div>
      </div>
    </div>

<div class="card md-6">
  <div class="title"><b>OPAM 2025 — 33rd Annual Workshop on Object Perception, Attention, and Memory</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual Workshop (satellite to Psychonomics)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.opamconference.com/">opamconference.com</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          TBD</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<div class="card md-6" id="psychonomics">
      <div class="title"><b>Psychonomic Society Annual Meeting</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual</div></div>
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.psychonomic.org/">https://www.psychonomic.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

  </div><!-- /cards (Conferences) -->

 <!-- Online Seminars/Clubs -->
<h3 id="talks">Online Seminars/Clubs</h3>
<p class="section-desc">
  Recurring journal clubs and seminar series open to the community, plus notable one-off talks.
</p>

<div class="cards">

<!-- Online Seminar — BIU Vision Science Seminar: Prof. MiYoung Kwon -->
<div class="card md-6"
  data-tags="online-seminar vision-science talk oculomotor retina adaptation perception israel biu zoom november-2025">
  <div class="title">
    <b>BIU Vision Science Online Seminar — Prof. MiYoung Kwon</b>
  </div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Speaker</div>
      <div>Prof. MiYoung Kwon (Northeastern University)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Title</div>
      <div>
        “From retinal encoding to oculomotor adaptation: How the human visual system adapts to degraded sensory information”
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Date</div>
      <div>Tuesday, November 18, 2025</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Time (global)</div>
      <div>
        3pm Israel • 2pm Central Europe • 1pm UK • 8am Eastern (US/Canada) • 22:00 Tokyo • 23:59 Melbourne/Sydney
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>
        Online (Zoom):
        <a href="https://biu-ac-il.zoom.us/j/82928146713?pwd=Wa7RxsDNR1vErbuXaMYOZi100kp2V5.1" target="_blank" rel="noopener">
          Join via Zoom
        </a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Series</div>
      <div>
        BIU Vision Science Seminar (Bar-Ilan University) •
        <a href="https://www.worldwideneuro.com/seminar-series.html?name=BIU_Vision_Science" target="_blank" rel="noopener">
          Series page
        </a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Recordings</div>
      <div>
        <a href="https://www.youtube.com/channel/UCnZQ6jqzktLbeoEJgufKg6Q/videos" target="_blank" rel="noopener">
          BIU Vision Science YouTube channel
        </a>
      </div>
    </div>

  </div>
</div>



<!-- Online seminar series — TSCN: Current Topics in Sleep & Circadian Health -->
<div class="card md-6" data-tags="online seminars circadian sleep light exposure logging">
  <div class="title"><b>Current Topics in Sleep &amp; Circadian Health — TSCN (Online Seminar Series)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div>
      <div>October&nbsp;2025&nbsp;–&nbsp;January&nbsp;2026</div>
    </div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Online (Zoom)</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.tscnlab.org/">Translational Sensory &amp; Circadian Neuroscience Unit (TSCN)</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Themes</div>
      <div>
        Innovation in light &amp; visual exposure logging • Emerging sleep technology &amp; digital health •
        Clocks beyond 24&nbsp;hours • Sleep &amp; women’s health
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        Free and open to researchers, students, and professionals. Talks by international experts hosted by the TSCN Unit (MPS/TUM/TUMCREATE).
      </div>
    </div>
  </div>
</div>


<!-- Webinar — JIVP: Trends in Image Aesthetics Assessment -->
<div class="card md-6">
  <div class="title"><b>JIVP Webinar — Trends in Image Aesthetics Assessment</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Speaker</div><div>Prof. Leida Li (Xidian University)</div></div>
    <div class="meta-row"><div class="meta-label">Date</div><div>Thursday, October 9, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Time</div><div>12:00–13:00 UTC • 14:00–15:00 CEST • 05:00–06:00 PDT (Los Angeles)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Online (Cassyni)</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://cassyni.com/events/X5JtSQaWByayBdTbPQZEfX" rel="noopener">RSVP on Cassyni</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Host</div><div>EURASIP Journal on Image and Video Processing — JIVP Webinar Series</div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Topics: image aesthetics assessment (IAA), personalized aesthetics, image cropping.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Webinar — Optica Vision TG: Myopia -->
<div class="card md-6">
  <div class="title"><b>Optica Vision Technical Group Webinar — The Global Myopia Crisis (NASEM 2024)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Date &amp; Time</div><div>Tuesday, October 14, 2025 • 12:00–13:00 EDT (UTC−04:00)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Online (Optica Vision Technical Group)</div></div>
    <div class="meta-row"><div class="meta-label">Speaker</div><div>Machelle T. Pardue (Emory University; Atlanta VA)</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.optica.org/events/webinar/2025/10_october/the_global_myopia_crisis_insights_from_the_2024_nasem_report/">Event page</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Key findings from the 2024 National Academies (NASEM) consensus report on myopia.</li>
          <li>Organized by Optica’s Vision Technical Group; session hosted with Dr. Pardue.</li>
        </ul>
      </div>
    </div>
  </div>
</div>



  <div class="card md-6">
    <div class="title"><b>Intermural Colour Club</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>Ongoing in 2025</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Online</div></div>
      <div class="meta-row"><div class="meta-label">Website</div>
        <div><a href="https://docs.google.com/document/d/1LH_FEBCwvE0z8OuWMLs7e_OVKg1VB2vwleyXzHHug44/edit?tab=t.0">Journal Club Schedule 2025</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>For information or to join, email:</li>
            <li>Christoph Witzel — <a href="mailto:witzel@daad-alumni.de">witzel@daad-alumni.de</a></li>
            <li>Ilgin Cebioglu — <a href="mailto:I.Cebioglu@newcastle.ac.uk">I.Cebioglu@newcastle.ac.uk</a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>


  <div class="card md-6">
  <div class="title"><b>Optica Color Technical Group Webinar Series</b></div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>Ongoing</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Online (hosted by Optica)</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://opg.optica.org/ao/colorgroup.cfm">Optica Color Technical Group</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Recurring webinar series on color science, vision, and display technology</li>
          <li>Recent talk: <i>Color Discrimination and Chromatic Adaptation</i> (Prof. Yuhao Zhu, University of Rochester)</li>
          <li>Topics include psychophysics, XR displays, perception, and color imaging</li>
        </ul>
      </div>
    </div>
  </div>
</div>



  <div class="card md-6">
    <div class="title"><b>OPAM Online Workshop — Preregistration &amp; Registered Reports</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>October 8, 2025 • 10:00 AM ET</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Online (recording available after)</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div>
          <a href="https://www.opamconference.com/online-workshops">Workshop page</a> •
          <a href="https://docs.google.com/forms/d/e/1FAIpQLSe09feybP_GYvo2NrTOzYrTcFxaQnt25pn-b6DnUls9PHccIg/viewform?usp=header">Sign-up</a>
        </div>
      </div>
      <div class="meta-row"><div class="meta-label">Speaker</div><div>Dr. Caro Hautikiet (VU Amsterdam)</div></div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>How preregistration &amp; registered reports improve credibility and reproducibility</li>
            <li>Part of the OPAM33 program (main event Nov 20, Denver)</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

</div>


 <!-- Publications -->
<h3 id="calls">Special/Feature Issues</h3>

<p class="section-desc">
  <a href="./journals%20list.html">Click here</a> for a comprehensive list of relevant journals,
  complete with website information, type of journal, open access status, and more.
</p>

<p class="section-desc">
  For special or feature issues, we try to keep up to date with the following journals:
  Attention, Perception &amp; Performance; Color Research and Application; IOVS; JOSA A/B; JOV;
  Multisensory Perception; Perception; and Vision Research. We also provide permanent links for
  some journals&rsquo; special or feature issues. If you have suggestions for journals or calls we
  should track, or would like to share information about a special issue, please contact us.
</p>


<div class="cards">


<!-- Visual Cognition Special Issue — The Control of Visual Attention -->
<div class="card md-6"
  data-tags="journals special-issue visual-cognition attention control charles-folk roger-remington deadline-2025">

  <div class="title">
    <b>Visual Cognition — Special Issue: The Control of Visual Attention (in honor of Charles Folk &amp; Roger Remington)</b>
  </div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Journal</div>
      <div>Visual Cognition</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Theme</div>
      <div>
        The control of visual attention, inspired by and extending the foundational work of Charles Folk and 
        Roger Remington (e.g., the Contingent Involuntary Orienting Hypothesis).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Article Types</div>
      <div>
        Full Research Articles · Brief Articles · Review Articles
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Submission Deadline</div>
      <div>
        <b>December 1, 2025</b> (extended deadline)
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Guest Editors</div>
      <div>
        Andrew B. Leber · Brad Gibson · Brian Anderson
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Special Issue Page</div>
      <div>
        <a href="https://think.taylorandfrancis.com/special_issues/the-control-of-visual-attention-in-honor-of-charles-folk-roger-remington/">
          Submission &amp; issue details (Taylor &amp; Francis)
        </a>
      </div>
    </div>

  </div>
</div>


<!-- JOV Special Issue — Interface Between Vision and Language -->
<div class="card md-6"
  data-tags="journals special-issue journal-of-vision jov vision language reading semantics dyslexia sign-language eye-movements neuroimaging eeg tms llm">
  <div class="title">
    <b>Journal of Vision — Special Issue on the Interface Between Vision and Language</b>
  </div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Journal</div>
      <div>Journal of Vision (JOV)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Scope</div>
      <div>
        Interactions between visual and language systems during reading, naming, sign language,
        gesture perception, and visual communication; how literacy and language experience shape
        visual cortex and cortical specialization.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Topics</div>
      <div>
        Functional specialization for reading &amp; naming · semantic representations · letter
        recognition &amp; perceptual learning · sign language comprehension · eye movements in
        reading/linguistic tasks · development of visual skills for reading/communication ·
        links between visual cortex activity and large language models · visual processing
        differences in dyslexia.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Editors</div>
      <div>
        Susana Chung · Kalanit Grill-Spector · Garikoitz Lerma-Usabiaga · Hans Op de Beeck ·
        Zeynep Saygin · Alex White · Oscar Woolnough
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Submission Deadline</div>
      <div>December 31, 2025</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Special Issue Page</div>
      <div>
        <a href="https://jov.arvojournals.org/ss/visionlanguageinterface.aspx">
          jov.arvojournals.org/ss/visionlanguageinterface.aspx
        </a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Instructions for Authors</div>
      <div>
        <a href="https://jov.arvojournals.org/SS/PeerReview.aspx">
          Submission &amp; peer review instructions
        </a>
      </div>
    </div>

  </div>
</div>

  
<!-- Colour Turn (Journal + Colloquium) -->
<div class="card md-6">
  <div class="title"><b>Colour Turn</b> — University of&nbsp;Tübingen</div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://colourturn.net/" target="_blank" rel="noopener">colourturn.net</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Type</div>
      <div>Interdisciplinary colour studies journal (Humanities · Sciences · Technology · Arts)</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Access</div>
      <div>Open&nbsp;Access · ISSN&nbsp;3052-8534</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Languages</div>
      <div>English / German</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Description</div>
      <div>
        Peer-reviewed journal published electronically via OJS, funded by the Deutsche&nbsp;Forschungsgemeinschaft (DFG).  
        Explores colour across eight thematic areas: context, mind, nature, technology, communication, art & media, culture & society, and reviews.  
        Latest issue: Vol.&nbsp;1&nbsp;No.&nbsp;1 (June 17 2025).
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Related Event</div>
      <div>
        <b>Colour Research Colloquium (CRC)</b> — recurring online symposium organised by <i>Colour Turn</i>.<br>
        <ul class="notes-list">
          <li>Next sessions: Feb 20 2025 · May 15 2025 · Oct 23 2025 (abstracts due Oct 9).</li>
          <li>Short-talk format (~15 min); interdisciplinary and early-career friendly.</li>
          <li>Participation via <a href="mailto:submission@colourturn.net">submission@colourturn.net</a>.</li>
          <li>More info: <a href="https://colourturn.net/colloquium" target="_blank" rel="noopener">colourturn.net/colloquium</a></li>
        </ul>
      </div>
    </div>
  </div>
</div>

  
<!-- Special Issue — Nature Portfolio: Rhythmic Cognition (Cross-Journal Collection) -->
<div class="card md-6">
  <div class="title"><b>Rhythmic Cognition — Cross-Journal Collection</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Journals</div>
      <div>Communications Psychology; Nature Communications; Scientific Reports</div>
    </div>
    <div class="meta-row"><div class="meta-label">Deadline</div>
      <div>CLOSED</div>
    </div>
    <div class="meta-row"><div class="meta-label">Scope</div>
      <div>Rhythmic attention; neural entrainment; oscillatory dynamics in perception, memory &amp; decision-making; computational/theoretical models; brain stimulation; neuroimaging; advanced methods for behavioural &amp; neural rhythms.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Formats</div>
      <div>Research Articles; Registered Reports; Resources (contact editors for Review/Opinion suitability)</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.nature.com/collections/jhcjbjgfgj/about-this-collection" rel="noopener">nature.com — Rhythmic Cognition Collection</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Open across the three participating journals; see collection page for submission routing.</li>
          <li>Guest Editor: David Pascucci, PhD.</li>
        </ul>
      </div>
    </div>
  </div>
</div>



  <!-- BMC Biology -->
  <div class="card md-6">
    <div class="title"><b>BMC Biology</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Location</div><div>Online — BMC</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div><a href="https://www.biomedcentral.com/journal/12915">Journal homepage</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Special Issues &amp; Collections</div>
        <div>
          <ul class="notes-list">
            <li>
              <a href="https://www.biomedcentral.com/collections/itsd"><b>Integrative and Translational Serial Dependence</b></a> — Submission deadline: <b>October 11, 2025</b> — <em>Status: Open</em>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <!-- Visual Cognition -->
  <div class="card md-6">
    <div class="title"><b>Visual Cognition</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Location</div><div>Online — Taylor &amp; Francis</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div><a href="https://www.tandfonline.com/journals/pvis20">Journal homepage</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Special Issues &amp; Collections</div>
        <div>
          <ul class="notes-list">
            <li>
  <a href="https://think.taylorandfrancis.com/special_issues/the-control-of-visual-attention-in-honor-of-charles-folk-roger-remington/">
    The Control of Visual Attention — Special Issue (Visual Cognition)
  </a>
  — Submission deadline: <b>December 1, 2025</b> — <em>Status: Open</em>
</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <!-- Optics Express -->
  <div class="card md-6">
    <div class="title"><b>Optics Express</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Location</div><div>Online — Optica Publishing Group</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div><a href="https://opg.optica.org/oe/feature_issues.cfm">Feature Issues (2007–2026)</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Special Issues &amp; Collections</div>
        <div>
          <ul class="notes-list">
            <li>
              <a href="https://opg.optica.org/content/feature/announcement/item/oe-3d-image-acquisition-and-display-2025"><b>3D Image Acquisition and Display (2025)</b></a> — Submission deadline: <b>November 30, 2025</b> — <em>Status: Open</em>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <!-- Applied Optics / Biomedical Optics Express / Optics Express (joint) -->
  <div class="card md-6">
    <div class="title"><b>Applied Optics / Biomedical Optics Express / Optics Express</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Location</div><div>Online — Optica Publishing Group</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div><a href="https://opg.optica.org/content/feature/announcement/">Feature Issue Announcements</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Special Issues &amp; Collections</div>
        <div>
          <ul class="notes-list">
            <li>
              <a href="https://opg.optica.org/content/feature/announcement/item/oe-cosi2025"><b>Computational Optical Sensing and Imaging (COSI 2025)</b></a> — Submission deadline: <b>December 1, 2025</b> — <em>Status: Open</em>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <!-- Journal of Vision (JOV) -->
  <div class="card md-6">
    <div class="title"><b>Journal of Vision (JOV)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Location</div><div>Online — ARVO</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div><a href="https://jov.arvojournals.org/special-issues">JOV — Special Issues</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Special Issues &amp; Collections</div>
        <div>
          <ul class="notes-list">
            <li>
              <a href="https://jov.arvojournals.org/special-issues"><b>Choose Your Stimuli Wisely: Advances in Stimulus Synthesis and Selection</b></a> — Submission deadline: <b>December 12, 2025</b> — <em>Status: Open</em>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <!-- JOSA A / JOSA B -->
  <div class="card md-6">
    <div class="title"><b>JOSA A / JOSA B</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Location</div><div>Online — Optica Publishing Group</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div>
          <a href="https://opg.optica.org/josaa/feature_issues.cfm">JOSA A — Feature Issues</a> •
          <a href="https://opg.optica.org/josab/feature_issues.cfm">JOSA B — Feature Issues</a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Special Issues &amp; Collections</div>
        <div>
          <ul class="notes-list">
            <li>
              <a href="https://opg.optica.org/content/feature/announcement/item/josaa-spatiotemporal-structured-light"><b>Spatiotemporal Structured Light: Generation, Propagation, Interaction, and Application</b></a> — Submission deadline: <b>December 15, 2025</b> — <em>Status: Open</em>
            </li>
            <li>
              <a href="https://opg.optica.org/content/feature/announcement/item/josaab-optics-in-south-asia"><b>Optics in South Asia</b></a> — Submission deadline: <b>January 15, 2026</b> — <em>Status: Open</em>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <!-- IOVS -->
  <div class="card md-6">
    <div class="title"><b>Investigative Ophthalmology &amp; Visual Science (IOVS)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Location</div><div>Online — ARVO</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div><a href="https://iovs.arvojournals.org/special-issues">IOVS — Special Issues</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Special Issues &amp; Collections</div>
        <div>
          <ul class="notes-list">
            <li>
              <a href="https://iovs.arvojournals.org/special-issues/nystagmus"><b>Nystagmus</b></a> — Submission deadline: <b>CLOSED</b> — <em>Status: Closed</em>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <!-- Color Research & Application -->
  <div class="card md-6">
    <div class="title"><b>Color Research &amp; Application</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Location</div><div>Online — Wiley</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div><a href="https://onlinelibrary.wiley.com/journal/15206378">Journal homepage</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Special Issues &amp; Collections</div>
        <div>
          <ul class="notes-list">
            <li>
              <a href="https://onlinelibrary.wiley.com/doi/toc/10.1002/%28ISSN%291520-6378.facial-appearance"><b>Facial Appearance Measurement and Perception</b></a> — Ongoing collection (rolling publications) — <em>Status: Rolling</em>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <!-- Attention, Perception, & Psychophysics -->
  <div class="card md-6">
    <div class="title"><b>Attention, Perception, &amp; Psychophysics (AP&amp;P)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Location</div><div>Online — Springer / Psychonomic Society</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div><a href="https://link.springer.com/journal/13414/updates">Journal updates (calls &amp; news)</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Special Issues &amp; Collections</div>
        <div>
          <ul class="notes-list">
            <li>See journal Updates page for current calls — <em>Status: Various</em></li>
          </ul>
        </div>
      </div>
    </div>
  </div>

</div> 
  
  <!-- Courses -->
  <h3 id="education">Education</h3>
<p class="section-desc">
  Workshops, classes, and summer schools to build skills in perception, imaging, analysis, and research methods.
</p>

  <h4>Summer Schools</h4>
  <div class="cards">

<!-- Summer School — Annual Nepal AI School (ANAIS 2025) -->
<div class="card md-6">
  <div class="title"><b>Annual Nepal AI School (ANAIS 2025)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>December 29, 2025 – January 8, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Kathmandu, Nepal</div></div>
    <div class="meta-row"><div class="meta-label">Apply by</div><div><b>October 16, 2025</b></div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://anais.naamii.org.np/" rel="noopener">anais.naamii.org.np</a></div></div>
    <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:anais@naamii.org.np">anais@naamii.org.np</a></div></div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>11-day intensive on AI foundations &amp; applications; prior editions featured 65+ speakers from academia &amp; industry.</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<!-- Summer School — Visual Neuroscience: from spikes to awareness -->
<div class="card md-6"
     data-tags="summer-school education training visual-neuroscience computational-neuroscience neuroscience europe germany 2026 rauischholzhausen frankfurt">
  <div class="title">
    <b>Summer School — Visual Neuroscience: from spikes to awareness</b>
  </div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Dates</div>
      <div>August 30 – September 11, 2026</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Rauischholzhausen Castle (near Frankfurt), Germany</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Category</div>
      <div>Summer school / advanced training</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Application deadline</div>
      <div>March 8, 2026</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://www.allpsych.uni-giessen.de/rauisch/" target="_blank" rel="noopener">
          https://www.allpsych.uni-giessen.de/rauisch/
        </a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Tuition</div>
      <div>1500 € (room & all meals)</div>
    </div>
  </div>
  <p class="card-notes">
    European summer school for late PhD and early postdocs, covering methods and key issues in contemporary
    visual neuroscience “from spikes to awareness”. Includes lectures by leading researchers plus afternoon
    computational/theoretical projects and after-dinner discussions.
  </p>
</div>


<!-- Summer School: ESSVN 2026 — European Summer School in Visual Neuroscience -->
<div class="card md-6" id="essvn2026" data-tags="summer-school course visual-neuroscience training 2026">
  <div class="title"><b>European Summer School in Visual Neuroscience (ESSVN 2026)</b></div>
  <div class="meta">

    <div class="meta-row"><div class="meta-label">Dates</div>
      <div>August 30 – September 11, 2026</div>
    </div>

    <div class="meta-row"><div class="meta-label">Location</div>
      <div>TBD (European venue; see program updates)</div>
    </div>

    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.visualneuroscience.eu">visualneuroscience.eu</a></div>
    </div>

    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>
        A selective, advanced training program in visual neuroscience, integrating psychophysics, physiology, imaging, computational methods, and theory. Includes hands-on workshops, lectures, and collaborative projects.
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Eligibility</div>
      <div>Graduate students and early-stage postdocs in vision science, neuroscience, psychology, biology, or computational fields.</div>
    </div>

    <div class="meta-row"><div class="meta-label">Application</div>
      <div>2026 application details forthcoming (per digest announcement).</div>
    </div>

  </div>
</div>


    <div class="card md-6">
      <div class="title"><b>Systems Vision Science Summer School and Symposium</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>August 2026 (dates TBA)</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Tübingen, Germany</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://summerschool.lizhaoping.org/">https://summerschool.lizhaoping.org/</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD 2026</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>Eastern European Machine Learning Summer School</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.eeml.eu/application">https://www.eeml.eu/application</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>MSc in Research Methods in Experimental Psychology</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>2026 TBD</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Universidad Autónoma de Madrid (UAM), Madrid, Spain</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.uam.es/uam/inicio">https://www.uam.es/uam/inicio</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>International Colour Vision Society (ICVS) Summer School</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD 2027</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Pembroke College, Oxford, UK</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.icvs.info/index.php/summer-school">https://www.icvs.info/index.php/summer-school</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li><b>Verriest Medal 2026:</b> Prof. Keiji Uchikawa — to be awarded at ICVS 2026 (University of Sussex, Brighton).</li>
</ul></div></div>
      </div>
    </div>
  </div>

<!-- Workshops -->
<h4>Workshops</h4>
<div class="cards">
 

<!-- Workshop — Why are there neuroscientists? Workshop of Ideas in Neuroscience -->
<div class="card md-6" data-tags="workshops neuroscience philosophy motivation heidelberg openlab">
  <div class="title"><b>Why are there neuroscientists? — Workshop of Ideas in Neuroscience (Heidelberg Open Lab)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>December 5, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Heidelberg, Germany</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://openlabhd.org/why/">https://openlabhd.org/why/</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Hosted by Heidelberg Open Lab in collaboration with the Schools and Workshops of Ideas program.</li>
          <li>Focus: exploring researchers’ motivations and values in neuroscience.</li>
          <li>Format: short talks, group discussions, and collaborative reflection.</li>
          <li>Organizer contact: Mateusz Kostecki, Nencki Institute of Experimental Biology (Poland).</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<!-- Workshop: Animal Tracking in DeepLabCut (Heidelberg Open Lab) -->
<div class="card md-6" data-tags="workshop deeplabcut behavioral-analysis tracking openlabhd heidelberg python">
  <div class="title"><b>Animal Tracking in DeepLabCut — Heidelberg Open Lab Workshop</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>November 20–21, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>COS Heidelberg Institute, Germany</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://openlabhd.org/2025/10/21/animal-tracking-in-deeplabcut-nov-20-21th/">openlabhd.org — Workshop Info</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Cost</div><div>Free (in-person only)</div></div>
    <div class="meta-row"><div class="meta-label">Instructors</div><div>Konrad Danelewski (Nencki Institute)</div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Hands-on training in pose estimation and behavioral analysis with DeepLabCut.</li>
          <li>Includes annotation, model training, evaluation, and multi-animal tracking.</li>
          <li>Participants encouraged to bring their own experimental videos.</li>
          <li>Designed for Master’s, PhD students, postdocs, and researchers using behavioral tracking tools.</li>
          <li>Requirements: Basic Python knowledge and personal laptop.</li>
        </ul>
      </div>
    </div>
  </div>
</div>
</div>

  
  <h4>Classes</h4>
<div class="cards">

  <div class="card md-6">
    <div class="title"><b>Creating Perceptual Experiments with Unity and VR (CIC 33)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>October 27, 2025 • 8:30 am–12:45 pm HKT</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Online</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.imaging.org/IST/iCore/Events/Function_Display.aspx?EventKey=C25&amp;FunctionKey=C25/SC03">Course information</a></div></div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>Part of the <b>Color and Imaging Conference (CIC 2025)</b>, Hong Kong (Online-only)</li>
            <li><b>Date &amp; Time:</b> October 27 2025 (8:30 am–12:45 pm HKT) / October 26 8:30 pm–12:45 am EST</li>
            <li>Instructor: <a href="mailto:rfm@yorku.ca">Richard F. Murray</a> (York University)</li>
            <li>No CIC registration required — may register for the course alone at reduced fee</li>
            <li>Topics: Unity for psychophysics, C# essentials, luminance calibration via tonemapping, callback-based experimental coding</li>
            <li>More info: <a href="https://www.imaging.org/IST/IST/Conferences/CIC/CIC2025/CIC_Home.aspx">CIC 2025 Homepage</a> | <a href="https://www.imaging.org/IST/iCore/Events/Function_Display.aspx?EventKey=C25&amp;FunctionKey=C25/SC03">Course Page</a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="card md-6">
    <div class="title"><b>Imaging for XR 2025 (IS&amp;T)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>November 14, 2025</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Online</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://bit.ly/Imaging4XR_Register">https://bit.ly/Imaging4XR_Register</a></div></div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>Time: 08:00–16:00 PT (11:00–19:00 ET; 17:00–01:00 Paris)</li>
            <li>Early-bird deadline: October 14, 2025</li>
            <li>Student &amp; classroom discounts available</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="card md-6">
    <div class="title"><b>A Practical Introduction to Eye Tracking</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>November 26–28, 2025</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Lund, Sweden</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.humlab.lu.se/education/commissioned-education/">Course page</a></div></div>
      <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>TBD</li></ul></div></div>
    </div>
  </div>

  <div class="card md-6">
    <div class="title"><b>LightLogR: Open &amp; Reproducible Analysis of Light Exposure &amp; Visual Experience Data</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>Beginner: Mar 4, 2026; Advanced: Dec 9, 2025 • May 6, 2026</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Online</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://tum-conf.zoom.us/webinar/register/WN_xGoHi1i_Sz2flC6-qGcLoQ#/registration">Registration (Zoom)</a></div></div>
      <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Hands-on training in open, reproducible analysis</li></ul></div></div>
    </div>
  </div>

</div>





  
<h3 id="gradprograms">Grad Programs</h3>
<p class="section-desc">
  Doctoral and Masters opportunities. Includes dedicated PhD and MSc programs submitted by members.
</p>
<div class="cards">


<!-- Grad Program: Penn ITCN PhD Training Program -->
<div class="card md-6"
  data-tags="gradprogram phd computational-neuroscience training-program penn interdisciplinary brain-modeling ai neuroengineering 2025">

  <div class="title"><b>Interdisciplinary Training in Computational Neuroscience (ITCN) — University of Pennsylvania</b></div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        PhD-level interdisciplinary training across Neuroscience, Bioengineering, Physics, and Psychology, 
        integrating computational methods with experimental neuroscience.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Deadlines</div>
      <div>
        December 1, 2025 — Neuroscience, Psychology, Physics<br>
        December 15, 2025 — Bioengineering
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Participation</div>
      <div>
        Students apply to a home PhD program first, then apply to ITCN before or during Year 1.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Application</div>
      <div>Submit materials to: Vijay Balasubramanian, Maria Geffen, and Joshua Gold.</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div>mgeffen@pennmedicine.upenn.edu</div>
    </div>

  </div>
</div>


<!-- Grad Program: PhD Positions in NeuroAI — University of Birmingham -->
<div class="card md-6"
  data-tags="gradprogram phd neuroai computational-neuroscience machine-learning motor-planning modelling uk birmingham 2025">

  <div class="title"><b>PhD Positions in NeuroAI — University of Birmingham (UK)</b></div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Areas</div>
      <div>
        Computational neuroscience · neural dynamics · neural network modeling · motor sequence planning · 
        human &amp; non-human primate data integration.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Featured Project Deadline</div>
      <div>November 27, 2025 (still open)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Other Openings</div>
      <div>Additional funded PhD projects available (rolling applications).</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Apply</div>
      <div><a href="https://warwick.ac.uk/fac/cross_fac/mibtp/phd/supervisors/jliu">Application page</a></div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div>Dr. Jian Liu — jiankliu@gmail.com</div>
    </div>

  </div>
</div>


<!-- PhD Opportunity — Color Imaging Lab (UGR), Spain -->
<div class="card md-6" 
  data-tags="gradprogram phd color-imaging hyperspectral multispectral art-conservation cultural-heritage machine-learning spectral-data vision-science spain">
  
  <div class="title"><b>PhD Positions — Color Imaging Lab (University of Granada, Spain)</b></div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Program</div>
      <div>FPU-funded PhD positions (Spanish Ministry)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Research Areas</div>
      <div>
        Multispectral &amp; hyperspectral imaging (UV → IR) · non-invasive analysis of artworks · digital twins · 
        ML for spectral data (detection, classification, unmixing) · modeling color perception &amp; color-vision deficiencies.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Lab Highlights</div>
      <div>
        Advanced imaging systems (hyperspectral/multispectral, spectroradiometers, thermal, UV/IR), access to XRF, XRD, FTIR, 
        macrophotography; strong publication record; interdisciplinary collaborators across fine arts, restoration, biology, 
        engineering, and cultural heritage.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Profile Sought</div>
      <div>
        Students in Physics, Engineering, Math, CS, or related fields; strong academic record; programming experience 
        (Matlab/Python preferred); motivated and research-driven.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">How to Apply</div>
      <div>
        Email <a href="mailto:colorimg@ugr.es">colorimg@ugr.es</a> with CV, transcript, and motivation letter.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Lab Website</div>
      <div>
        <a href="https://colorimaginglab.ugr.es/">colorimaginglab.ugr.es</a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Social</div>
      <div>
        Instagram: <a href="https://www.instagram.com/cimlabugr/">cimlabugr</a> · 
        X: <a href="https://x.com/colorimaginglab">@colorimaginglab</a> · 
        Facebook: <a href="https://www.facebook.com/ColorImgUGR">ColorImgUGR</a>
      </div>
    </div>

  </div>
</div>


<!-- PHD: Lingnan University — Eye Movements to Faces -->
<div class="card md-6" id="lingnan-hayward-phd-2026" data-tags="gradprograms phd vision eye-movements faces cognitive-science hong-kong">
  <div class="title"><b>PhD — Eye Movements to Faces (Lingnan University, Hong Kong)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">PI</div><div>Prof. Will Hayward — Chair Professor of Psychology & Dean of Social Sciences</div></div>
    <div class="meta-row"><div class="meta-label">Lab</div><div>Lingnan Visual Cognition Lab · LU Cognitive Science Research Centre</div></div>
    <div class="meta-row"><div class="meta-label">Project</div>
      <div>Funded doctoral project on <b>idiosyncratic eye-movement patterns to faces</b>.  Uses eye-tracking and data-analytic methods to understand individual differences in face scanning.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Preferred Skills</div>
      <div>Eye-tracking (Eyelink 1000, Tobii Pro Fusion, Pupil Labs Neon); PsychoPy / PsychToolbox; strong quantitative analysis skills.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>December 1 2025 (Hong Kong time)</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>September 2026</div></div>
    <div class="meta-row"><div class="meta-label">Funding</div>
      <div>Hong Kong PhD Fellowship Scheme (<a href="https://www.ln.edu.hk/rpg/hong-kong-phd-fellowship-scheme-hkpfs">HKPFS</a>) — administered by the Hong Kong Research Grants Council and available for outstanding applicants.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Facilities</div>
      <div>Tobii Pro Fusion & Pupil Labs Neon eye-trackers · Neuroscan Okti 128 Ch EEG system · newly refurbished lab space with conference and travel support.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Example Study</div>
      <div><a href="https://doi.org/10.1080/13506285.2024.2315783">Zhong et al., Visual Cognition (2023)</a> — association of idiosyncratic eye-movements with holistic face processing.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div>
      <div><a href="mailto:willhayward@ln.edu.hk">willhayward@ln.edu.hk</a> · <a href="https://www.ln.edu.hk/psy/about-us/people/academic-staff/professor-hayward-william">Faculty Profile</a></div>
    </div>
  </div>
</div>


<!-- PHD: JLU Giessen × MPI CBS — Computational Cognitive Neuroscience -->
<div class="card md-6" id="jlu-giessen-phd-hebart-2025" data-tags="gradprograms phd computational-neuroscience cognitive-neuroscience vision python germany giessen leipzig adaptive-mind">
  <div class="title"><b>PhD (4 years) — Computational Cognitive Neuroscience</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Institution</div><div>Justus Liebig University Giessen (JLU) — Hebart Lab, with collaboration at Max Planck Institute for Human Cognitive & Brain Sciences (Leipzig)</div></div>
    <div class="meta-row"><div class="meta-label">Cluster</div><div>Excellence Cluster “The Adaptive Mind”</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div><b>November 25, 2025</b> (reference no. <b>531/11</b>)</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>January 2026 or as soon as interviews complete</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Giessen (primary) · Leipzig (collaborations)</div></div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>Project to characterize the dimensions underlying human mental object representations across a broader range of visual content, with options to explore cross-cultural comparisons, VR scene interactions, and links to AI systems.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Requirements</div>
      <div>Strong interest in visual/cognitive computational neuroscience; very good to excellent <b>Python</b> programming skills.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Apply</div>
      <div>
        Official ad: <a href="https://www.uni-giessen.de/de/ueber-uns/karriere/stellenangebote/wissenschaftliche-mitarbeiter/531-11-e">uni-giessen.de — Posting 531/11</a><br/>
        Application portal: <a href="https://www.uni-giessen.de/de/ueber-uns/karriere/application">uni-giessen.de/…/application</a> (use ref. <b>531/11</b>)
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">PI / Contact</div><div>Dr. Martin Hebart — <a href="mailto:martin.hebart@gmail.com">martin.hebart@gmail.com</a></div></div>
    <div class="meta-row"><div class="meta-label">Background</div>
      <div>Related prior work: <a href="https://www.nature.com/articles/s41562-020-00951-3">Nature Human Behaviour (2020)</a>.</div>
    </div>
  </div>
</div>


<!-- GRAD PROGRAM: NYU Visual Sciences PhD -->
<div class="card md-6" id="nyu-visual-sciences-phd" data-tags="gradprograms phd visual-science nyu neuroscience psychology data-science computational-vision">
  <div class="title"><b>Doctoral Studies in Visual Sciences — New York University</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>NYU hosts a large interdisciplinary community in the Visual Sciences spanning neuroscience, psychology, computer science, data science, mathematics, and philosophy.  Students may pursue cross-departmental research under faculty across these programs.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Participating Departments</div>
      <div>
        <ul class="notes-list">
          <li><b>Center for Neural Science (CNS)</b> — deadline: <b>Dec 1 2025</b> — <a href="https://as.nyu.edu/cns/DoctoralProgram.html">CNS Doctoral Program</a></li>
          <li><b>Psychology (Cognition & Perception)</b> — deadline: <b>Dec 1 2025</b> — <a href="http://as.nyu.edu/psychology/graduate/phd-cognition-perception.html">Psychology PhD</a></li>
          <li><b>Data Science</b> — deadline: <b>Dec 4 2025</b> — <a href="https://cds.nyu.edu/phd-program/">CDS PhD</a></li>
          <li><b>Computer Science</b> — deadline: <b>Dec 12 2025</b> — <a href="http://www.cs.nyu.edu/home/phd/">CS PhD</a></li>
          <li><b>Mathematics</b> — deadline: <b>Dec 18 2025</b> — <a href="https://gsas.nyu.edu/admissions/arc/programs/mathematics.html">Mathematics PhD</a></li>
          <li><b>Biology</b> — deadline: <b>Dec 1 2025</b> — <a href="https://as.nyu.edu/biology/academics/phd.html">Biology PhD</a></li>
          <li><b>Philosophy</b> — deadline: <b>Jan 7 2026</b> — <a href="https://as.nyu.edu/departments/philosophy/graduate.html">Philosophy PhD</a></li>
        </ul>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Faculty Highlights</div>
      <div>Includes Eero Simoncelli, Marisa Carrasco, Michael Landy, David Heeger, Rob Fergus, Yann LeCun, Grace Lindsay, Claude Desplan, Ned Block, David Chalmers, and others across NYU CNS, Psychology, Data Science, and related departments.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:eero.simoncelli@nyu.edu">eero.simoncelli@nyu.edu</a></div></div>
  </div>
</div>


<!-- PhD: University of Hertfordshire — Continual & Open-ended Reinforcement Learning -->
<div class="card md-6" data-tags="phd program reinforcement-learning continual-learning open-ended-learning robotics computer-vision hertfordshire uk">
  <div class="title"><b>PhD — Continual &amp; Open-ended Reinforcement Learning (University of Hertfordshire)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Department</div><div>Computer Science — Adaptive Systems Research Group</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Hatfield, Hertfordshire, United Kingdom</div></div>
    <div class="meta-row"><div class="meta-label">Funding</div><div>Fully funded: £20,700/year bursary + tuition fees</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>November 14, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.herts.ac.uk/study/schools-of-study/physics-engineering-and-computer-science/engineering-and-computer-science/research-in-engineering-and-computer-science/the-phd-programme-in-computer-science">Application portal &amp; program info</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:n.catenacci-volpi@herts.ac.uk">Dr. Nicola Catenacci&nbsp;Volpi</a></div></div>
    <div class="meta-row">
      <div class="meta-label">Areas</div>
      <div>Deep RL, continual/open-ended learning, intrinsic motivation, probabilistic ML, information theory; benchmarks in games &amp; robotics (simulation/real-world).</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Requirements</div>
      <div>CS/Math background, strong programming; RL/robotics experience desirable; fluency in English.</div>
    </div>
  </div>
</div>


<!-- PhD — UMass Boston (Developmental & Brain Sciences) -->
<div class="card md-6" data-tags="graduate phd neuroscience developmental brain boston umass">
  <div class="title"><b>University of Massachusetts Boston — PhD in Developmental &amp; Brain Sciences</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual Admissions</div></div>
    <div class="meta-row"><div class="meta-label">Deadlines</div><div>Application deadline: December&nbsp;1, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Boston, MA (USA)</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://www.umb.edu/academics/program-finder/developmental-brain-sciences-phd/">Program page</a> •
        <a href="https://go.umb.edu/register/?id=ede749c0-f480-49af-9b56-5a5ef8a768aa">Info session registration</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Online info session: October&nbsp;22, 2025 at 4:30&nbsp;pm</li>
          <li>Research spans cognitive &amp; behavioral neuroscience; training across psychophysics, EEG, molecular/genetic methods</li>
          <li>Limited fee waivers available</li>
          <li>Contact: <a href="mailto:dbs@umb.edu">dbs@umb.edu</a></li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Graduate Program: University of Houston — Physiological Optics & Vision Science -->
<div class="card md-6" data-tags="graduate phd ms vision-science optometry houston">
  <div class="title"><b>University of Houston — Graduate Program in Physiological Optics &amp; Vision Science</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Degrees</div><div>PhD &amp; MS</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>January&nbsp;31,&nbsp;2026 (for Fall&nbsp;2026 entry)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Houston, Texas (USA)</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div>
        <a href="https://www.opt.uh.edu/education/graduate-programs/">Graduate Programs overview</a> •
        <a href="https://www.opt.uh.edu/education/graduate-programs/how-to-apply.cfm">How to apply</a> •
        <a href="https://www.opt.uh.edu/research/areas-of-focus/">Research areas</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Research Areas</div>
      <div>Biomedical Optics • Visual Neuroscience • Ocular Biology • Clinical &amp; Translational Research</div>
    </div>
    <div class="meta-row"><div class="meta-label">Funding</div>
      <div>Tuition fellowships and stipends as teaching or research assistants.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Contacts</div>
      <div>
        Laura&nbsp;J.&nbsp;Frishman, PhD — Associate Dean (<a href="mailto:lfrishma@central.uh.edu">lfrishma@central.uh.edu</a>)<br/>
        Rebecca Rattelade — Graduate Program Coordinator (<a href="mailto:rrattelade@central.uh.edu">rrattelade@central.uh.edu</a>)
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        The program offers interdisciplinary training across basic, clinical, and translational vision sciences.
        Research ranges from molecular and cellular mechanisms to behavioral and optical studies.
        Students work with collaborative faculty in the University of Houston College of Optometry.
      </div>
    </div>
  </div>
</div>



<div class="card md-6" data-tags="graduate phd vision neuroscience computational nyu">
  <div class="title"><b>New York University — Doctoral Studies in Visual Sciences</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Frequency</div><div>Annual Admissions</div></div>
    <div class="meta-row"><div class="meta-label">Deadlines</div><div>
      Center for Neural Science: Dec&nbsp;1 • Psychology (Cognition &amp; Perception): Dec&nbsp;1 •
      Data Science: Dec&nbsp;4 • Computer Science: Dec&nbsp;12 • Mathematics: Dec&nbsp;18 • Biology: Dec&nbsp;1 • Philosophy: Jan&nbsp;7
    </div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>New York, NY (USA)</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div>
        <a href="https://as.nyu.edu/cns/DoctoralProgram.html">Center for Neural Science</a> •
        <a href="http://as.nyu.edu/psychology/graduate/phd-cognition-perception.html">Psychology</a> •
        <a href="https://cds.nyu.edu/phd-program/">Data Science</a> •
        <a href="http://www.cs.nyu.edu/home/phd/">Computer Science</a> •
        <a href="https://gsas.nyu.edu/admissions/arc/programs/mathematics.html">Mathematics</a> •
        <a href="https://as.nyu.edu/biology/academics/phd.html">Biology</a> •
        <a href="https://as.nyu.edu/departments/philosophy/graduate.html">Philosophy</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        NYU hosts a large interdisciplinary community in visual sciences. Programs span computational neuroscience,
        psychology, biology, data science, and philosophy of mind. Faculty include Eero&nbsp;Simoncelli, David&nbsp;Heeger,
        Marisa&nbsp;Carrasco, Yann&nbsp;LeCun, and others. Students can pursue cross-department research.
      </div>
    </div>
  </div>
</div>


<!-- Graduate Programs — University of Houston (Physiological Optics & Vision Science) -->
<div class="card md-6">
  <div class="title"><b>PhD &amp; MS — Physiological Optics &amp; Vision Science (Fall 2026 entry)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>University of Houston — College of Optometry</div></div>
    <div class="meta-row"><div class="meta-label">Category</div><div>Graduate Program (PhD, MS)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>January 31, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Houston, Texas, USA</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://www.opt.uh.edu/education/graduate-programs/" rel="noopener">Program overview</a><br>
        <a href="https://www.opt.uh.edu/education/graduate-programs/how-to-apply.cfm" rel="noopener">How to apply</a><br>
        <a href="https://www.opt.uh.edu/research/areas-of-focus/" rel="noopener">Research areas</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div><div>Grad Studies (Program Manager): <a href="mailto:rrattelade@central.uh.edu">rrattelade@central.uh.edu</a> · +1 (713) 743-1885</div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Tuition fellowships and stipends available via TA/RA appointments.</li>
          <li>Typical application window runs Oct 1–Feb 1; date above reflects the program’s stated target for Fall 2026.</li>
        </ul>
      </div>
    </div>
  </div>
</div>


  <div class="card md-6">
    <div class="title"><b>PhD Position — Cognitive &amp; Neural Development Lab (UMass Amherst)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>Start: Fall 2026</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>University of Massachusetts Amherst, Amherst, MA</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div><a href="https://www.umass.edu/pbs/blaser-lab" aria-label="UMass Amherst Cognitive & Neural Development PhD position">Blaser Lab – UMass Amherst</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>Research in cognitive and neural development (behavioral + fMRI)</li>
            <li>Mentored by Dr. Erik Blaser (lab PI)</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

 <div class="card md-6">
      <div class="title"><b>PhD or Postdoctoral Position – Vision Science</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>University of Iceland, Reykjavik</div></div>
        <div class="meta-row"><div class="meta-label">Website</div>
          <div><a href="https://visionscience.com/pipermail/visionlist_visionscience.com/2025/009473.html">VisionList post</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Icelandic Vision Lab</li></ul></div></div>
      </div>
    </div>
  </div>


  <!-- Jobs -->
  <h3 id="jobs">Jobs</h3>
<p class="section-desc">
  PhD, postdoc, RA, and faculty positions in vision, perception, neuroscience, imaging, and allied areas.
</p>

<h4>Jobs and Student Positions</h4>
  <div class="cards">

<!-- Student Position: Genentech Summer 2026 Internship (Ophthalmology Biomarkers) -->
<div class="card md-6"
  data-tags="job internship student-position genentech industry translational-medicine retina biomarkers imaging clinical-trials 2026">

  <div class="title"><b>Summer 2026 Internship — OMNI Ophthalmology (Genentech, South San Francisco)</b></div>

  <div class="meta">

    <div class="meta-row"><div class="meta-label">Role</div>
      <div>
        Lead a research project on retinal imaging biomarkers and clinical outcomes in retinal vascular disease. 
        Analyze clinical-trial datasets; explore mechanisms &amp; treatment response; communicate findings across teams.
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Location</div><div>South San Francisco, CA (On-site)</div></div>

    <div class="meta-row"><div class="meta-label">Start</div><div>May / June 2026</div></div>

    <div class="meta-row"><div class="meta-label">Apply</div>
      <div>
        Apply via Workday:<br>
        <a href="https://roche.wd3.myworkdayjobs.com/ROG-A2O-GENE/job/South-San-Francisco/XMLNAME-2026-Summer-Intern---Translational-Medicine--Ophthalmology-Biomarker-Development_202511-130178">
        Genentech Careers: Ophthalmology Biomarker Internship</a>
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Contact</div><div>Charlotte Wang, OD PhD — wang.charlotte@gene.com</div></div>

  </div>
</div>


<!-- Industry: Vision Scientist (Computational Modeling) — Apple -->
<div class="card md-6" id="apple-vision-scientist-2025" data-tags="jobs industry vision-science computational-modeling imaging displays apple cupertino">
  <div class="title"><b>Vision Scientist (Computational Modeling) — Apple Vision Science</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Organization</div>
      <div>Apple — Vision Science Team</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Cupertino, California, USA</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Role</div>
      <div>
        Vision scientist focusing on computational modeling and psychophysical experiments
        to support cutting-edge imaging and display technologies across the Apple ecosystem
        (e.g., iPhone, iPad, Mac, Watch, and emerging platforms).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        Join a collaborative, interdisciplinary team working at the intersection of
        vision science, image processing, color science, neuroscience, and optics.
        The position centers on developing vision models and metrics to quantify and
        optimize user experience for advanced display and imaging systems.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Responsibilities</div>
      <div>
        Develop and validate models/metrics of human vision and image quality •
        Design and run psychophysical studies of user experience •
        Compare and optimize algorithms for perceptual quality •
        Inform feature and specification decisions for future Apple products.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Profile</div>
      <div>
        Strong background in vision science or related fields (e.g., neuroscience,
        optics, image processing) • Experience with computational modeling and
        quantitative data analysis • Comfortable working in an interdisciplinary,
        product-oriented environment.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://jobs.apple.com/en-us/details/200631317-0836/vision-scientist">
          Apple Jobs — Vision Scientist (Computational Modeling)
        </a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div>
        Applicants may optionally notify<br>
        <a href="mailto:laura_walker@apple.com">Laura&nbsp;Walker, PhD (Apple | Vision Science)</a><br>
        (no responses to direct inquiries; applications via Apple Jobs link only).
      </div>
    </div>

  </div>
</div>


<!-- Internship: JEP:HPP Intern Junior Editorial (IJE) 2026 -->
<div class="card md-6" data-tags="jobs editorial internship phd postdoc jep-hpp psychology publishing">
  <div class="title"><b>Intern Junior Editorial (IJE) Positions — JEP:HPP 2026</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>Journal of Experimental Psychology: Human Perception &amp; Performance (APA)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>Term: January 1 – December 31, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>November 28, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Eligibility</div>
      <div>Open to PhD students (≥3rd year) and postdocs ≤5 years; international applicants welcome</div>
    </div>
    <div class="meta-row"><div class="meta-label">Compensation</div><div>$1,500 USD honoraria stipend</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://redcap.link/HPPIJE26">Application link (REDCap)</a></div></div>
    <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:isabel.gauthier@vanderbilt.edu">Isabel Gauthier</a> (Editor, JEP:HPP)</div></div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>12-month editorial training role assisting with the journal’s pre-external review process.</li>
          <li>Work on 3–4 manuscripts / month under direct mentorship of the editor.</li>
          <li>Focus on transparency, rigor, and feedback to authors prior to full peer review.</li>
          <li>Encourages participation from historically excluded groups in scientific publishing leadership.</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<!-- Internship: Computation & Cognition Tübingen Summer Internship (CaCTüS) -->
<div class="card md-6" data-tags="jobs internships summer2026 germany tuebingen ai neuroscience">
  <div class="title"><b>Computation &amp; Cognition Tübingen Summer Internship (CaCTüS) 2026</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Type</div><div>Paid Summer Internship (Undergraduate / Master’s level)</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>Summer 2026 (3 months)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Max Planck Institutes for Biological Cybernetics &amp; Intelligent Systems, Tübingen, Germany</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>November 20, 2025</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://cactus-internship.tuebingen.mpg.de/">cactus-internship.tuebingen.mpg.de</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Up to 12 paid positions (≈3 months)</li>
          <li>Projects span AI, machine learning, theoretical neuroscience, robotics, and behavioral research</li>
          <li>Open to Bachelor’s and Master’s students worldwide</li>
          <li>Priority for applicants facing financial, regional, or societal barriers</li>
          <li>Organized by the Max Planck Institutes &amp; the Tübingen AI Center</li>
          <li>Contact: <a href="mailto:manuel.spitschan@tum.de">manuel.spitschan@tum.de</a></li>
        </ul>
      </div>
    </div>
  </div>
</div>


<!-- Doctoral positions: MSCA-DN EXPLORA -->
<div class="card md-6" data-tags="jobs phd doctoral europe msca materials perception multisensory">
  <div class="title"><b>MSCA-DN EXPLORA — Doctoral Positions (Perception of Materials, Objects &amp; Spaces)</b></div>
  <div class="meta">

    <div class="meta-row"><div class="meta-label">Employer</div>
      <div>EXPLORA — Marie Skłodowska-Curie Doctoral Network (multiple host universities & labs)</div>
    </div>

    <div class="meta-row"><div class="meta-label">Fields</div>
      <div>Psychology • Neuroscience • Robotics • Computer Science • Architecture &amp; Design</div>
    </div>

    <div class="meta-row"><div class="meta-label">Location</div>
      <div>Multiple sites across Europe (host-specific)</div>
    </div>

    <div class="meta-row"><div class="meta-label">Website</div>
      <div>
        <a href="https://explora-network.github.io/web/">Project overview</a> •
        <a href="https://explora-network.github.io/web/projects.html">Open projects &amp; hosts</a>
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Requirements</div>
      <div>
        Must not already hold a doctoral degree; EU mobility rule applies (not resident in host country &gt;12 months in the 36 months before recruitment).
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Funding</div>
      <div>MSCA fellowship (3 years); remuneration per EC rules; total duration may be 3–4 years depending on host regulations.</div>
    </div>

    <div class="meta-row"><div class="meta-label">Contact</div>
      <div>
        EXPLORA Coordinator: <a href="mailto:katja.doerschner@psychol.uni-giessen.de">katja.doerschner@psychol.uni-giessen.de</a> •
        Co-coordinator: <a href="mailto:S.C.Pont@tudelft.nl">S.C.Pont@tudelft.nl</a>
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        International training network with cross-sector workshops, collaboration across partner labs, and 2 research secondments for each doctoral candidate.
      </div>
    </div>

  </div>
</div>


<!-- Job: Johnson & Johnson Vision — Staff R&D Scientist (IOL / Vision Science) -->
<div class="card md-6" data-tags="jobs industry optics iol vision-science netherlands jnj">
  <div class="title"><b>Johnson & Johnson Vision — Staff R&amp;D Scientist (IOL Optics & Vision Science)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Employer</div><div>Johnson &amp; Johnson Vision (AMO Groningen B.V.)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Groningen, Netherlands</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://jj.wd5.myworkdayjobs.com/JJ/job/Groningen-Netherlands/R-D-Scientist_R-030298">Apply / Job posting</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div>
      <div><a href="mailto:CCanovas@its.jnj.com">CCanovas@its.jnj.com</a> (Carmen Cánovas Vidal, Director R&amp;D)</div>
    </div>
    <div class="meta-row"><div class="meta-label">Requirements</div>
      <div>PhD in Optics/Physics/Vision Science/Mechanical Modelling (ideal); 5+ years experience; proficiency with optical design (Zemax/OSLO/etc.).</div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>Role focuses on design &amp; development of intraocular lens (IOL) optical systems within a small, integrated R&amp;D team.</div>
    </div>
  </div>
</div>


<!-- Administrative — University of Cambridge (Adaptive Brain Lab) -->
<div class="card md-6">
  <div class="title"><b>Research Programme Manager — Adaptive Brain Lab (Full-time or Part-time)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>University of Cambridge — Department of Psychology (ABL &amp; AI-deas BrainHealthX Hub)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Cambridge, UK</div></div>
    <div class="meta-row"><div class="meta-label">Category</div><div>Professional Services / Programme Management (≥3 days/week possible)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 15, 2025</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://www.cam.ac.uk/jobs/research-programme-manager-full-time-or-part-time-pj47418" rel="noopener">cam.ac.uk/jobs — PJ47418</a><br>
        <a href="https://www.jobs.cam.ac.uk/job/52958/file/Further%2BParticulars_Research%2BProgramme%2BManager.pdf" rel="noopener">Further Particulars (PDF)</a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Coordinate cross-disciplinary research programmes and partnerships; support ABL &amp; BrainHealthX Hub.</li>
          <li>Salary band listed on site for professional services roles; see posting for range and details.</li>
          <li>Reference: <code>PJ47418</code>.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Job: Scientific Coordinator — Excellence Cluster "The Adaptive Mind" (JLU Giessen) -->
<div class="card md-6"
  data-tags="jobs coordinator scientific-coordinator germany europe giessen adaptive-mind psychology neuroscience cognitive-science ai ml robotics">
  <div class="title"><b>Scientific Coordinator — Excellence Cluster “The Adaptive Mind” (JLU Giessen)</b></div>

  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Organization</div>
      <div>Justus Liebig University Giessen (JLU), Excellence Cluster “The Adaptive Mind”</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Giessen, Germany</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Role</div>
      <div>Scientific Coordinator (project-duration, leadership role)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Fields</div>
      <div>
        Experimental, clinical & developmental psychology; cognitive science; neuroscience; AI/ML; robotics.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Deadline</div>
      <div>Listed as “soon” (apply immediately)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Apply</div>
      <div>
        <a href="https://www.uni-giessen.de/de/ueber-uns/karriere/stellenangebote/administratives-personal/527-06-1" target="_blank" rel="noopener">
          Job posting
        </a>
      </div>
    </div>
  </div>
</div>


<!-- Job: TrainingHub Coordinator — Excellence Cluster "The Adaptive Mind" (JLU Giessen) -->
<div class="card md-6"
  data-tags="jobs coordinator traininghub coordinator-permanent germany europe giessen adaptive-mind psychology neuroscience cognitive-science ai ml robotics">
  <div class="title"><b>TrainingHub Coordinator — Excellence Cluster “The Adaptive Mind” (JLU Giessen)</b></div>

  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Organization</div>
      <div>Justus Liebig University Giessen (JLU), Excellence Cluster “The Adaptive Mind”</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Giessen, Germany</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Role</div>
      <div>Coordinator of TrainingHub (permanent position; German language required)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Fields</div>
      <div>
        Experimental, clinical & developmental psychology; cognitive science; neuroscience; AI/ML; robotics.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Deadline</div>
      <div>Listed as “soon” (apply immediately)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Apply</div>
      <div>
        <a href="https://www.uni-giessen.de/de/ueber-uns/karriere/stellenangebote/administratives-personal/532-06" target="_blank" rel="noopener">
          Job posting (German)
        </a>
      </div>
    </div>
  </div>
</div>



<!-- Postdoc/RA — University of Cambridge (Adaptive Brain Lab) -->
<div class="card md-6">
  <div class="title"><b>Research Assistant/Associate — Cognitive &amp; Computational Neuroscience (Fixed Term)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>University of Cambridge — Adaptive Brain Lab, Department of Psychology</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Cambridge, UK</div></div>
    <div class="meta-row"><div class="meta-label">Category</div><div>Postdoctoral (Research Associate) / Research Assistant — Fixed Term (18 months)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 20, 2025</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.cam.ac.uk/jobs/research-assistantassociate-fixed-term-pj47416" rel="noopener">cam.ac.uk/jobs — PJ47416</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">PI / Contact</div><div>Prof. Zoe Kourtzi — <a href="mailto:zk240@cam.ac.uk">zk240@cam.ac.uk</a></div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Focus: neuroimaging &amp; neurocomputational studies of learning and brain plasticity.</li>
          <li>Methods: 7T fMRI / MRS, EEG, TMS/tDCS, ML/RL modelling.</li>
          <li>Reference: <code>PJ47416</code>; initial term 18 months.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- PhD — University of Nevada, Reno (Berryhill & Haigh Labs) -->
<div class="card md-6">
  <div class="title"><b>PhD Positions — Cognitive &amp; Brain Sciences (UNR)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>University of Nevada, Reno — Labs of Marian Berryhill &amp; Sarah Haigh</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Reno, Nevada, USA</div></div>
    <div class="meta-row"><div class="meta-label">Deadlines</div><div><b>Nov 3, 2025</b> (Spring 2026 start) • <b>Dec 15, 2025</b> (Fall 2026 start)</div></div>
    <div class="meta-row"><div class="meta-label">Research</div>
      <div>Sensory &amp; working memory; subclinical populations (e.g., schizotypy, autism); team-based research</div>
    </div>
    <div class="meta-row"><div class="meta-label">Funding</div><div>Permanent teaching assistantships; support for external fellowships</div></div>
    <div class="meta-row"><div class="meta-label">Contact</div>
      <div><a href="mailto:mberryhill@unr.edu">mberryhill@unr.edu</a> • <a href="mailto:shaigh@unr.edu">shaigh@unr.edu</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Web</div>
      <div><a href="https://labs.psych.unr.edu/mblab/" rel="noopener">Berryhill Lab</a> • <a href="https://sarahmhaigh.github.io/" rel="noopener">Haigh Lab</a></div>
    </div>
  </div>
</div>

<!-- PhD — Scalable Indexing & Retrieval in Multimedia and Geospatial Contents (DALEAS Project) -->
<div class="card md-6">
  <div class="title"><b>PhD — Scalable Indexing &amp; Retrieval in Multimedia and Geospatial Contents (DALEAS Project)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>LASTIG Lab / IGN &amp; Gustave Eiffel University (Paris area, France)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div><b>November 21, 2025</b></div></div>
    <div class="meta-row"><div class="meta-label">Summary</div>
      <div>
        Research on large-scale multimedia content indexing and retrieval (images, text, 3D point clouds) for geospatial data; integrating multimodal large language models and Apache Spark™ for efficient, flexible search and geolocation across heterogeneous datasets.
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Keywords</div>
      <div>Information retrieval, indexing, multimodal learning, 3D point clouds, geolocation, big data, Apache Spark</div>
    </div>
    <div class="meta-row"><div class="meta-label">Apply</div>
      <div>Send CV, cover letter, transcripts (3 years), and 2 referee contacts (single PDF) to <a href="mailto:Valerie.Gouet@ign.fr">Valerie.Gouet@ign.fr</a> and <a href="mailto:Laurent.Caraffa@ign.fr">Laurent.Caraffa@ign.fr</a> before <b>Nov 21, 2025</b>.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Timeline</div>
      <div>Interviews: Nov 24–Dec 5, 2025 • Decision: by Dec 10, 2025</div>
    </div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://www.umr-lastig.fr/vgouet/News/PhD_Thesis_proposal25-DALEAS.pdf">PhD proposal (PDF)</a></div>
    </div>
  </div>
</div>

<!-- PhD — Early Stage Detection of Alzheimer’s & Parkinson’s Disease using Retinal Imaging -->
<div class="card md-6">
  <div class="title"><b>PhD — Early Stage Detection of Alzheimer’s &amp; Parkinson’s Disease using Advanced Retinal Imaging Technology</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div>
      <div>University of Manchester, Manchester Royal Eye Hospital, and Greater Manchester Mental Health NHS Foundation Trust</div>
    </div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Manchester, United Kingdom</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div><b>November 8, 2025</b></div></div>
    <div class="meta-row"><div class="meta-label">Summary</div>
      <div>PhD project using advanced retinal imaging and machine learning to develop sensitive and specific diagnostic tools for early detection of Alzheimer’s and Parkinson’s disease.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Methods</div>
      <div>Adaptive Optics Optical Coherence Tomography (AO-OCT), Canon Xephilio ultra-wide retinal imaging, AI-based image analysis, and statistical/machine learning modeling.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Apply</div>
      <div>
        Apply via <a href="https://www.findaphd.com/phds/project/mrc-dtp-early-stage-detection-of-alzheimer-s-and-parkinson-s-disease-using-advanced-retinal-imaging-technology/?p187166" rel="noopener">FindAPhD — Project P187166</a>.
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div>
      <div><a href="mailto:karen.hampson-2@manchester.ac.uk">karen.hampson-2@manchester.ac.uk</a></div>
    </div>
  </div>
</div>

<div class="card md-6" id="camma-phd-vision-healthcare-2026" data-tags="job phd computer-vision ai healthcare surgery france erc">
  <div class="title"><b>2 PhD Positions — AI/Computer Vision for Healthcare (CAMMA, Strasbourg)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Institution</div><div>CAMMA Research Group, University of Strasbourg • IHU Strasbourg</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Strasbourg, France</div></div>
    <div class="meta-row"><div class="meta-label">Focus</div><div>Foundation models for surgery; large-scale surgical video analysis; semantic graphs; self-supervised &amp; multi-modal learning.</div></div>
    <div class="meta-row"><div class="meta-label">Funding</div><div>ERC Consolidator Project <i>CompSURG</i> &amp; ENACT AI Chair.</div></div>
    <div class="meta-row"><div class="meta-label">Requirements</div>
      <div>
        <ul class="notes-list">
          <li>Master’s in Computer Science (or equivalent)</li>
          <li>Strong Python; solid CV/ML background</li>
          <li>English proficiency (oral &amp; written)</li>
          <li>Plus: DL/NLP, multi-modal video+text experience</li>
        </ul>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">How to Apply</div>
      <div>Send CV, motivation letter, and transcripts to <a href="mailto:npadoy@unistra.fr">Nicolas Padoy</a>.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Links</div>
      <div>
        <a href="http://camma.u-strasbg.fr/opportunities">Position details / Opportunities</a> •
        <a href="https://camma.unistra.fr/">CAMMA Group</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>Projects with international clinical collaborators; goal: improve surgical safety via new video modeling methods.</div>
    </div>
  </div>
</div>




<!-- PhD — ESI / Max Planck (Rademaker Lab) -->
<div class="card md-6">
  <div class="title"><b>PhD Position — Perception &amp; Cognitive Computational Neuroscience</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>Ernst Strüngmann Institute (ESI), Frankfurt — Max Planck research group of Dr. Rosanne Rademaker</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Frankfurt, Germany</div></div>
    <div class="meta-row"><div class="meta-label">Category</div><div>PhD (Fully funded)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>First review: October 20, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>January 2026</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.rademakerlab.org/" rel="noopener">rademakerlab.org</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Application</div><div>Email <a href="mailto:rademakerlab@gmail.com">rademakerlab@gmail.com</a> with a 1-page (double-spaced) research statement, recent CV incl. publications/competencies/GPA+scale, and 2 referees.</div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Topics span perception, working memory, spatial representations, cognition &amp; action.</li>
          <li>Please avoid AI-generated text; such submissions won’t be evaluated per the call.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Postdoc — Dartmouth College (Störmer Lab) -->
<div class="card md-6">
  <div class="title"><b>Postdoctoral Position — Cross-Modal Perception &amp; Attention (Störmer Lab, Dartmouth)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>Dartmouth College — Department of Psychological &amp; Brain Sciences</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Hanover, New Hampshire, USA</div></div>
    <div class="meta-row"><div class="meta-label">Start Date</div><div>January 2026 (flexible)</div></div>
    <div class="meta-row"><div class="meta-label">Funding</div><div>Fully funded through NIH grant</div></div>
    <div class="meta-row"><div class="meta-label">Summary</div>
  <div>
    Research on how higher-level cognition shapes perception and how neural architecture constrains cognitive processes,
    with emphases on <b>cross-modal perception, attention, and working memory</b>. Methods include psychophysics, EEG,
    fMRI, and computational modeling.
  </div>
</div>
    <div class="meta-row"><div class="meta-label">Requirements</div>
      <div>
        Strong background in experimental and computational approaches to human cognition; programming, data analysis, and modeling skills required.  
        EEG or fMRI experience preferred but not essential.
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Apply</div>
      <div>
        Email a cover statement, CV, and 2–3 references to  
        <a href="mailto:viola.s.stoermer@dartmouth.edu">viola.s.stoermer@dartmouth.edu</a>.
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://sites.dartmouth.edu/stoermerlab/join-the-lab/">https://sites.dartmouth.edu/stoermerlab/join-the-lab/</a></div>
    </div>
  </div>
</div>


<!-- PhD Studentship — Cardiff University (GW4 BioMed2 MRC DTP) -->
<div class="card md-6">
  <div class="title"><b>PhD Studentship — Area-Modulation Stimuli for Visual Field Sensitivity in AMD</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>Cardiff University — School of Optometry &amp; Vision Sciences</div></div>
    <div class="meta-row"><div class="meta-label">Category</div><div>PhD Studentship (4 years, funded)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 20, 2025 &nbsp;@&nbsp; 17:00 UK (BST)</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>October 1, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Cardiff, Wales, UK (collaborations: Bristol, Ulster, UCL, Moorfields)</div></div>
    <div class="meta-row"><div class="meta-label">Supervisors</div><div>Dr Tony Redmond (lead), Dr Kristian Skoczek, Dr Denize Atan (Bristol), Dr Pádraig Mulholland (Ulster)</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://gw4biomed.ac.uk/novel-area-modulation-stimuli-for-identifying-changes-in-visual-field-sensitivity-in-age-related-macular-degeneration-amd/" rel="noopener">Project page (GW4 BioMed2)</a><br>
        <a href="https://gw4biomed.ac.uk/how-to-apply/student-applications/how-to-apply/" rel="noopener">How to apply (DTP instructions)</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:RedmondT1@cardiff.ac.uk">RedmondT1@cardiff.ac.uk</a></div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Theme: Neuroscience &amp; Mental Health; Project code: <code>MRCNMH26Ca Redmond</code>.</li>
          <li>Methods: psychophysics, OCT, perimetry; aligned with the REVAMP study.</li>
          <li>Funding: UK/Home tuition + UKRI-rate stipend; RTSG (£2–5k/yr) and ~£300/yr travel; open to UK &amp; International candidates.</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<!-- Administrative — NECO Senior Research Grants & Contracts Manager -->
<div class="card md-6">
  <div class="title"><b>Senior Research Grants &amp; Contracts Manager</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>New England College of Optometry (NECO)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Boston, MA, USA</div></div>
    <div class="meta-row"><div class="meta-label">Category</div><div>Administrative / Staff</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Open until filled</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://recruiting.paylocity.com/Recruiting/Jobs/Details/3562667" rel="noopener">Apply — Paylocity (Job #3562667)</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div><div>Prof. Fuensanta Vera-Diaz — <a href="mailto:vera_diazf@neco.edu">vera_diazf@neco.edu</a></div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>End-to-end research administration: pre-award, post-award, compliance, reporting (NIH/eRA Commons, ASSIST, Research.gov).</li>
          <li>Budgets, subawards, industry agreements; policy guidance and training.</li>
          <li>Req: ≥3 years higher-ed research grants admin; Pref: 5+ years; CRA a plus.</li>
          <li>Benefits include medical/dental, paid holidays, and 9% employer 403(b) contribution after 1 year.</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<div class="card md-6">
  <div class="title"><b>PhD Student — Color Consistency & Automated White Balancing</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>ams OSRAM</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Jena, Germany</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 10, 2025</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://jobs.ams-osram.com/en/job/PHD-Student-Color-Consistency-and-Automated-White-Balancing-d_m_f-temporary-for-3-years-Jena?id=21525">Job posting</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div><div>Fixed-term (3 years).</div></div>
  </div>
</div>

<!-- Internship (PhD) — Amazon -->
<div class="card md-6">
  <div class="title"><b>2026 Applied Science Internship — Computer Vision (PhD)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>Amazon</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Multiple US locations</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 15, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.amazon.jobs/en/jobs/3050077/2026-applied-science-internship-computer-vision-united-states-phd-student-science-recruiting">Job posting</a></div></div>
  </div>
</div>

<!-- Internship — Meta -->
<div class="card md-6">
  <div class="title"><b>Research Scientist Intern — Computational Photography</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>Meta</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>AR/VR (12-month internship)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Rolling / Until filled</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.metacareers.com/jobs/1049499833732409">Job posting</a></div></div>
  </div>
</div>

<!-- Industry Research — Google -->
<div class="card md-6">
  <div class="title"><b>Research Scientist — Computational Photography/Vision + Generative AI</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>Google</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>TBD</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Rolling / Until filled</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.google.com/about/careers/applications/jobs/results/97937785272312518-research-scientist-computational-photography-generative-ai">Job posting</a></div></div>
  </div>
</div>

    
<div class="card md-6">
  <div class="title"><b>PhD Position — Fairness in Face Recognition (GOOD-BIAS!)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Location</div><div>EURECOM, Sophia Antipolis, France</div></div>
   <div class="meta-row">
  <div class="meta-label">Website</div>
  <div><a href="https://www.eurecom.fr/en/job/holistic-and-generalizable-solutions-bias-mitigation-face-recognition">https://www.eurecom.fr/en/job/holistic-and-generalizable-solutions-bias-mitigation-face-recognition</a></div>
</div>
    <div class="meta-row"><div class="meta-label">Notes</div><div>
      <ul class="notes-list">
        <li>Focus: bias and fairness in biometrics</li>
      </ul>
    </div></div>
  </div>
</div>

<div class="card md-6">
  <div class="title"><b>PhD Position — Sensory Perception in Autism</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Location</div><div>Technische Universität Dresden, Germany</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://www.verw.tu-dresden.de/StellAus/stelle.asp?id=12400&amp;lang=en">
          https://www.verw.tu-dresden.de/StellAus/stelle.asp?id=12400&amp;lang=en
        </a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div><div>
      <ul class="notes-list">
        <li>3-year funded position</li>
        <li>Research: visual thalamus alterations in autism</li>
      </ul>
    </div></div>
  </div>
</div>


<div class="card md-6">
  <div class="title"><b>Assistant/Associate Professor — Computational Psychology / Cognitive Science</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>Tufts University — Department of Psychology</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Medford, MA (USA)</div></div>
    <div class="meta-row"><div class="meta-label">Category</div><div>Tenure-track (Assistant or Associate Professor)</div></div>
    <div class="meta-row"><div class="meta-label">Review begins</div><div><b>October 1, 2025</b></div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://apply.interfolio.com/172807">Job posting (Interfolio)</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>AI-framed scope; broad computational modeling fits.</li>
          <li>Contact: <a href="mailto:Stephanie.Badde@tufts.edu">Stephanie Badde</a></li>
        </ul>
      </div>
    </div>
  </div>
</div>


<div class="card md-6">
      <div class="title"><b>Research Assistants – Visual Plasticity (2 positions)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>Start Jan 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Child Vision Lab, University College London (UCL), London, UK</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.ucl.ac.uk/child-vision-lab">https://www.ucl.ac.uk/child-vision-lab</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Two RA posts available</li>
              <li>Application deadline: October 16, 2025</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>


  <h4>Postdoctoral Career Opportunities</h4>
  <div class="cards">

<!-- Postdoc — InnoHK Centre for Eye & Vision Research (Hong Kong) -->
<div class="card md-6"
  data-tags="job postdoc vision-science neuromodulation brain-stimulation perceptual-learning eye-tracking eeg hong-kong cevr neuroplasticity rehabilitation">

  <div class="title"><b>Postdoctoral Positions — InnoHK Centre for Eye & Vision Research (Hong Kong)</b></div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Project</div>
      <div>
        Novel neuromodulation approaches for vision enhancement using non-invasive brain stimulation 
        and perceptual learning.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Research Methods</div>
      <div>
        Psychophysics · Eye tracking · EEG · Electrophysiology · Non-invasive brain stimulation (TMS/tES).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Qualifications</div>
      <div>
        Doctoral degree in vision science, optometry, psychology, neuroscience, or related fields; 
        programming skills (Python/Matlab); strong analytical &amp; software skills; team-focused.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Desirable</div>
      <div>
        Experience with brain stimulation, EEG, eye-tracking, psychophysics; optometry or clinical background.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Salary</div>
      <div>
        HKD 35,000/month + HKD 10,000 living allowance.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Apply</div>
      <div>
        Email CV + cover letter + completed application form to 
        <a href="mailto:career@cevr.hk">career@cevr.hk</a> (quote “RP 1.5S Postdoc Position”).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">About</div>
      <div>
        CEVR is a collaboration between Hong Kong Polytechnic University and University of Waterloo, 
        based at Hong Kong Science Park.
      </div>
    </div>

  </div>
</div>


<!-- Postdoc — Visual & Cognitive Neuroscience Lab, University of Fribourg -->
<div class="card md-6"
  data-tags="job postdoc neuroscience vision cognition blindness attention fMRI eeg eye-tracking switzerland auditory spatial-attention rehabilitation">
  
  <div class="title"><b>Post-doctoral Position — Visual & Cognitive Neuroscience Lab (University of Fribourg, Switzerland)</b></div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Project</div>
      <div>
        “Auditory spatial attention and eye-movement guidance in blindness and its use for sight rehabilitation.”
        Collaboration with Georgetown University (Ella Striem-Amit) and Lausanne University Hospital (Marzia de Lucia).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Research Methods</div>
      <div>
        Behavioral experiments · fMRI &amp; advanced fMRI analysis · EEG/EOG · eye-tracking · sensory-aid translation.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Qualifications</div>
      <div>
        PhD in Cognitive Neuroscience or related field; strong programming skills (Python/R/Matlab);
        experience with fMRI design/analysis; strong analytical/statistical skills; publication record;
        ability to work independently &amp; supervise students.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Desired</div>
      <div>
        Advanced fMRI (pRF, MVPA, RSA, retinotopy, ML approaches); EEG/EOG experience; eye-tracking expertise.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>
        Department of Psychology, University of Fribourg, Switzerland.  
        Optional research stays at Georgetown University (Washington DC).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Duration</div>
      <div>
        Funded for ≥2 years (possible extension to 3 years).  
        Start date: Flexible in 2026.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Deadline</div>
      <div><b>January 18, 2026</b> (late applications considered if no suitable candidate found)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Apply</div>
      <div>
        Email CV, motivation letter, and 2 referee contacts to  
        <a href="mailto:petra.vetter@unifr.ch">petra.vetter@unifr.ch</a>.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Lab Website</div>
      <div><a href="https://www.unifr.ch/psycho/en/research/vcnl/">unifr.ch/psycho/en/research/vcnl</a></div>
    </div>

  </div>
</div>


<!-- JOBS/GRAD: Denison Lab — Boston University -->
<div class="card md-6" id="denisonlab-bu-2025" data-tags="jobs postdoc phd perception attention decision-making neuroimaging bu boston-university">
  <div class="title"><b>Postdoc & PhD Positions — Denison Lab (Boston University)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">PI</div><div>Rachel Denison, Assistant Professor of Psychological & Brain Sciences</div></div>
    <div class="meta-row"><div class="meta-label">Focus</div><div>Visual perception, attention, decision-making, and temporal dynamics via neuroimaging (fMRI/EEG/MEG), computational modeling, and behavioral methods.</div></div>
    <div class="meta-row"><div class="meta-label">Postdoc Application</div>
      <div>Email CV + cover letter to <a href="mailto:rdenison@bu.edu">rdenison@bu.edu</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">PhD Programs</div>
      <div><a href="https://www.bu.edu/psych/academics/phd/bbc/">Brain, Behavior & Cognition PhD</a> • <a href="https://www.bu.edu/neuro/academics/graduate/">Graduate Program in Neuroscience</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Lab Website</div><div><a href="https://sites.bu.edu/denisonlab/">sites.bu.edu/denisonlab</a></div></div>
  </div>
</div>


<!-- Postdoc: University of York × Google DeepMind — ML & Computer Vision -->
<div class="card md-6" data-tags="jobs postdoc computer-vision machine-learning deepmind york uk developmental-psychology">
  <div class="title"><b>Postdoctoral Research Associate — Machine Learning &amp; Computer Vision</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>University of York (Psychology) in collaboration with Google DeepMind</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>York, United Kingdom</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div><b>November&nbsp;11, 2025</b></div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>January 2026 (or as soon as possible)</div></div>
    <div class="meta-row"><div class="meta-label">Areas</div>
      <div>Developmental curriculum learning; perception models; computer vision (image/video); ML frameworks (PyTorch/JAX); interdisciplinary ML&nbsp;&times;&nbsp;psychology</div>
    </div>
    <div class="meta-row"><div class="meta-label">Funding</div><div>12-month fully funded post; conference travel support; substantial Google Compute Credits</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://jobs.york.ac.uk/vacancy/research-associate-in-machine-learning-and-computer-vision-597358.html">Application details</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div>
      <div><a href="mailto:viorica@google.com">viorica@google.com</a> • <a href="mailto:elena.geangu@york.ac.uk">elena.geangu@york.ac.uk</a></div>
    </div>
  </div>
</div>



<!-- Postdoc: Smith-Kettlewell Eye Research Institute (San Francisco) -->
<div class="card md-6" data-tags="jobs postdoc vision-research low-vision neuroscience sanfrancisco ski">
  <div class="title"><b>Postdoctoral Fellowships — Smith-Kettlewell Eye Research Institute (San&nbsp;Francisco)</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Organization</div>
      <div>Smith-Kettlewell Eye Research Institute</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>San&nbsp;Francisco, California, USA</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Fellowships (2026)</div>
      <div>
        <ul class="notes-list">
          <li>
            <b>NEI Training Grant Fellowship:</b> 1-year appointment (with possible renewal) for
            U.S. citizens or permanent residents. Apply by <b>December&nbsp;10,&nbsp;2025</b>;
            start before <b>March&nbsp;31,&nbsp;2026</b>.
          </li>
          <li>
            <b>Smith-Kettlewell Funded Fellowship:</b> 2-year appointment open to fellows
            of all nationalities. Apply by <b>January&nbsp;30,&nbsp;2026</b>;
            start in <b>summer or fall 2026</b>.
          </li>
        </ul>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Eligibility</div>
      <div>Ph.D., O.D., or M.D. required.</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Research Areas</div>
      <div>
        Visual development • Strabismus &amp; amblyopia • Eye movements • Visual attention •
        Brain plasticity • Mobility • Low vision / blindness rehabilitation • Accessibility
        technologies.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Methods</div>
      <div>
        Psychophysics • Eye &amp; head tracking • Clinical assessment • Computational modeling •
        EEG • fMRI • Computer vision &amp; AI • Sensor technologies.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Program</div>
      <div>
        Fellows can propose independent projects, work with one or more mentors, and
        apply for mentored or independent grants, with potential to move onto the PI track.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://ski.org/what-does-fellowship-smith-kettlewell-offer">What does a fellowship offer?</a> •
        <a href="https://ski.org/how-apply">How to apply</a> •
        <a href="https://ski.org/current-mentors">Current mentors</a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div>
        <a href="mailto:fellowships@ski.org">fellowships@ski.org</a> •
        <a href="mailto:preeti@ski.org">Preeti&nbsp;Verghese</a>
      </div>
    </div>

  </div>
</div>


<!-- Postdoc: InnoHK Centre for Eye and Vision Research (Hong Kong) -->
<div class="card md-6" data-tags="jobs postdoc neuroscience vision cevr hongkong brain-stimulation">
  <div class="title"><b>Postdoctoral Fellowships — InnoHK Centre for Eye &amp; Vision Research (CEVR)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>InnoHK Centre for Eye &amp; Vision Research (CEVR)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Hong Kong Science Park, Sha Tin, Hong Kong</div></div>
    <div class="meta-row"><div class="meta-label">Supervisors</div><div>Dr. Ken&nbsp;Tan &amp; Prof.&nbsp;Benjamin&nbsp;Thompson</div></div>
    <div class="meta-row"><div class="meta-label">Duration</div><div>Up to 3&nbsp;years</div></div>
    <div class="meta-row"><div class="meta-label">Salary</div><div>HKD&nbsp;35,000&nbsp;/month&nbsp;+&nbsp;HKD&nbsp;10,000&nbsp;living allowance</div></div>
    <div class="meta-row"><div class="meta-label">Research</div>
      <div>
        Novel neuromodulation approaches for vision enhancement using non-invasive brain stimulation and perceptual learning; psychophysical, eye-tracking, and electrophysiological techniques.
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Prerequisites</div>
      <div>
        PhD in vision science, optometry, psychology, or neuroscience; programming experience (Python/MATLAB); strong analytical and teamwork skills.
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Preferred Experience</div>
      <div>
        Non-invasive brain stimulation (TMS/tES), EEG, eye-tracking, psychophysics, or clinical optometry background.
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Application</div>
      <div>
        Send completed <a href="http://cevr.hk/wp-content/uploads/2023/10/Job-application-form-CEVR_202310.docx">application form</a>, CV, and cover letter to 
        <a href="mailto:career@cevr.hk">career@cevr.hk</a> (quote “RP&nbsp;1.5S Postdoc&nbsp;Position”).
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Preference for candidates experienced in brain stimulation or research with elderly/visually impaired populations.</li>
          <li>Joint initiative of Hong&nbsp;Kong&nbsp;PolyU and University&nbsp;of&nbsp;Waterloo under the InnoHK program.</li>
          <li>Focus areas: myopia, ocular drug delivery, vision enhancement, tear film &amp; ocular surface, and optometric technology.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Postdoc: NIMH – Neural Circuits of Vision & Recovery After V1 Injury -->
<div class="card md-6" data-tags="jobs postdoc vision neuroscience circuits marmoset neuroanatomy imaging injury recovery NIH NIMH USA bethesda 2026-start">
  <div class="title"><b>Postdoctoral Fellow — Neural Circuits of Vision & Recovery After V1 Injury (NIMH/NIH)</b></div>

  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Organization</div>
      <div>Section on Cellular & Cognitive Neurodevelopment (SCCN), NIMH/NIH</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Bethesda, Maryland, USA</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Research Focus</div>
      <div>
        Neural circuitry underlying visual perception and recovery after primary visual cortex (V1) damage;
        pulvinar & LGN contributions to visual processing; translational marmoset model.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Techniques</div>
      <div>
        Viral circuit tracing, chemogenetics (DREADDs), MRI-guided stereotaxic surgery, DTI,
        calcium imaging (GCaMP/miniscope), behavioral & eye-tracking assays, histology, immunohistochemistry.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Start Date</div>
      <div>Flexible; early 2026 preferred</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Ideal Candidate</div>
      <div>
        Ph.D. in neuroscience or related field; experience in systems/visual neuroscience,
        behavioral analysis, neuroanatomy, in-vivo imaging, viral vector work, or surgical methods.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div>
        Dr. James Bourne — <a href="mailto:james.bourne@nih.gov">james.bourne@nih.gov</a>
      </div>
    </div>
  </div>
</div>


<!-- PhD: Colour Vision Deficiencies & Universal Design — UCM Madrid -->
<div class="card md-6" data-tags="jobs phd colour-vision color-vision deficiencies madrid ucm psychology">
  <div class="title"><b>4-Year Funded PhD — Colour Vision Deficiencies &amp; Universal Design (UCM, Madrid)</b></div>
  <div class="meta">
    <div class="meta-row">
      <div class="meta-label">Organization</div>
      <div>Research Group “Color blindness and universal design”, Faculty of Psychology, Universidad Complutense de Madrid (UCM)</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Madrid, Spain</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Project</div>
      <div>“Colour Vision Deficiencies: Diagnosis, Compensation Mechanisms and Perceptual Learning in children and adults” (PID2024-155495NA-I00; PI: Leticia Álvaro)</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Duration</div>
      <div>4-year fully funded PhD position (final year may convert into a postdoctoral contract if the thesis is completed within 3 years)</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Overview</div>
      <div>
        Investigates colour vision deficiency (CVD) across the lifespan, combining molecular genetics and perceptual training to improve screening, diagnosis, and compensation strategies in children and adults.
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Eligibility</div>
      <div>Bachelor’s and Master’s degree in Psychology or a related field; good English skills.</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Application</div>
      <div>
        Apply via the UCM online platform:
        <a href="https://www.ucm.es/ct65-25">ucm.es/ct65-25</a>
        (national call “Ayudas para contratos predoctorales para la formación de doctores 2025”). Applicants must hold a valid electronic certificate.
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Research Group</div>
      <div>
        <a href="https://produccioncientifica.ucm.es/grupos/5410/detalle">Color blindness and universal design</a>
      </div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Deadline</div>
      <div>November 24, 2025</div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div><a href="mailto:lalvaro@ucm.es">Dr. Leticia Álvaro</a></div>
    </div>
  </div>
</div>

<!-- Graduate Position: Visual Perception, Medical Imaging & AI — University of Arizona -->
<div class="card md-6"
  data-tags="phd graduate funded vision-perception visual-search medical-imaging ai radiology psychology arizona usa eye-tracking eeg">
  <div class="title">
    <b>Graduate Student Position — Visual Perception, Medical Imaging & AI (University of Arizona)</b>
  </div>

  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Lab</div>
      <div>ADAMO Lab (Attention Detection and Medical Observation)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Departments</div>
      <div>Psychology; Radiology & Imaging Sciences</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Tucson, Arizona, USA</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Research Areas</div>
      <div>
        Visual search and attention; medical image perception; breast cancer detection in simulated radiology tasks;
        applied human factors; AI-assisted perception; behavioral, eye-tracking, and EEG experiments.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Responsibilities</div>
      <div>
        Lead and support grant projects; conduct psychophysics, EEG, and eye-tracking studies; manage data collection
        (domestic & international); prepare manuscripts; contribute to grants; present at meetings.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Qualifications</div>
      <div>
        Bachelor’s in Psychology, Neuroscience, Cognitive Science, Computer Science, Biomedical Engineering, or related fields;
        experience with programming (Python, MATLAB, R, etc.); behavioral research experience; EEG/eye-tracking preferred;
        AI/ML skills preferred.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Funding</div>
      <div>Full-time and fully funded position</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Apply</div>
      <div>
        <a href="https://psychology.arizona.edu/graduate/how-apply" target="_blank" rel="noopener">
          psychology.arizona.edu/graduate/how-apply
        </a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div>
        Dr. Stephen Adamo — <a href="mailto:rachelmorris@arizona.edu">rachelmorris@arizona.edu</a>
      </div>
    </div>

  </div>
</div>



<!-- PhD: Computational Cognitive Neuroscience — Hebart Lab (Giessen) -->
<div class="card md-6" id="phd-hebart-giessen" data-tags="jobs phd cognitive-neuroscience computational modeling decision-making vision">
  <div class="title"><b>PhD — Computational Cognitive Neuroscience (Hebart Lab, University of Giessen)</b></div>
  <div class="meta">

    <div class="meta-row"><div class="meta-label">Lab</div>
      <div>Dept. of General Psychology, University of Giessen (Germany) — Prof. Michael N. Hebart</div>
    </div>

    <div class="meta-row"><div class="meta-label">Research Areas</div>
      <div>
        Computational models of perceptual decision-making, probabilistic inference, perceptual organization, and human behavior in complex environments.
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Position</div>
      <div>Fully funded PhD — 3-year term with extension possible</div>
    </div>

    <div class="meta-row"><div class="meta-label">Requirements</div>
      <div>
        Master’s in psychology, neuroscience, computer science, cognitive science, or related field.  
        Strong background in quantitative methods, modeling, machine learning, or computational approaches desirable.
      </div>
    </div>

    <div class="meta-row"><div class="meta-label">Application</div>
      <div>Send application materials to <a href="mailto:apply@cog-giessen.de">apply@cog-giessen.de</a> (details per digest announcement).</div>
    </div>

    <div class="meta-row"><div class="meta-label">Contact</div>
      <div><a href="mailto:Michael.Hebart@psychol.uni-giessen.de">Michael.Hebart@psychol.uni-giessen.de</a></div>
    </div>

  </div>
</div>

<!-- PhD Positions: ML/AI — COSMOS Center, UT Arlington -->
<div class="card md-6" id="phd-uta-cosmos-2026" data-tags="jobs phd ml ai deep-learning data-science texas 2026">
  <div class="title"><b>PhD Positions in ML/AI — COSMOS Center, University of Texas at Arlington (Spring &amp; Fall 2026)</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Center</div>
      <div>Center on Stochastic Modeling, Optimization, &amp; Statistics (COSMOS), University of Texas at Arlington (UTA)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Arlington, Texas, USA (Dallas–Fort Worth metro area)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Start</div>
      <div>PhD positions available for Spring 2026 (for students already in the US) and Fall 2026 intakes.</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Research Areas</div>
      <div>
        Deep learning • Data analytics • Artificial intelligence • Intelligent systems •
        Stochastic modeling &amp; optimization.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Topics</div>
      <div>
        Deep learning &amp; decision analytics for healthcare and biomedical imaging (cancer/brain imaging, precision medicine) •
        Generative AI for computational life science &amp; drug discovery (protein/DNA/RNA, molecular design) •
        Multivariate time series &amp; sequential data modeling (energy, healthcare, agriculture, finance) •
        Interpretable &amp; probabilistic deep learning (uncertainty, robustness) •
        AI-driven smart systems for agriculture •
        AI engineering &amp; intelligent agent systems (LLMs, autonomous AI agents).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Support</div>
      <div>
        Full financial support for PhD students (tuition, stipend, and benefits).  
        Visiting student/scholar positions also available in ML/AI and intelligent systems.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Eligibility</div>
      <div>
        Strong quantitative background, solid programming skills, and experience in machine learning / deep learning.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Application</div>
      <div>
        Email CV, transcripts, and any materials highlighting experience (papers, thesis, awards, certificates, etc.) to  
        <a href="mailto:shouyiw@uta.edu">Dr. Shouyi&nbsp;Wang (shouyiw@uta.edu)</a>.  
        Applications are reviewed on a rolling basis with prompt feedback.
      </div>
    </div>

  </div>
</div>



<!-- PhD Positions: Scene Grammar Lab (LMU Munich) -->
<div class="card md-6" id="phd-scene-grammar-lmu" data-tags="jobs phd scene-grammar visual-attention real-world-vision vr eye-tracking eeg computational-modeling germany">
  <div class="title"><b>Two 4-Year PhD Positions — Scene Grammar Lab (LMU Munich)</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Lab</div>
      <div>Scene Grammar Lab, Chair of Neuro-Cognitive Psychology (Prof. Melissa Lê-Hoa Võ), LMU Munich</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Location</div>
      <div>Munich, Germany</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Positions</div>
      <div>
        <b>PhD #1:</b> 4-year funded position within the Cluster of Excellence <i>The Adaptive Mind (TAM)</i>, focusing on real-world search using VR eye-tracking and (mobile) EEG.<br>
        <b>PhD #2:</b> University-funded 4+2 year position (with ~4 hr/week teaching), flexible research agenda, long-term perspective.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Research Areas</div>
      <div>
        Scene perception • Scene grammar • Object memory • Visual attention • Real-world & VR environments • Eye-tracking • EEG • Computational modeling.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Methods</div>
      <div>Psychophysics • Real-world & VR eye-tracking • EEG (including mobile EEG) • Computational modeling • Machine learning approaches.</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Eligibility</div>
      <div>
        MSc in Psychology, Neuroscience, Computer Vision, Cognitive Science, or related fields.  
        Experience with eye-tracking and/or EEG strongly preferred.  
        Programming experience (Python, Unity) and solid statistical/methodological skills beneficial.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Application</div>
      <div>
        Send CV, statement of research interests, and contact information for two academic references to  
        <a href="mailto:melissa.vo@psy.lmu.de">melissa.vo@psy.lmu.de</a>.  
        Preferred deadline: <b>before Nov 30, 2025</b> (later applications considered).
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.scenegrammarlab.com/">scenegrammarlab.com</a></div>
    </div>

  </div>
</div>


<!-- Postdoc & PhD Positions: Denison Lab (Boston University) -->
<div class="card md-6" id="denison-bu" data-tags="jobs postdoc phd perception attention neuroimaging decision-making boston">
  <div class="title"><b>Postdoc & PhD Positions — Denison Lab (Boston University)</b></div>
  <div class="meta">

    <div class="meta-row">
      <div class="meta-label">Lab</div>
      <div>Denison Lab — Psychological & Brain Sciences, Boston University (PI: Dr. Rachel Denison)</div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Research Areas</div>
      <div>
        Visual perception • Attention • Decision-making • Temporal dynamics of vision  
        Neuroimaging (fMRI, EEG, MEG) • Behavioral & computational modeling approaches.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Positions</div>
      <div>
        <b>Postdoctoral Researcher:</b> Experience with human neuroimaging required (fMRI/EEG/MEG).  
        <br>
        <b>PhD Students:</b> Applications via BU's BBC Program and Graduate Program in Neuroscience.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Requirements</div>
      <div>
        Strong quantitative skills and interests.  
        PhD applicants: Minimum 2 years relevant research experience.
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Application</div>
      <div>
        <b>Postdoc:</b> Email CV, cover letter, and 2 reference contacts to  
        <a href="mailto:rdenison@bu.edu">rdenison@bu.edu</a>.  
        <br>
        <b>PhD:</b> Apply through BU programs:<br>
        <a href="https://www.bu.edu/psych/academics/phd/bbc/">Brain, Behavior & Cognition PhD</a> •  
        <a href="https://www.bu.edu/neuro/academics/graduate/">Graduate Program in Neuroscience</a>
      </div>
    </div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://sites.bu.edu/denisonlab/">Denison Lab</a></div>
    </div>

  </div>
</div>



<!-- Postdoc — ESI / Max Planck (Rademaker Lab) -->
<div class="card md-6">
  <div class="title"><b>Postdoctoral Researcher — Perception &amp; Cognitive Computational Neuroscience</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>Ernst Strüngmann Institute (ESI), Frankfurt — Max Planck research group of Dr. Rosanne Rademaker</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Frankfurt, Germany</div></div>
    <div class="meta-row"><div class="meta-label">Category</div><div>Postdoctoral (Fully funded)</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>2026 (flexible)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Rolling / Until filled</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.rademakerlab.org/" rel="noopener">rademakerlab.org</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:rademakerlab@gmail.com">rademakerlab@gmail.com</a></div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Focus: interaction of sensation, working memory, spatial representations, cognition &amp; action.</li>
          <li>Resources: research-dedicated high-field fMRI, MEG, and strong computational infrastructure.</li>
          <li>Computational methods experience expected; neuroimaging (EEG/fMRI) a plus; inclusive lab culture.</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<!-- Postdoc — OUHSC -->
<div class="card md-6">
  <div class="title"><b>Postdoctoral Fellow — Vision Research</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>University of Oklahoma Health Sciences Center</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Oklahoma City, OK, USA</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 1, 2025 <span aria-label="closing soon">⚠</span></div></div>
    <div class="meta-row"><div class="meta-label">PI / Contact</div><div>Michael A. Elliott, PhD</div></div>
    <div class="meta-row"><div class="meta-label">Focus</div><div>Microglia/macrophages in RPE degeneration</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://graduate.ouhsc.edu/Postdoctoral-Fellows/Available-Positions">Position page</a></div></div>
  </div>
</div>

<!-- Postdoc — UAB -->
<div class="card md-6">
  <div class="title"><b>Postdoctoral Position — Optometry & Vision Science (Myopia)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>University of Alabama at Birmingham (Khanal Lab)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Birmingham, AL, USA</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 5, 2025 <span aria-label="closing soon">⚠</span></div></div>
    <div class="meta-row"><div class="meta-label">PI / Contact</div><div>Sangeeta Khanal, PhD</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.uab.edu/postdocs/prospective-postdocs/available-positions/g242201">Position page</a></div></div>
  </div>
</div>

<!-- Postdoc — KAUST -->
<div class="card md-6">
  <div class="title"><b>Postdoctoral Fellowship — Computer Vision / AI</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>KAUST</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Saudi Arabia</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 11, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.kaust.edu.sa/about/postdoc-researcher-positions">Postdoc positions</a></div></div>
  </div>
</div>

<!-- Postdoc/RA — University of Cambridge (Adaptive Brain Lab) -->
<div class="card md-6">
  <div class="title"><b>Research Assistant/Associate — Cognitive &amp; Computational Neuroscience (Fixed Term)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>University of Cambridge — Adaptive Brain Lab, Department of Psychology</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Cambridge, UK</div></div>
    <div class="meta-row"><div class="meta-label">Category</div><div>Postdoctoral (Research Associate) / Research Assistant — Fixed Term (18 months)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 20, 2025</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://www.cam.ac.uk/jobs/research-assistantassociate-fixed-term-pj47416" rel="noopener">cam.ac.uk/jobs — PJ47416</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">PI / Contact</div><div>Prof. Zoe Kourtzi — <a href="mailto:zk240@cam.ac.uk">zk240@cam.ac.uk</a></div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Focus: neuroimaging &amp; neurocomputational studies of learning and brain plasticity.</li>
          <li>Methods: 7T fMRI / MRS, EEG, TMS/tDCS, ML/RL modelling.</li>
          <li>Reference: <code>PJ47416</code>; initial term 18 months.</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<!-- Postdoc — University of Leeds -->
<div class="card md-6">
  <div class="title"><b>Post-doctoral Research Fellow — Computer Vision for AR in Liver Resection</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>University of Leeds — School of Computer Science (with Leeds Teaching Hospitals NHS Trust)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Leeds, UK</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>October 26, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Dates</div><div>Start January 1, 2026</div></div>
    <div class="meta-row"><div class="meta-label">PI / Contact</div><div>Dr. Sharib Ali — <a href="mailto:s.s.ali@leeds.ac.uk">s.s.ali@leeds.ac.uk</a></div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://jobs.leeds.ac.uk/Vacancy.aspx?ref=EPSCP1176">jobs.leeds.ac.uk/Vacancy.aspx?ref=EPSCP1176</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Focus: segmentation, 2D/3D registration, tracking, and visualisation for real-time surgical planning &amp; decision support</li>
          <li>Close collaboration with clinical teams; experimental setups include robotic arm + sensors on phantom models</li>
          <li>EPSRC-funded, 36 months</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Postdoc — UT Austin -->
<div class="card md-6">
  <div class="title"><b>Postdoctoral Fellow — Computer Vision (Center for Generative AI)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>UT Austin — ML Lab</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Austin, TX, USA</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Rolling / Until filled</div></div>
    <div class="meta-row"><div class="meta-label">Contact</div><div>Adam Klivans, PhD (Lab Director)</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://ml.utexas.edu/postdoctoral-positions">Postdoc positions</a></div></div>
  </div>
</div>

<!-- Postdoc/PhD — MPI Tübingen -->
<div class="card md-6">
  <div class="title"><b>Postdoc & PhD Positions — Human Psychophysics</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>Max Planck Institute for Biological Cybernetics</div></div>
    <div class="meta-row"><div class="meta-label">Department</div><div>Sensory & Sensorimotor Systems</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Tübingen, Germany</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Rolling / Until filled</div></div>
    <div class="meta-row"><div class="meta-label">PI</div><div>Li Zhaoping, PhD</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.kyb.mpg.de/328993/vacancies">Vacancies</a></div></div>
  </div>
</div>

    
<div class="card md-6">
      <div class="title"><b>Postdoc – Grounded Gesture Generation in Context</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>Start Feb 1, 2026 (negotiable)</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Max Planck Institute for Psycholinguistics, Nijmegen, NL</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.mpi.nl/career-positions">https://www.mpi.nl/career-positions</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>NWO-funded project “Grounded Gesture Generation in Context”</li>
              <li>Application deadline: October 31, 2025</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>Postdoctoral Fellow – Neuroimaging of Visual System</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>TBD</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Stanford University, Spencer Center for Vision Research</div></div>
        <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://www.nivs-lab.org/open-positions">Open positions</a></div></div>
        <div class="meta-row"><div class="meta-label">Notes</div><div><ul class="notes-list"><li>Contact: kevinchan@stanford.edu</li></ul></div></div>
      </div>
    </div>
   </div>

  <h4>Faculty Positions</h4>
  <div class="cards">

<div class="card md-6" id="bilkent-psychology-faculty-2026" data-tags="job faculty psychology neuroscience cognitive developmental social clinical turkey bilkent">
  <div class="title"><b>Tenure-Track Faculty Positions — Department of Psychology (Bilkent University)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Institution</div><div>Bilkent University, Department of Psychology</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Ankara, Turkey</div></div>
    <div class="meta-row"><div class="meta-label">Start Date</div><div>September 2026 (negotiable)</div></div>
    <div class="meta-row"><div class="meta-label">Rank</div><div>Open rank (Assistant, Associate, or Full Professor)</div></div>
    <div class="meta-row"><div class="meta-label">Areas</div><div>All areas of psychology, including social, developmental, cognitive, and clinical psychology</div></div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>Bilkent invites applications for multiple tenure-track positions as part of a departmental expansion. Successful applicants will maintain active research programs, supervise graduate students, and teach two courses per semester (no summer teaching).</div>
    </div>
    <div class="meta-row"><div class="meta-label">Facilities</div>
      <div>Newly renovated labs; access to 3 T MRI, animal research center, EEG suites, and interdisciplinary Neuroscience Program resources.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Compensation</div>
      <div>Competitive salary, free on-campus housing, private health insurance, and International Baccalaureate schooling for dependents.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Application Materials</div>
      <div>
        <ul class="notes-list">
          <li>Cover letter</li>
          <li>Curriculum Vitae</li>
          <li>Teaching and Research Statements</li>
          <li>PDFs of three representative publications</li>
          <li>Contact information for three professional references</li>
        </ul>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>January 15 2026 (rolling review until filled)</div></div>
    <div class="meta-row"><div class="meta-label">Application Portal</div><div><a href="https://stars.bilkent.edu.tr/staffapp/PSYC2026OR">stars.bilkent.edu.tr/staffapp/PSYC2026OR</a></div></div>
    <div class="meta-row"><div class="meta-label">Department Info</div>
      <div><a href="https://psy.bilkent.edu.tr/">psy.bilkent.edu.tr</a> • <a href="https://neuro.bilkent.edu.tr/">neuro.bilkent.edu.tr</a> • <a href="https://feass.bilkent.edu.tr/">feass.bilkent.edu.tr</a></div>
    </div>
  </div>
</div>


<div class="card md-6" id="usm-psychology-teaching-professor-2026" data-tags="job faculty teaching-track psychology neuroscience developmental counseling usm mississippi">
  <div class="title"><b>Assistant Teaching Professor — School of Psychology (University of Southern Mississippi)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Institution</div><div>University of Southern Mississippi (USM)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Hattiesburg, Mississippi</div></div>
    <div class="meta-row"><div class="meta-label">Start Date</div><div>August 2026</div></div>
    <div class="meta-row"><div class="meta-label">Positions</div><div>Two full-time, 9-month Assistant Teaching Professor appointments</div></div>
    <div class="meta-row"><div class="meta-label">Area</div><div>Behavioral Neuroscience and Developmental Psychology (open specialization; applied fields encouraged)</div></div>
    <div class="meta-row"><div class="meta-label">Overview</div>
      <div>Doctoral-level graduates with strong teaching ability are invited to apply for teaching-track faculty roles supporting undergraduate and graduate programs at USM’s Hattiesburg campus. Positions are permanent with eligibility for promotion.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Highlights</div>
      <div>
        <ul class="notes-list">
          <li>Opportunities for practicum supervision for license-eligible applicants</li>
          <li>Leadership potential within the Master’s Counseling Psychology program</li>
          <li>Peer supervision available during licensure process</li>
          <li>Visa sponsorship not available</li>
        </ul>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Application Details</div>
      <div>
        <ul class="notes-list">
          <li>Review begins October 1, 2025, and continues until filled</li>
          <li>Submit: cover letter, CV, 3 references, academic transcripts, and teaching evaluations (if applicable)</li>
          <li>Apply online: <a href="https://usm.csod.com/ux/ats/careersite/1/home/requisition/4902?c=usm&sq=req4902">USM Job Portal</a></li>
          <li>Contact: <a href="mailto:kristy.mcraney@usm.edu">Dr. Kristy McRaney</a> (Search Chair)</li>
        </ul>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Department Info</div>
      <div>The School of Psychology serves ~600 undergraduates and 125 graduate students across counseling, clinical, school, and experimental psychology programs. Visit <a href="http://www.usm.edu/psychology">usm.edu/psychology</a> for details.</div>
    </div>
  </div>
</div>


<div class="card md-6" id="ai-chairs-enact-strasbourg-2026" data-tags="job faculty ai chair research france enact healthcare">
  <div class="title"><b>AI Chair Positions — ENACT Cluster (University of Strasbourg)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Institution</div><div>University of Strasbourg • AI Cluster ENACT • IHU Strasbourg • ICube Laboratory</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Strasbourg, France</div></div>
    <div class="meta-row"><div class="meta-label">Program</div>
      <div>ENACT — a France 2030–funded initiative uniting AI research and education partners across Eastern France. </div>
    </div>
    <div class="meta-row"><div class="meta-label">Focus Areas</div>
      <div>Artificial Intelligence for Healthcare, in collaboration with IHU Strasbourg and the ICube Laboratory. </div>
    </div>
    <div class="meta-row"><div class="meta-label">Application Info</div>
      <div>
        <a href="https://cluster-ia-enact.ai/">Program website</a> •
        <a href="https://cluster-ia-enact.ai/wp-content/uploads/2025/10/External-Chairs-Call-for-Proposals-ENG-v1.pdf">Call for Proposals (PDF)</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div>
      <div><a href="mailto:npadoy@unistra.fr">Prof. Nicolas Padoy</a> (IHU Strasbourg / CAMMA Group)</div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>Multiple Chair positions available; strong support package and collaboration opportunities with regional research institutes. </div>
    </div>
  </div>
</div>



<!-- Faculty: Vanderbilt University — Computation & Psychology -->
<div class="card md-6" data-tags="jobs faculty computation psychology ai vanderbilt nashville">
  <div class="title"><b>Open-Rank Faculty Search — Computation &amp; Psychology (Vanderbilt University)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>Vanderbilt University — College of Connected Computing</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Nashville, Tennessee, USA</div></div>
    <div class="meta-row"><div class="meta-label">Rank</div><div>Assistant (tenure-track), Associate, or Full Professor</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Application review begins November&nbsp;5,&nbsp;2025</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>Fall&nbsp;2026 (anticipated)</div></div>
    <div class="meta-row"><div class="meta-label">Fields</div>
      <div>
        Computational models of cognition · Computational social psychology · Affective computing · Clinical informatics · Computational models of development · Machine learning &amp; psychometrics · Computational neuroscience · NLP · Agentic &amp; generative AI
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Qualifications</div>
      <div>
        Ph.D. in computer science, psychology, neuroscience, data science, or related field; strong publication record in computational methods (e.g., ML, modeling, AI); collaborative, interdisciplinary research experience.
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Application</div>
      <div>
        Apply via Interfolio:
        <a href="https://apply.interfolio.com/175958">Assistant (TT)</a> ·
        <a href="https://apply.interfolio.com/175978">Associate/Full</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:hansen.schwartz@vanderbilt.edu">Andy&nbsp;Schwartz</a> (Search&nbsp;Chair)</div></div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Part of Vanderbilt’s new College of Connected Computing, integrating psychological theory with computation and AI.</li>
          <li>Opportunities for secondary/joint appointments with Psychology and related departments.</li>
          <li>Emphasis on building interdisciplinary graduate and academic programs in computational psychology.</li>
        </ul>
      </div>
    </div>
  </div>
</div>



<div class="card md-6">
  <div class="title"><b>New York University Abu&nbsp;Dhabi — Psychology Professor (Tenured/Tenure-Track)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Institution</div><div>New York University Abu&nbsp;Dhabi (Division of Science)</div></div>
    <div class="meta-row"><div class="meta-label">Position</div><div>Assistant, Associate, or Full Professor — Psychology</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>November&nbsp;30,&nbsp;2025</div></div>
    <div class="meta-row"><div class="meta-label">Areas</div>
      <div>Perception, Cognition, Cognitive&nbsp;Development, Social&nbsp;Psychology, Developmental&nbsp;Approaches</div>
    </div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://apply.interfolio.com/175417">apply.interfolio.com/175417</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Open to early-career and established scholars with demonstrated excellence in research and leadership.</li>
          <li>Strong research environment with interdisciplinary opportunities across NYU’s global network.</li>
          <li>Access to world-class facilities including a Siemens Prisma 3T MRI, MEG systems (SQUID &amp; OPM), multimodal EEG, VR, and child observation labs.</li>
          <li>For inquiries: <a href="mailto:nyuad.academicrecruitment@nyu.edu">nyuad.academicrecruitment@nyu.edu</a></li>
          <li>Equal-opportunity employer; applications from diverse candidates encouraged.</li>
        </ul>
      </div>
    </div>
  </div>
</div>


    <div class="card md-6">
      <div class="title"><b>Sensitometry &amp; Colorimetry Teacher</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>Start Nov 1, 2025</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>ENS Louis-Lumière, Cité du Cinéma, 20 rue Ampère, 93200 Saint-Denis, France</div></div>
        <div class="meta-row"><div class="meta-label">Website</div>
          <div>
            <a href="https://media.licdn.com/dms/document/media/v2/D4E1FAQG9dJXIDRmNGQ/feedshare-document-pdf-analyzed/B4EZklH8AvIQAY-/0/1757264463945?e=1758758400&v=beta&t=wo3cGIzo8QEPxPAO2lQCK9-hEVicxhwuWGj3aWdlJcw">Job PDF (LinkedIn)</a>
          </div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Full-time (Category A), 1-year renewable fixed-term</li>
              <li>Teaches sensitometry &amp; colorimetry across Photography &amp; Cinema Masters (L3/M1/M2)</li>
              <li>Supervision of theses; lab management</li>
              <li>Strong IT/Python required</li>
              <li>Apply by <b>Sept 21, 2025</b> to <a href="mailto:direction@ens-louis-lumiere.fr">direction@ens-louis-lumiere.fr</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>

<div class="card md-6">
  <div class="title"><b>Assistant Professor — Computational Neuroscience and/or Cognition</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Category</div><div>Faculty (Tenure-Track)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>November 10, 2025 (full consideration)</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>University of Rochester — Department of Brain &amp; Cognitive Sciences (BCS), Rochester, NY</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://apply.interfolio.com/173951">apply.interfolio.com/173951</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div><div>Prof. Florian Jaeger (fjaeger@ur.rochester.edu); Prof. Dora Biro (dbiro@ur.rochester.edu)</div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Focus: Computational theories/models of brain &amp; cognition in real-world, naturalistic contexts</li>
          <li>Cluster hire supported by Simons Foundation (Math, Physics, Biology, BCS)</li>
          <li>Research areas: decision-making, group communication, interactive/VR environments, multi-level modeling</li>
          <li>Department: Interdisciplinary, ties to Neuroscience, CS, Physics, Engineering, Data Science, etc.</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Faculty: Sabancı University — Psychology (Assistant/Associate Professor) -->
<div class="card md-6" data-tags="jobs faculty psychology sabanci turkey">
  <div class="title"><b>Sabancı University (Istanbul) — Faculty Position in Psychology</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Rank</div><div>Assistant Professor (preferred); exceptional Associate considered</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>September 2026</div></div>
    <div class="meta-row"><div class="meta-label">Areas</div><div>All areas welcome; emphasis on Developmental Psychology or computational approaches</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Istanbul, Türkiye</div></div>
    <div class="meta-row"><div class="meta-label">Website</div><div><a href="https://crm.sabanciuniv.edu/academiccv/application.php?advertcode=FASS-PSY-2025">Application portal</a></div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>November&nbsp;1, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Contact</div><div><a href="mailto:ayse.otenoglu@sabanciuniv.edu">ayse.otenoglu@sabanciuniv.edu</a></div></div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>English-language instruction</li>
          <li>Interdisciplinary, department-free structure</li>
          <li>Competitive package: research fund, health insurance, housing/stipend</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<!-- Faculty — University of Nevada, Reno (Cognitive & Brain Sciences) -->
<div class="card md-6">
  <div class="title"><b>Assistant Professor — Cognitive &amp; Brain Sciences (UNR)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Organization</div><div>University of Nevada, Reno — Department of Psychology</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>July 1, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Reno, Nevada (USA)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Review begins December 1, 2025</div></div>
    <div class="meta-row"><div class="meta-label">Website</div>
      <div><a href="https://nshe.wd1.myworkdayjobs.com/UNR-external/job/University-of-Nevada-Reno---Main-Campus/Assistant-Professor--Cognitive-and-Brain-Sciences_R0149374">Apply (Workday)</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Contact</div>
      <div><a href="mailto:shaigh@unr.edu">Sarah Haigh</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Focus: perception, attention, memory, language, decision-making, cognitive control, or neural mechanisms of cognition.</li>
          <li>Methods may include fMRI, EEG, fNIRS, computational modeling / ML, TMS / tDCS, adaptive optics.</li>
          <li>Department of Psychology (CBS program) + Institute for Neuroscience with shared EEG, 3 T MRI, fNIRS and modeling facilities.</li>
          <li>Application materials: CV, cover letter, research and teaching statements, three references.</li>
        </ul>
      </div>
    </div>
  </div>
</div>


<!-- Faculty (Teaching Track): University of Chicago — Instructional Professor, Psychology -->
<div class="card md-6" data-tags="jobs faculty teaching-track psychology uchicago instructional professor">
  <div class="title"><b>University of Chicago — Instructional Professor (Assistant/Associate/Full), Psychology</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Rank</div><div>Instructional Professor (Assistant/Associate/Full)</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>On or after July&nbsp;1,&nbsp;2026</div></div>
    <div class="meta-row"><div class="meta-label">Teaching</div><div>6 quarter-long undergraduate courses per year</div></div>
    <div class="meta-row"><div class="meta-label">Areas</div>
      <div>Broad coverage in perception, action, cognition; advanced research methods; experience conducting psychological research.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Chicago, IL (USA)</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://apply.interfolio.com/173929">Apply via Interfolio</a> •
        <a href="https://psychology.uchicago.edu/">Department of Psychology</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Review begins November&nbsp;3,&nbsp;2025 (rolling until filled)</div></div>
    <div class="meta-row"><div class="meta-label">Contact</div>
      <div><a href="mailto:mdrosenberg@uchicago.edu">Monica&nbsp;Rosenberg</a> (search contact)</div>
    </div>
    <div class="meta-row"><div class="meta-label">Notes</div>
      <div>Full-time, career-track teaching position (3–5 year renewable term; promotion within IP track). Union-covered role; benefits per University policy.</div>
    </div>
  </div>
</div>


<!-- Faculty: University of California, Irvine — Cognitive Sciences -->
<div class="card md-6" data-tags="jobs faculty cognitive-science uci california ai modeling perception">
  <div class="title"><b>University of California, Irvine — Assistant Professor, Cognitive Sciences</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Rank</div><div>Tenure-track Assistant Professor</div></div>
    <div class="meta-row"><div class="meta-label">Start</div><div>July 1, 2026</div></div>
    <div class="meta-row">
      <div class="meta-label">Areas</div>
      <div>
        Empirical human cognition (perception, memory, decision-making) combined with computational modeling,
        cognitive modeling, or data science. Interest in AI approaches and human–AI interaction encouraged.
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Irvine, California (USA)</div></div>

    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div>
        <a href="https://recruit.ap.uci.edu/JPF09896">Application portal</a> •
        <a href="https://www.cogsci.uci.edu/">Department of Cognitive Sciences</a>
      </div>
    </div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>November&nbsp;15,&nbsp;2025 (priority review)</div></div>
    <div class="meta-row">
      <div class="meta-label">Contact</div>
      <div><a href="mailto:zpizlo@uci.edu">Zygmunt&nbsp;Pizlo</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        Research may bridge cognitive science, neuroscience, computer science, and AI.
        UCI offers strong interdisciplinary collaborations across cognitive science, engineering, and statistics.
      </div>
    </div>
  </div>
</div>



    
<div class="card md-6">
  <div class="title"><b>Assistant Professor in Cognitive &amp; Psychological Sciences (AI &amp; the Mind)</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>Start July 1, 2026</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>Brown University — Department of Cognitive &amp; Psychological Sciences (CoPsy), Providence, RI</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://apply.interfolio.com/173939">apply.interfolio.com/173939</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Area: AI &amp; the Mind; intersections of AI and cognition</li>
          <li>Examples: AI models of cognition, BCI, cognitive enhancement, human–AI interaction, algorithmic bias</li>
          <li><b>Full-review deadline:</b> November 8, 2025</li>
          <li>Materials: CV; 3 pubs/preprints; research (≤2 pp) &amp; teaching statements (≤1 pp); 3 letters via Interfolio</li>
        </ul>
      </div>
    </div>
  </div>
</div>
    
    <div class="card md-6">
      <div class="title"><b>Assistant Professor in Psychology (Cognitive Neuroscience)</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>Start Aug 7, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>University of South Florida, Tampa, FL</div></div>
        <div class="meta-row"><div class="meta-label">Website</div>
          <div><a href="https://fa-ewkd-saasfaprod1.fa.ocs.oraclecloud.com/hcmUI/CandidateExperience/en/sites/USF/job/41553/?utm_medium=jobshare&utm_source=External+Job+Share">USF Job 41553</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Focus on human cognition</li>
              <li>Screening begins Oct 17, 2025</li>
              <li>Contact: <a href="mailto:ratchley@usf.edu">ratchley@usf.edu</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <div class="card md-6">
      <div class="title"><b>Senior Lecturer in Psychology</b></div>
      <div class="meta">
        <div class="meta-row"><div class="meta-label">Dates</div><div>Start Aug 16, 2026</div></div>
        <div class="meta-row"><div class="meta-label">Location</div><div>Vanderbilt University, Nashville, TN</div></div>
        <div class="meta-row"><div class="meta-label">Website</div>
          <div><a href="https://apply.interfolio.com/173008">apply.interfolio.com/173008</a></div>
        </div>
        <div class="meta-row"><div class="meta-label">Notes</div>
          <div>
            <ul class="notes-list">
              <li>Non-tenure-track instructional faculty (3-year renewable)</li>
              <li>Teach General Psychology plus quantitative/experimental and elective courses</li>
              <li>PhD required by Aug 1, 2026</li>
              <li>Screening begins Dec 2025</li>
              <li>Contact: <a href="mailto:isabel.gauthier@vanderbilt.edu">isabel.gauthier@vanderbilt.edu</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <div class="card md-6">
  <div class="title"><b>Senior Faculty Position — Psychological &amp; Brain Sciences</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Dates</div><div>Start date TBD</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>University of Iowa — Department of Psychological &amp; Brain Sciences</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://psychology.uiowa.edu/about/job-openings">psychology.uiowa.edu/about/job-openings</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Rank: Associate or Full Professor</li>
          <li>Research area open; search details via department website</li>
          <li>No fixed application deadline listed</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<div class="card md-6">
  <div class="title"><b>Assistant Professor — Computational Cognitive Neuroscience</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Category</div><div>Faculty (Tenure-Track)</div></div>
    <div class="meta-row"><div class="meta-label">Deadline</div><div>Open until filled</div></div>
    <div class="meta-row"><div class="meta-label">Location</div><div>University at Albany (SUNY) — Department of Psychology, Albany, NY</div></div>
    <div class="meta-row">
      <div class="meta-label">Website</div>
      <div><a href="https://albany.interviewexchange.com/jobofferdetails.jsp?JOBID=193092">albany.interviewexchange.com/jobofferdetails.jsp?JOBID=193092</a></div>
    </div>
    <div class="meta-row">
      <div class="meta-label">Notes</div>
      <div>
        <ul class="notes-list">
          <li>Focus: human cognition &amp; brain with computational approaches (AI/ML/neuromorphic)</li>
          <li>Intersects modern neuroscience methods; departmental appointment in Psychology</li>
          <li>Start date per posting; standard materials via portal</li>
        </ul>
      </div>
    </div>
  </div>
</div>

  </div>

<!-- Funding -->
<h3 id="funding">Funding</h3>
<p class="section-desc">
  Travel awards, fellowships, and research support opportunities with typical timelines and eligibility notes.
</p>

<div class="cards">
  <div class="card md-6">
    <div class="title"><b>Females of Vision et al. Travel and Networking Award</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>Deadline TBD (typically March)</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Vision Sciences Society Annual Meeting</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="http://www.foveavision.org/awards/accepting-applications">http://www.foveavision.org/awards/accepting-applications</a></div></div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>$1,000 each, up to 8 awards</li>
            <li>Open to women, transgender, intersex, or non-binary graduate students, postdocs, or early-career vision scientists</li>
            <li>Contact: <a href="mailto:fovea.award@gmail.com">fovea.award@gmail.com</a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="card md-6">
    <div class="title"><b>Shanahan Foundation Fellowship (Allen Institute + UW)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>Application deadline: December 1, 2025</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Seattle, WA (Allen Institute &amp; University of Washington)</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div><a href="https://alleninstitute.org/what-we-do/brain-science/careers/shanahan-foundation-fellowship/" aria-label="Shanahan Foundation Fellowship page">Allen Institute – Shanahan Fellowship</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>Prestigious 3-year postdoctoral fellowship</li>
            <li>Start date: Fall 2026</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>



<h3 id="competitions">Competitions and Prizes</h3>
<p class="section-desc">
  Community contests and prizes highlighting creativity and achievement (e.g., Illusion of the Year).
</p>

<div class="cards">

<!-- AWARD: ICVS 2026 Early Career Investigator Award -->
<div class="card md-6" id="icvs-earlycareer-2026" data-tags="award competition icvs colour-vision early-career 2026">
  <div class="title"><b>ICVS 2026 Early Career Investigator Award</b></div>
  <div class="meta">
    <div class="meta-row"><div class="meta-label">Deadline</div><div>January 16 2026</div></div>
    <div class="meta-row"><div class="meta-label">Presented At</div><div>ICVS Symposium 2026 · Brighton, UK</div></div>
    <div class="meta-row"><div class="meta-label">Eligibility</div>
      <div>Researchers ≤ 10 years post-PhD (extensions possible with justification). Recognizes exceptional achievement in colour vision research.</div>
    </div>
    <div class="meta-row"><div class="meta-label">Nomination Materials</div>
      <div>Letter of nomination + candidate CV to <a href="mailto:anya.hurlbert@newcastle.ac.uk">anya.hurlbert@newcastle.ac.uk</a></div>
    </div>
    <div class="meta-row"><div class="meta-label">More Info</div><div><a href="https://www.icvs.info/">icvs.info</a></div></div>
  </div>
</div>


  <div class="card md-6" id="ludwig-von-sallmann-prize-2026" data-tags="award prize ophthalmology vision-research iser arvo nomination">
    <div class="title"><b>2026 Ludwig von Sallmann Prize — Call for Nominations</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Organizer</div><div>International Society for Eye Research (ISER)</div></div>
      <div class="meta-row"><div class="meta-label">Deadline</div><div>November 14, 2025</div></div>
      <div class="meta-row"><div class="meta-label">Frequency</div><div>Biennial (every two years)</div></div>
      <div class="meta-row">
        <div class="meta-label">Overview</div>
        <div>Honors an individual or team for significant contributions to vision research and ophthalmology, commemorating Dr. Ludwig von Sallmann’s legacy in advancing understanding of the visual system.</div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Nomination Requirements</div>
        <div>
          <ul class="notes-list">
            <li>Three independent nomination letters (≤ 3 pages each, on institutional letterhead)</li>
            <li>Candidate’s current NIH-format biosketch (max 5 pages)</li>
            <li>All documents merged into a single PDF and emailed to the committee chair</li>
          </ul>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Submission</div>
        <div>Email to <a href="mailto:fliesler@buffalo.edu">Dr. Steven J. Fliesler</a> (Chair, The Ludwig von Sallmann Prize Committee)<br>Subject: “2026 Ludwig von Sallmann Prize Nomination”</div>
      </div>
      <div class="meta-row"><div class="meta-label">Nomination Period</div><div>October 1 – November 14, 2025</div></div>
      <div class="meta-row"><div class="meta-label">More Info</div><div><a href="https://iser.org/page/Awards_Prizes">ISER Awards &amp; Prizes page</a></div></div>
    </div>
  </div>


  <div class="card md-6">
    <div class="title"><b>Best Illusion of the Year Contest (2025)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div><b>Submission deadline CLOSED</b></div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Online</div></div>
      <div class="meta-row"><div class="meta-label">Website</div><div><a href="mailto:smart@neuralcorrelate.com">smart@neuralcorrelate.com</a></div></div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>Submit a 1-minute video of a novel illusion (any modality)</li>
            <li>Top-10 finalists; worldwide voting</li>
            <li>Prizes: $3,000 / $2,000 / $1,000</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="card md-6">
    <div class="title"><b>ISMAR 2025 – “Pitch Your Lab” (Call for Presentations)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>Submission deadline: CLOSED</div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Online (with ISMAR 2025)</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div><a href="https://ismar25.org/" aria-label="ISMAR 2025 Pitch Your Lab call">ismar25.org</a></div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>Short presentations highlighting lab work</li>
            <li>Opportunity to showcase your group during ISMAR</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="card md-6">
    <div class="title"><b>Optics &amp; Photonics News — Annual Photo Contest (OPN)</b></div>
    <div class="meta">
      <div class="meta-row"><div class="meta-label">Dates</div><div>2025 deadline: <b>CLOSED</b></div></div>
      <div class="meta-row"><div class="meta-label">Location</div><div>Online</div></div>
      <div class="meta-row">
        <div class="meta-label">Website</div>
        <div>
          <a href="https://www.optica-opn.org/home/photo_contest/" aria-label="OPN Photo Contest">OPN Photo Contest — contest page</a><br>
          <a href="https://www.optica-opn.org/home/gallery/photo_contests/" aria-label="OPN Photo Contest gallery">Past winners &amp; galleries</a>
        </div>
      </div>
      <div class="meta-row">
        <div class="meta-label">Notes</div>
        <div>
          <ul class="notes-list">
            <li>First prize typically includes a canvas-stretched print, a gift card, and publication in <em>OPN</em></li>
            <li>Submit up to <b>3</b> high-impact images highlighting an aspect of optics</li>
            <li>Image specs (typical): at least <b>2,500 × 3,300 px @ 72 dpi</b> or <b>8.5&quot; × 11&quot; @ 300 dpi</b>; .gif/.png/.jpg</li>
            <li>Annual competition; watch the contest page for the next cycle</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

</div>

<p style="text-align: center; margin: 24px 0;">
    CVNet is supported by the
    <a href="https://www.ski.org/">Smith-Kettlewell Eye Research Institute</a>
  </p>
    
<footer class="site-footer" role="contentinfo">
  <div class="footer-inner">
    <div class="footer-left">
      <small>
        © <span id="cal-year"></span> CVNet &nbsp;•&nbsp; This page is maintained by
       David Peterzell and Michaela Tedesco <span id="cal-updated"></span>
      </small>
    </div>

    <!-- Right: add the resource-style quick actions -->
    <nav class="footer-nav" aria-label="Footer">
      <a href="#top" aria-label="Back to top">Back to top ↑</a>
<a href="https://info.cvnet.org/" aria-label="Back to CVNet">CVNet</a>
      <a href="resources.html">Online Resources</a>
      <a href="./journals%20list.html">Journals</a>
      <a href="https://cvnet.org/mailman/listinfo/cvnet">CVNet Archive</a>
      <a href="http://visionscience.com/pipermail/visionlist_visionscience.com">VisionList Archive</a>
    </nav>
  </div>

   <div class="footer-sub">
    This calendar is community-curated. Send additions or corrections to  <a href="mailto:davidpeterzell@mac.com">davidpeterzell@mac.com</a> &
        <a href="mailto:mtedesco12@gmail.com">mtedesco12@gmail.com</a>.

  </div>
</footer>


<script>
  function walk(node, regex) {
    // Skip excluded containers entirely (header/nav or any [data-no-highlight] region)
    if (node.nodeType === 1) {
      if (node.closest && node.closest('.header-fixed,[data-no-highlight]')) return;
    }

    if (node.nodeType === 3) {
      const match = node.nodeValue.match(regex);
      if (match) {
        const span = document.createElement('span');
        const html = node.nodeValue.replace(regex, '<mark class="cvnet-highlight">$1</mark>');
        span.innerHTML = html;
        node.parentNode.replaceChild(span, node);
      }
    } else if (
      node.nodeType === 1 &&
      node.childNodes &&
      !['SCRIPT', 'STYLE', 'MARK'].includes(node.tagName)
    ) {
      for (let i = 0; i < node.childNodes.length; i++) {
        walk(node.childNodes[i], regex);
      }
    }
  }

  function removeHighlights() {
    document.querySelectorAll('mark.cvnet-highlight').forEach(mark => {
      const parent = mark.parentNode;
      parent.replaceChild(document.createTextNode(mark.textContent), mark);
      parent.normalize();
    });
  }

  function setHeaderOffset() {
    const fixed = document.querySelector('.header-fixed');
    if (!fixed) return;
    const h = fixed.offsetHeight || 0;
    document.body.style.paddingTop = (h + 10) + 'px';
    document.querySelectorAll('h3[id], h4[id]').forEach(el => {
      el.style.scrollMarginTop = (h + 10) + 'px';
    });
  }

  function debounce(fn, ms) {
  let t;
  return (...args) => { clearTimeout(t); t = setTimeout(() => fn.apply(this, args), ms); };
}


  document.addEventListener('DOMContentLoaded', function () {
    const input = document.getElementById('search-input');

    setHeaderOffset();

    let searchTimer = null;
    let lastQuery = '';
    function highlightFilter(value) {
      removeHighlights();
      if (!value || value.length < 2 || value === lastQuery) {
        if (!value) lastQuery = '';
        return;
      }
      lastQuery = value;
      const regex = new RegExp('(' + value.replace(/[.*+?^${}()|[\]\\]/g, '\\$&') + ')', 'gi');
      // Only search within main content areas, never in the fixed header
      document.querySelectorAll('.page').forEach(root => {
        walk(root, regex);
      });
    }

    input.addEventListener('input', () => {
      clearTimeout(searchTimer);
      searchTimer = setTimeout(() => {
        const q = input.value.trim();
        if (!q) { lastQuery = ''; removeHighlights(); return; }
        highlightFilter(q);
      }, 180);
    });
  });

  window.addEventListener('resize', debounce(setHeaderOffset, 100));
  // Also recalc after full assets load to avoid initial anchor "peek"
  window.addEventListener('load', setHeaderOffset);
</script>


<script>
(() => {
  const month = {jan:0,feb:1,mar:2,apr:3,may:4,jun:5,jul:6,aug:7,sep:8,sept:8,oct:9,nov:10,dec:11};
  const re = /\b(January|February|March|April|May|June|July|August|September|Sept\.?|October|November|December|Jan\.?|Feb\.?|Mar\.?|Apr\.?|Jun\.?|Jul\.?|Aug\.?|Oct\.?|Nov\.?|Dec\.?)\s+(\d{1,2}),\s*(\d{4})\b/g;
  const today = new Date(); today.setHours(0,0,0,0);

  document.querySelectorAll('ul.notes-list li').forEach(li => {
    const t = li.textContent;
    let m, future = false;
    re.lastIndex = 0;
    while ((m = re.exec(t))) {
      const mm = month[m[1].toLowerCase().replace('.', '').slice(0,3)];
      const dd = +m[2], yy = +m[3];
      if (mm == null) continue;
      const d = new Date(yy, mm, dd); d.setHours(0,0,0,0);
      if (d >= today) { future = true; break; }
    }
    if (future) {
      li.classList.add('deadline-open');
      if (!li.querySelector('.visually-hidden')) {
        const sr = document.createElement('span');
        sr.className = 'visually-hidden';
        sr.textContent = ' (deadline open)';
        li.appendChild(sr);
      }
    }
  });
})();
</script>
<script>
(function () {
  const $input = document.getElementById('search-input');
  if (!$input) return;

  const $status = document.getElementById('search-status');
  const cards = Array.from(document.querySelectorAll('.cards .card'))
    .filter(el => !el.classList.contains('intro')); // keep the intro card untouched

  // Precompute searchable text for speed
  const haystacks = new Map();
  const norm = s => (s || '').normalize('NFKD').toLowerCase();
  const clean = s => s.replace(/\s+/g, ' ').trim();

  function cardText(card) {
    const title = card.querySelector('.title')?.textContent || '';
    const meta = card.querySelector('.meta')?.textContent || '';
    const tags = card.getAttribute('data-tags') || '';
    const id   = card.id || '';
    return norm([title, meta, tags.replace(/[-_]/g, ' '), id].join(' '));
  }
  function ensureHaystack(card) {
    if (!haystacks.has(card)) haystacks.set(card, cardText(card));
    return haystacks.get(card);
  }

  // Highlight utility (title only, for readability)
  function unhighlight(card) {
    const t = card.querySelector('.title');
    if (!t) return;
    // remove any previous <mark>
    t.innerHTML = t.textContent;
  }
  function highlight(card, tokens) {
    const t = card.querySelector('.title');
    if (!t) return;
    let text = t.textContent;
    for (const tok of tokens) {
      if (!tok) continue;
      const esc = tok.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
      text = text.replace(new RegExp(`(${esc})`, 'ig'), '<mark class="search-hit">$1</mark>');
    }
    t.innerHTML = text;
  }

  let debounceTimer = null;
  function onSearch() {
    const q = clean($input.value);
    const tokens = q.split(' ').filter(Boolean).map(norm);

    let visibleCount = 0;
    for (const card of cards) {
      unhighlight(card);
      if (tokens.length === 0) {
        card.classList.remove('is-filtered-out');
        continue;
      }
      const hay = ensureHaystack(card);
      const matchAll = tokens.every(tok => hay.includes(tok));
      card.classList.toggle('is-filtered-out', !matchAll);
      if (matchAll) {
        highlight(card, tokens);
        visibleCount++;
      }
    }
    if ($status) {
      $status.textContent = tokens.length
        ? `${visibleCount} result${visibleCount === 1 ? '' : 's'}`
        : `Showing all`;
    }
  }

  function debounced() {
    clearTimeout(debounceTimer);
    debounceTimer = setTimeout(onSearch, 120);
  }

  // Clear on Escape
  $input.addEventListener('keydown', (e) => {
    if (e.key === 'Escape' && $input.value) {
      $input.value = '';
      debounced();
      $input.blur();
    }
  });

  $input.addEventListener('input', debounced);

  // If the page loads with a query param ?q=..., prefill and search
  const params = new URLSearchParams(location.search);
  const preset = params.get('q');
  if (preset) {
    $input.value = preset;
    onSearch();
  }
})();
</script>
</body>
</html>
